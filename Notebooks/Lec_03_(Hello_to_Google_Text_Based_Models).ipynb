{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a41c76e0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "---   \n",
    " <img align=\"left\" width=\"75\" height=\"75\"  src=\"https://upload.wikimedia.org/wikipedia/en/c/c8/University_of_the_Punjab_logo.png\"> \n",
    "\n",
    "<h1 align=\"center\">Department of Data Science</h1>\n",
    "\n",
    "---\n",
    "<h3><div align=\"right\">Instructor: Muhammad Arif Butt, Ph.D.</div></h3>    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6fe310",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "<h1 align=\"center\">Lec-03: A Hello to Google Family of AI Models</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef15071",
   "metadata": {},
   "source": [
    "# Learning agenda of this notebook\n",
    "\n",
    "1. Installing Google SDK and Accessing Google Gemini family of AI Models via API keys\n",
    "    - Google Gemini Family of Models\n",
    "    - Installing Google SDKs (`google-genai` and  `google-generativeai`)\n",
    "    - Generating API Keys for Google Gemini\n",
    "    - Saving the API Keys\n",
    "    - Initialize the Google genai Client using `genai.Client()` Method\n",
    "    - Access API Endpoint: `client.models.list()`\n",
    "    - Access API Endpoint: `client.models.generate_content()`\n",
    "    - Access Gemini Models using OpenAI `client.chat.completions.create()`\n",
    "2. Hello World Examples using Gemini family of Models for Text Processing (using OpenAI API)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa00096-4d61-4b45-ab3f-bd56857e41d4",
   "metadata": {},
   "source": [
    "# <span style='background :lightgreen' >1. Installing Google SDK and Accessing Google AI Models via API keys</span>\n",
    "\n",
    "<h2 align=\"center\"><div class=\"alert alert-success\" style=\"margin: 20px\">Google is a global tech company dounded in 1998 by Larry Page  and Sergey Brin. Google acquired DeepMind in 2014 and has become a major player in AI research.</h2>\n",
    "\n",
    "\n",
    "## a. Google Gemini Family of Models\n",
    "\n",
    "- **Overview:**\n",
    "    - Gemini is Google’s flagship family of multimodal AI models, built to handle text, code, images, audio, video, and high-context reasoning within a unified architecture.\n",
    "    - Gemini directly competes with OpenAI’s GPT family and Anthropic’s Claude models, and is deeply integrated across Google products such as Search, Workspace, Android, and Cloud.\n",
    "    - Google positions Gemini as a platform for agentic AI, enterprise workloads, and full multimodal understanding at scale.\n",
    "- **Access Points:**\n",
    "    - Gemini Web Interface: https://gemini.google.com/\n",
    "    - Google AI Studio: https://aistudio.google.com/\n",
    "    - Google Cloud Vertex AI Console: https://console.cloud.google.com/vertex-ai\n",
    "- **Gemini Model Tiers:** Google’s Gemini family is fully multimodal by design, meaning all major versions natively support text, vision, and structured reasoning. The family is organized into performance tiers—Ultra, Pro, and Flash—each optimized for different computational and latency needs.\n",
    "    - **Gemini 1.5 Ultra:** The highest-capability tier with extremely long context (up to 1M tokens), advanced reasoning, high-fidelity multimodal understanding, and state-of-the-art coding performance. Suitable for scientific workflows, research agents, and complex enterprise applications.\n",
    "    - **Gemini 1.5 Pro:** The balanced, general-purpose tier offering strong reasoning, long-context capabilities, and robust multimodal understanding. Ideal for chat assistants, copilots, content generation, and business automations.\n",
    "    - **Gemini 1.5 Flash / Flash-Lite:** Highly optimized for speed and cost efficiency while retaining strong multimodal capabilities. Used for real-time applications, high-throughput workloads, embeddings, and tasks that require rapid responses.\n",
    "\n",
    "| Model (Name + ID) | Cost (Input / Output per MTok) | Context Window | Max Output | Latency |\n",
    "| :--- | :--- | :--- | :--- | :--- |\n",
    "| **Gemini 3 Ultra**<br>`gemini-3.0-ultra` | $8.00 / $24.00 | 2M tokens | 16K | Medium (~3–5s) |\n",
    "| **Gemini 3 Pro**<br>`gemini-3.0-pro` | $2.00 / $12.00* | 2M tokens | 16K | Fast (~1–2s) |\n",
    "| **Gemini 3 Flash**<br>`gemini-3.0-flash` | $0.07 / $0.21 | 1M tokens | 8K | Very Fast (~0.3–0.6s) |\n",
    "| **Gemini 2.5 Pro**<br>`gemini-2.5-pro` | $1.25 / $10.00 | 1M tokens | 8K | Fast (~1–2s) |\n",
    "| **Gemini 2.5 Flash**<br>`gemini-2.5-flash` | $0.10 / $0.40 | 1M tokens | 8K | Very Fast (~0.4s) |\n",
    "| **Gemini 2.5 Flash-Lite**<br>`gemini-2.5-flash-lite` | $0.05 / $0.15 | 1M tokens | 4K | Extremely Fast (~0.3s) |\n",
    "| **Gemini 2.0 Ultra**<br>`gemini-2.0-ultra` | $12.00 / $24.00 | 1M tokens | 8K | Slower (~4–7s) |\n",
    "| **Gemini 2.0 Pro**<br>`gemini-2.0-pro` | $2.50 / $5.00 | 1M tokens | 8K | Fast (~1–2s) |\n",
    "| **Gemini 2.0 Flash**<br>`gemini-2.0-flash` | $0.10 / $0.20 | 1M tokens | 8K | Very Fast (~0.4–0.7s) |\n",
    "| **Gemini 2.0 Nano-2**<br>`gemini-2.0-nano-2` | Free / Free | 128K tokens | 2K | Instant (<0.3s) |\n",
    "| **Gemini 1.5 Pro**<br>`gemini-1.5-pro` | $3.50 / $7.00 | 1M tokens | 8K | Fast (~1–2s) |\n",
    "| **Gemini 1.5 Flash**<br>`gemini-1.5-flash` | $0.35 / $0.70 | 1M tokens | 8K | Very Fast (~0.5–1s) |\n",
    "\n",
    "> (Pricing varies slightly by region and whether used via AI Studio or Vertex AI. Enterprise plans may differ.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a4efa9-f287-4d9e-95e8-101291121a3e",
   "metadata": {},
   "source": [
    "## b. Installing `google-genai` SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03a6d26d-2abd-48dd-8e8a-4b432b43cdcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install google SDK\n",
    "#!uv add  google-genai      # Newer Unified SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e43ed50-0f8c-467b-a685-d81c5c6dc309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mResolved \u001b[1m240 packages\u001b[0m \u001b[2min 0.83ms\u001b[0m\u001b[0m\n",
      "├── google-genai v1.56.0\n"
     ]
    }
   ],
   "source": [
    "# check out the version of your google installed package\n",
    "!uv tree | grep google-genai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07e7f75-c8bb-4b2e-abb8-4726f8bc09ee",
   "metadata": {},
   "source": [
    "## c. Generating API Keys for Google Gemini\n",
    "<h3 align=\"center\"><div class=\"alert alert-success\" style=\"margin: 20px\">Google lets you create an API key for free and use it with some limited free quota</h3>\n",
    "\n",
    "- Today we will be accessing Google Gemini LLMs via API — meaning you call the model running on Google’s cloud, and it will respond with answers to your questions.\n",
    "- Follow the steps below to generate your Google Gemini API key:\n",
    "    - **Visit Google AI Studio:** Go to https://aistudio.google.com/ and sign in with your Google account.\n",
    "    - Enable API access: Once logged in, navigate to the Get API key option in the left-hand sidebar.\n",
    "    - **Enable and Create API key:** Once logged in, navigate to the Get API key option in the left-hand sidebar. Generate a new API key for use with Gemini models.\n",
    "    - **Save your key:** Copy the API key and store it securely (e.g., in environment variables). Do not hardcode it in public code or share it.\n",
    "    - Set up billing (if required): For production or high-usage projects, you’ll need to enable billing through Google Cloud Console.\n",
    "    - Pricing reference: For up-to-date pricing of various Gemini models (Pro, Flash, Ultra), visit: https://ai.google.dev/gemini-api/docs/pricing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302ffe92-7905-4e20-8d84-be7ae22405d8",
   "metadata": {},
   "source": [
    "## d. Saving the API Keys\n",
    "- An **environment variable** is a key–value pair stored by your operating system that programs can access at runtime. They’re used to configure programs without hardcoding settings in code. Think of them as invisible notes your system uses to tell programs what to do. For example: When you run a Python program, it might check an environment variable called DEBUG to decide whether to show detailed error logs. APIs like OpenAI or Stripe often require you to store your secret API keys in an environment variable like ANTHROPIC_API_KEY.\n",
    "\n",
    "#### Option 1: (Save API Keys in an environment variable on Mac/Linux/Windows)\n",
    "- For Linux/MAC machines, create an environment variable named ANTHROPIC_API_KEY inside .zshrc and save your key there by running this command on your terminal:`echo \"export ANTHROPIC_API_KEY='yourkey'\" >> ~/.zshrc`\n",
    "- Update the shell with the new variable by running the command: `source ~/.zshrc`\n",
    "- Confirm that you have set your environment variable using the command `echo $ANTHROPIC_API_KEY`\n",
    "- Now inside your Python code, you can access your key using this LOC `openai.api_key = os.environ[\"ANTHROPIC_API_KEY\"]`\n",
    "\n",
    "#### Option 2: (Save API Keys inside `.env` file):\n",
    "- The best way is to create a `.env` or just `env` file within your Python project directory and save all your API keys inside it, and it doesnot go into your git repositories. Your .env file must not be shared publicly as it will contain secrets like API keys or passwords. To ensure this you should add `.env` to your `.gitignore` file.\n",
    "- In a real project, we may need to use more than one API keys, one for OpenAI, another for Hugging Face, another for Google's Gemini, another for Pinecone, and so on. Finally, your `.env` file might contain many keys like:\n",
    "```\n",
    "OPENAI_API_KEY=sk-proj-xxxx\n",
    "ANTHROPIC_API_KEY=sk-proj-xxxx\n",
    "GOOGLE_API_KEY=sk-proj-xxxx\n",
    "```\n",
    "- Now inside your Python code, you can access your key using the `load_dotenv()` method of Python `dotenv` module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19dbabd3-7e51-45d7-a993-e922c05c74b1",
   "metadata": {},
   "source": [
    "## e. Accessing and Testing the API Keys\n",
    "- The `load_dotenv()` method looks for a file named `.env` in the current directory or in the specified path as mentioned in its first argument. The second argument `override=True` is optional and it means  any existing environment variables will be overwritten by values from the .env file. This method reads each line in the `.env` file and sets the environment variables accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95224568-482c-4c0e-bf9d-b649b6aa534b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google API Key exists and begins AIzaSyDAKD\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv('../keys/.env', override=True) \n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "\n",
    "if google_api_key:\n",
    "    print(f\"Google API Key exists and begins {google_api_key[:10]}\")\n",
    "else:\n",
    "    print(\"Google API Key not set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0491c68c-aaea-4507-9556-d4c512ab9201",
   "metadata": {},
   "source": [
    "## f. Initialize the Google genai Client using `genai.Client()` Method\n",
    "- The return value is a Gemini client object, which you use to make API calls to following endpoints:\n",
    "    - client.models.list() – list available Gemini models\n",
    "    - client.text.generate() – basic text completion\n",
    "    - client.chat.completions.create() (if using chat-style models)\n",
    "    - client.responses.create() – unified multimodal text responses\n",
    "    - client.images.generate() – image generation (if enabled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36da142c-09b1-4f28-b2d9-0f9adaafe6a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<google.genai.client.Client at 0x128b86b70>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from google import genai  # Google Gemini SDK\n",
    "\n",
    "load_dotenv('../keys/.env', override=True) \n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "\n",
    "# Initialize the Google Gen AI client\n",
    "client = genai.Client(api_key=google_api_key)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631e6409-de4a-47b8-8507-e941a4f110e0",
   "metadata": {},
   "source": [
    "## g. Access API Endpoint: `client.models.list()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f53a41b5-36b8-4ede-8c0b-1c3d9e7bf8d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Gemini models:\n",
      "\n",
      "  - models/embedding-gecko-001\n",
      "  - models/gemini-2.5-flash\n",
      "  - models/gemini-2.5-pro\n",
      "  - models/gemini-2.0-flash-exp\n",
      "  - models/gemini-2.0-flash\n",
      "  - models/gemini-2.0-flash-001\n",
      "  - models/gemini-2.0-flash-exp-image-generation\n",
      "  - models/gemini-2.0-flash-lite-001\n",
      "  - models/gemini-2.0-flash-lite\n",
      "  - models/gemini-2.0-flash-lite-preview-02-05\n",
      "  - models/gemini-2.0-flash-lite-preview\n",
      "  - models/gemini-exp-1206\n",
      "  - models/gemini-2.5-flash-preview-tts\n",
      "  - models/gemini-2.5-pro-preview-tts\n",
      "  - models/gemma-3-1b-it\n",
      "  - models/gemma-3-4b-it\n",
      "  - models/gemma-3-12b-it\n",
      "  - models/gemma-3-27b-it\n",
      "  - models/gemma-3n-e4b-it\n",
      "  - models/gemma-3n-e2b-it\n",
      "  - models/gemini-flash-latest\n",
      "  - models/gemini-flash-lite-latest\n",
      "  - models/gemini-pro-latest\n",
      "  - models/gemini-2.5-flash-lite\n",
      "  - models/gemini-2.5-flash-image-preview\n",
      "  - models/gemini-2.5-flash-image\n",
      "  - models/gemini-2.5-flash-preview-09-2025\n",
      "  - models/gemini-2.5-flash-lite-preview-09-2025\n",
      "  - models/gemini-3-pro-preview\n",
      "  - models/gemini-3-flash-preview\n",
      "  - models/gemini-3-pro-image-preview\n",
      "  - models/nano-banana-pro-preview\n",
      "  - models/gemini-robotics-er-1.5-preview\n",
      "  - models/gemini-2.5-computer-use-preview-10-2025\n",
      "  - models/deep-research-pro-preview-12-2025\n",
      "  - models/embedding-001\n",
      "  - models/text-embedding-004\n",
      "  - models/gemini-embedding-exp-03-07\n",
      "  - models/gemini-embedding-exp\n",
      "  - models/gemini-embedding-001\n",
      "  - models/aqa\n",
      "  - models/imagen-4.0-generate-preview-06-06\n",
      "  - models/imagen-4.0-ultra-generate-preview-06-06\n",
      "  - models/imagen-4.0-generate-001\n",
      "  - models/imagen-4.0-ultra-generate-001\n",
      "  - models/imagen-4.0-fast-generate-001\n",
      "  - models/veo-2.0-generate-001\n",
      "  - models/veo-3.0-generate-001\n",
      "  - models/veo-3.0-fast-generate-001\n",
      "  - models/veo-3.1-generate-preview\n",
      "  - models/veo-3.1-fast-generate-preview\n",
      "  - models/gemini-2.5-flash-native-audio-latest\n",
      "  - models/gemini-2.5-flash-native-audio-preview-09-2025\n",
      "  - models/gemini-2.5-flash-native-audio-preview-12-2025\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from google import genai  # Google Gemini SDK\n",
    "\n",
    "load_dotenv('../keys/.env', override=True) \n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "\n",
    "# Initialize the Google Gen AI client\n",
    "client = genai.Client(api_key=google_api_key)\n",
    "\n",
    "# Call the models.list() method that returns an iterable list of model objects\n",
    "models = client.models.list()       \n",
    "\n",
    "print(\"Available Gemini models:\\n\")\n",
    "for m in models:\n",
    "    # Each model object has a .name property representing the model ID\n",
    "    print(f\"  - {m.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea5f1bd-f3b6-471e-a43e-b2b8881dce97",
   "metadata": {},
   "source": [
    "## h. Access API Endpoint: `client.models.generate_content()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74275cc3-ad94-44a5-80a1-bcffe81c15e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of Pakistan is **Islamabad**.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">GenerateContentResponse</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">sdk_http_response</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">HttpResponse</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">headers</span>=<span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'content-type'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'application/json; charset=UTF-8'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'vary'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Origin, X-Origin, Referer'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'content-encoding'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'gzip'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'date'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Sun, 01 Feb 2026 07:54:01 GMT'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'server'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'scaffolding on HTTPServer2'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'x-xss-protection'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'0'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'x-frame-options'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'SAMEORIGIN'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'x-content-type-options'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'nosniff'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'server-timing'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'gfet4t7; dur=1240'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'alt-svc'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'transfer-encoding'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'chunked'</span>\n",
       "        <span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">body</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">candidates</span>=<span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Candidate</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Content</span><span style=\"font-weight: bold\">(</span>\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">parts</span>=<span style=\"font-weight: bold\">[</span>\n",
       "                    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Part</span><span style=\"font-weight: bold\">(</span>\n",
       "                        <span style=\"color: #808000; text-decoration-color: #808000\">media_resolution</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "                        <span style=\"color: #808000; text-decoration-color: #808000\">code_execution_result</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "                        <span style=\"color: #808000; text-decoration-color: #808000\">executable_code</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "                        <span style=\"color: #808000; text-decoration-color: #808000\">file_data</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "                        <span style=\"color: #808000; text-decoration-color: #808000\">function_call</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "                        <span style=\"color: #808000; text-decoration-color: #808000\">function_response</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "                        <span style=\"color: #808000; text-decoration-color: #808000\">inline_data</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "                        <span style=\"color: #808000; text-decoration-color: #808000\">text</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'The capital of Pakistan is **Islamabad**.'</span>,\n",
       "                        <span style=\"color: #808000; text-decoration-color: #808000\">thought</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "                        <span style=\"color: #808000; text-decoration-color: #808000\">thought_signature</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "                        <span style=\"color: #808000; text-decoration-color: #808000\">video_metadata</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>\n",
       "                    <span style=\"font-weight: bold\">)</span>\n",
       "                <span style=\"font-weight: bold\">]</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">role</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'model'</span>\n",
       "            <span style=\"font-weight: bold\">)</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">citation_metadata</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">finish_message</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">token_count</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">finish_reason</span>=<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">FinishReason.STOP:</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #008000; text-decoration-color: #008000\">'STOP'</span><span style=\"color: #000000; text-decoration-color: #000000\">&gt;,</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">            </span><span style=\"color: #808000; text-decoration-color: #808000\">avg_logprobs</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">            </span><span style=\"color: #808000; text-decoration-color: #808000\">grounding_metadata</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">            </span><span style=\"color: #808000; text-decoration-color: #808000\">index</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">            </span><span style=\"color: #808000; text-decoration-color: #808000\">logprobs_result</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">            </span><span style=\"color: #808000; text-decoration-color: #808000\">safety_ratings</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">            </span><span style=\"color: #808000; text-decoration-color: #808000\">url_context_metadata</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">        </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">)</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">    </span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">]</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">    </span><span style=\"color: #808000; text-decoration-color: #808000\">create_time</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">    </span><span style=\"color: #808000; text-decoration-color: #808000\">model_version</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">'gemini-2.5-flash-lite'</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">    </span><span style=\"color: #808000; text-decoration-color: #808000\">prompt_feedback</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">    </span><span style=\"color: #808000; text-decoration-color: #808000\">response_id</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">'mQZ_acS_H6mc28oP7MGVyQw'</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">    </span><span style=\"color: #808000; text-decoration-color: #808000\">usage_metadata</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">GenerateContentResponseUsageMetadata</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">        </span><span style=\"color: #808000; text-decoration-color: #808000\">cache_tokens_details</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">        </span><span style=\"color: #808000; text-decoration-color: #808000\">cached_content_token_count</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">        </span><span style=\"color: #808000; text-decoration-color: #808000\">candidates_token_count</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">        </span><span style=\"color: #808000; text-decoration-color: #808000\">candidates_tokens_details</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">        </span><span style=\"color: #808000; text-decoration-color: #808000\">prompt_token_count</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span><span style=\"color: #000000; text-decoration-color: #000000\">,</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000\">        </span><span style=\"color: #808000; text-decoration-color: #808000\">prompt_tokens_details</span><span style=\"color: #000000; text-decoration-color: #000000\">=</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">[</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ModalityTokenCount</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">modality</span><span style=\"color: #000000; text-decoration-color: #000000\">=&lt;MediaModality.TEXT: </span><span style=\"color: #008000; text-decoration-color: #008000\">'TEXT'</span><span style=\"font-weight: bold\">&gt;</span>, <span style=\"color: #808000; text-decoration-color: #808000\">token_count</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span><span style=\"font-weight: bold\">)]</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">thoughts_token_count</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">tool_use_prompt_token_count</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">tool_use_prompt_tokens_details</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">total_token_count</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">17</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">traffic_type</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">automatic_function_calling_history</span>=<span style=\"font-weight: bold\">[]</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">parsed</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mGenerateContentResponse\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33msdk_http_response\u001b[0m=\u001b[1;35mHttpResponse\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mheaders\u001b[0m=\u001b[1m{\u001b[0m\n",
       "            \u001b[32m'content-type'\u001b[0m: \u001b[32m'application/json; \u001b[0m\u001b[32mcharset\u001b[0m\u001b[32m=\u001b[0m\u001b[32mUTF\u001b[0m\u001b[32m-8'\u001b[0m,\n",
       "            \u001b[32m'vary'\u001b[0m: \u001b[32m'Origin, X-Origin, Referer'\u001b[0m,\n",
       "            \u001b[32m'content-encoding'\u001b[0m: \u001b[32m'gzip'\u001b[0m,\n",
       "            \u001b[32m'date'\u001b[0m: \u001b[32m'Sun, 01 Feb 2026 07:54:01 GMT'\u001b[0m,\n",
       "            \u001b[32m'server'\u001b[0m: \u001b[32m'scaffolding on HTTPServer2'\u001b[0m,\n",
       "            \u001b[32m'x-xss-protection'\u001b[0m: \u001b[32m'0'\u001b[0m,\n",
       "            \u001b[32m'x-frame-options'\u001b[0m: \u001b[32m'SAMEORIGIN'\u001b[0m,\n",
       "            \u001b[32m'x-content-type-options'\u001b[0m: \u001b[32m'nosniff'\u001b[0m,\n",
       "            \u001b[32m'server-timing'\u001b[0m: \u001b[32m'gfet4t7; \u001b[0m\u001b[32mdur\u001b[0m\u001b[32m=\u001b[0m\u001b[32m1240\u001b[0m\u001b[32m'\u001b[0m,\n",
       "            \u001b[32m'alt-svc'\u001b[0m: \u001b[32m'\u001b[0m\u001b[32mh3\u001b[0m\u001b[32m=\":443\"; \u001b[0m\u001b[32mma\u001b[0m\u001b[32m=\u001b[0m\u001b[32m2592000\u001b[0m\u001b[32m,h3-\u001b[0m\u001b[32m29\u001b[0m\u001b[32m=\":443\"; \u001b[0m\u001b[32mma\u001b[0m\u001b[32m=\u001b[0m\u001b[32m2592000\u001b[0m\u001b[32m'\u001b[0m,\n",
       "            \u001b[32m'transfer-encoding'\u001b[0m: \u001b[32m'chunked'\u001b[0m\n",
       "        \u001b[1m}\u001b[0m,\n",
       "        \u001b[33mbody\u001b[0m=\u001b[3;35mNone\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[33mcandidates\u001b[0m=\u001b[1m[\u001b[0m\n",
       "        \u001b[1;35mCandidate\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mcontent\u001b[0m=\u001b[1;35mContent\u001b[0m\u001b[1m(\u001b[0m\n",
       "                \u001b[33mparts\u001b[0m=\u001b[1m[\u001b[0m\n",
       "                    \u001b[1;35mPart\u001b[0m\u001b[1m(\u001b[0m\n",
       "                        \u001b[33mmedia_resolution\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "                        \u001b[33mcode_execution_result\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "                        \u001b[33mexecutable_code\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "                        \u001b[33mfile_data\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "                        \u001b[33mfunction_call\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "                        \u001b[33mfunction_response\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "                        \u001b[33minline_data\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "                        \u001b[33mtext\u001b[0m=\u001b[32m'The capital of Pakistan is **Islamabad**.'\u001b[0m,\n",
       "                        \u001b[33mthought\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "                        \u001b[33mthought_signature\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "                        \u001b[33mvideo_metadata\u001b[0m=\u001b[3;35mNone\u001b[0m\n",
       "                    \u001b[1m)\u001b[0m\n",
       "                \u001b[1m]\u001b[0m,\n",
       "                \u001b[33mrole\u001b[0m=\u001b[32m'model'\u001b[0m\n",
       "            \u001b[1m)\u001b[0m,\n",
       "            \u001b[33mcitation_metadata\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "            \u001b[33mfinish_message\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "            \u001b[33mtoken_count\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "            \u001b[33mfinish_reason\u001b[0m=\u001b[1m<\u001b[0m\u001b[1;95mFinishReason.STOP:\u001b[0m\u001b[39m \u001b[0m\u001b[32m'STOP'\u001b[0m\u001b[39m>,\u001b[0m\n",
       "\u001b[39m            \u001b[0m\u001b[33mavg_logprobs\u001b[0m\u001b[39m=\u001b[0m\u001b[3;35mNone\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m            \u001b[0m\u001b[33mgrounding_metadata\u001b[0m\u001b[39m=\u001b[0m\u001b[3;35mNone\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m            \u001b[0m\u001b[33mindex\u001b[0m\u001b[39m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m            \u001b[0m\u001b[33mlogprobs_result\u001b[0m\u001b[39m=\u001b[0m\u001b[3;35mNone\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m            \u001b[0m\u001b[33msafety_ratings\u001b[0m\u001b[39m=\u001b[0m\u001b[3;35mNone\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m            \u001b[0m\u001b[33murl_context_metadata\u001b[0m\u001b[39m=\u001b[0m\u001b[3;35mNone\u001b[0m\n",
       "\u001b[39m        \u001b[0m\u001b[1;39m)\u001b[0m\n",
       "\u001b[39m    \u001b[0m\u001b[1;39m]\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m    \u001b[0m\u001b[33mcreate_time\u001b[0m\u001b[39m=\u001b[0m\u001b[3;35mNone\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m    \u001b[0m\u001b[33mmodel_version\u001b[0m\u001b[39m=\u001b[0m\u001b[32m'gemini-2.5-flash-lite'\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m    \u001b[0m\u001b[33mprompt_feedback\u001b[0m\u001b[39m=\u001b[0m\u001b[3;35mNone\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m    \u001b[0m\u001b[33mresponse_id\u001b[0m\u001b[39m=\u001b[0m\u001b[32m'mQZ_acS_H6mc28oP7MGVyQw'\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m    \u001b[0m\u001b[33musage_metadata\u001b[0m\u001b[39m=\u001b[0m\u001b[1;35mGenerateContentResponseUsageMetadata\u001b[0m\u001b[1;39m(\u001b[0m\n",
       "\u001b[39m        \u001b[0m\u001b[33mcache_tokens_details\u001b[0m\u001b[39m=\u001b[0m\u001b[3;35mNone\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m        \u001b[0m\u001b[33mcached_content_token_count\u001b[0m\u001b[39m=\u001b[0m\u001b[3;35mNone\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m        \u001b[0m\u001b[33mcandidates_token_count\u001b[0m\u001b[39m=\u001b[0m\u001b[1;36m9\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m        \u001b[0m\u001b[33mcandidates_tokens_details\u001b[0m\u001b[39m=\u001b[0m\u001b[3;35mNone\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m        \u001b[0m\u001b[33mprompt_token_count\u001b[0m\u001b[39m=\u001b[0m\u001b[1;36m8\u001b[0m\u001b[39m,\u001b[0m\n",
       "\u001b[39m        \u001b[0m\u001b[33mprompt_tokens_details\u001b[0m\u001b[39m=\u001b[0m\u001b[1;39m[\u001b[0m\u001b[1;35mModalityTokenCount\u001b[0m\u001b[1;39m(\u001b[0m\u001b[33mmodality\u001b[0m\u001b[39m=<MediaModality.TEXT: \u001b[0m\u001b[32m'TEXT'\u001b[0m\u001b[1m>\u001b[0m, \u001b[33mtoken_count\u001b[0m=\u001b[1;36m8\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        \u001b[33mthoughts_token_count\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "        \u001b[33mtool_use_prompt_token_count\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "        \u001b[33mtool_use_prompt_tokens_details\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "        \u001b[33mtotal_token_count\u001b[0m=\u001b[1;36m17\u001b[0m,\n",
       "        \u001b[33mtraffic_type\u001b[0m=\u001b[3;35mNone\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[33mautomatic_function_calling_history\u001b[0m=\u001b[1m[\u001b[0m\u001b[1m]\u001b[0m,\n",
       "    \u001b[33mparsed\u001b[0m=\u001b[3;35mNone\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from google import genai  # Google Gemini SDK\n",
    "from google.genai import types\n",
    "import rich\n",
    "\n",
    "load_dotenv('../keys/.env', override=True) \n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "\n",
    "# Initialize the Google Gen AI client\n",
    "client = genai.Client(api_key=google_api_key)\n",
    "\n",
    "# Call the Gemini model for text generation\n",
    "response = client.models.generate_content(\n",
    "                                    model=\"gemini-2.5-flash-lite\",\n",
    "                                    contents=\"What is the capital of Pakistan?\",\n",
    "                                    config= types.GenerateContentConfig(temperature=0.3, top_p=0.9, max_output_tokens=100) # `types` is a sub-module in genai and `GenerateContentConfig` is a class that structure the generation settings (temperature, top-p, and max output tokens) into a clean, validated object instead of loose keyword arguments.\n",
    "                                )\n",
    "# Print the model's answer\n",
    "print(response.text)          # do understand the structure of the response object returned by models.generate_content() method\n",
    "rich.print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4fb6fa-2d52-4bed-9d6a-ac0e9a4a5ab6",
   "metadata": {},
   "source": [
    "## i. Access Gemini Models using OpenAI `client.chat.completions.create()`\n",
    "- The OpenAI API has become a defacto standard, therefore, other providers like Google, Groq, Ollama, Deepseek etc have provided a compatibility layer that speaks the same OpenAI REST API format for thier endpoints.\n",
    "- So instead of initializing the client using the respective provider API we can use OpenAI() constructor and passing it the appropriate api_key and the specific base_url that reroute your request to appropriate provider like Google\n",
    "        - OpenAI → `https://api.openai.com/v1`\n",
    "        - Google → `https://generativelanguage.googleapis.com/v1beta/openai`\n",
    "        - Groq → `https://api.groq.com/openai/v1`\n",
    "        - Ollama → `http://localhost:11434/v1`\n",
    "        - Deepseek → `https://api.deepseek.com/v1`   \n",
    "        - Mistral → `https://api.mistral.ai/v1`\n",
    "```python\n",
    "client = OpenAI(\n",
    "                api_key=google_api_key,\n",
    "                base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\" # OpenAI-compatible endpoint for Gemini, enables developers to use Gemini models through OpenAI chat completion API\n",
    "                )\n",
    "```\n",
    "\n",
    "- Now when we use this `client` to send our request, it will be rerouted to Google’s Gemini API by due to the custom base_url, is authenticated there using the `google_api_key`.\n",
    "- The request format stays the same (OpenAI API style), however, Google’s server translates those calls into Gemini model executions behind the scenes.\n",
    "- Responses come from Gemini models, not OpenAI.\n",
    "```python\n",
    "response = client.chat.completions.create(\n",
    "                                model='gemini-2.5-flash-lite',  \n",
    "                                messages=[{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"Which is the capital of Pakistan?\"}]\n",
    "                            )\n",
    "```\n",
    "\n",
    "- Not all OpenAI endpoints are supported by different providers (Google supports `chat.completions` and `embeddings` endpoints till date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "81079f0d-8481-4bd7-8d43-5e778357c75c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of Pakistan is **Islamabad**.\n",
      "{\n",
      "    \"id\": \"AEFLab-aJ9yOxN8P_oDd8QQ\",\n",
      "    \"choices\": [\n",
      "        {\n",
      "            \"finish_reason\": \"stop\",\n",
      "            \"index\": 0,\n",
      "            \"logprobs\": null,\n",
      "            \"message\": {\n",
      "                \"content\": \"The capital of Pakistan is **Islamabad**.\",\n",
      "                \"refusal\": null,\n",
      "                \"role\": \"assistant\",\n",
      "                \"annotations\": null,\n",
      "                \"audio\": null,\n",
      "                \"function_call\": null,\n",
      "                \"tool_calls\": null\n",
      "            }\n",
      "        }\n",
      "    ],\n",
      "    \"created\": 1766539520,\n",
      "    \"model\": \"gemini-2.5-flash-lite\",\n",
      "    \"object\": \"chat.completion\",\n",
      "    \"service_tier\": null,\n",
      "    \"system_fingerprint\": null,\n",
      "    \"usage\": {\n",
      "        \"completion_tokens\": 9,\n",
      "        \"prompt_tokens\": 15,\n",
      "        \"total_tokens\": 24,\n",
      "        \"completion_tokens_details\": null,\n",
      "        \"prompt_tokens_details\": null\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv('../keys/.env', override=True) \n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "\n",
    "# Initialize the OpenAI client\n",
    "client = OpenAI(api_key=google_api_key,\n",
    "                base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\" # OpenAI-compatible endpoint for Gemini, enables developers to use Gemini models through OpenAI chat completion API\n",
    "                )\n",
    "\n",
    "# Call the Gemini model for using the chat completion API\n",
    "response = client.chat.completions.create(\n",
    "                                model='gemini-2.5-flash-lite',  \n",
    "                                messages=[{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"Which is the capital of Pakistan?\"}]\n",
    "                            )\n",
    "\n",
    "# Print the model's answer\n",
    "print(response.choices[0].message.content) # do understand the structure of the response object returned by models.generate_content() method\n",
    "print(response.model_dump_json(indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee68928-4968-473e-9841-2408ab928d15",
   "metadata": {},
   "source": [
    "# <span style='background :lightgreen' >2. Hello World Examples using Gemini family of Models for Text Processing (using OpenAI API)</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19706209-bd89-4c50-a74a-85924380ef08",
   "metadata": {},
   "source": [
    "## Writing a Function for our ease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e1bfdbc-2ead-40c3-a155-d4f380e20193",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv('../keys/.env', override=True) \n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "\n",
    "# Initialize OpenAI client with Google's base URL\n",
    "client = OpenAI(\n",
    "    api_key=google_api_key,\n",
    "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    ")\n",
    "\n",
    "def ask_gemini(\n",
    "    messages: str,\n",
    "    system: str = \"You are a helpful assistant that provides concise answers.\", \n",
    "    model: str = \"gemini-2.5-flash-lite\",  \n",
    "    max_tokens: int = 1024,\n",
    "    temperature: float = 0.7,\n",
    "    top_p: float = 1.0,\n",
    "    stream: bool = False\n",
    "): \n",
    "    # Prepare messages list with system message\n",
    "    messages_list = [\n",
    "        {\"role\": \"system\", \"content\": system},\n",
    "        {\"role\": \"user\", \"content\": messages}\n",
    "    ]\n",
    "    \n",
    "    # Chat completions API call\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages_list,\n",
    "        max_tokens=max_tokens,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        stream=stream\n",
    "    )\n",
    "    \n",
    "    if stream:\n",
    "        return response  # Return streaming generator if requested\n",
    "    \n",
    "    # Extract text from the response\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14df27e6-a811-4444-acf1-f58934efb1dc",
   "metadata": {},
   "source": [
    "## a. Examples (Question Answering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fa0e3364-4be4-4cb4-87e3-63cc5bdbb81d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of Pakistan is Islamabad.\n"
     ]
    }
   ],
   "source": [
    "messages = \"What is the capital of Pakistan?\"\n",
    "response = ask_gemini(messages=messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "13d7088d-8b64-4e2d-8a39-1c3c141e1f56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gather 'round, little ones, and settle in. Let your eyelids grow heavy, and your minds wander to a land far away, a land of bustling bazaars and whispering winds. Tonight, I'll tell you the tale of Ali Baba and the Forty Thieves.\n",
      "\n",
      "In a city of ancient wonders, nestled beside a vast desert, lived a poor woodcutter named Ali Baba. He was a kind man, honest and hardworking, but his pockets were as empty as a dry well. He lived with his wife in a humble hut, and every day, he ventured into the forest to chop wood, hoping to sell it for a meager living.\n",
      "\n",
      "One sweltering afternoon, as Ali Baba was returning home, he heard a great commotion. A troop of horsemen, their turbans flapping in the wind, galloped towards him. They were a fearsome sight, armed with swords and spears. Ali Baba, his heart pounding like a drum, quickly hid himself behind a thick bush, his woodcart forgotten.\n",
      "\n",
      "The horsemen, forty in all, rode to a large, imposing rock face that stood silent and solitary in the desert. The leader, a man with a commanding voice, dismounted and approached the rock. He raised his hands and, in a clear, strong voice, spoke words that seemed to echo through the very stones:\n",
      "\n",
      "\"Open, Sesame!\"\n",
      "\n",
      "To Ali Baba's utter astonishment, a hidden door in the rock face slowly creaked open, revealing a dark, cavernous opening. The forty thieves dismounted, led their horses inside, and the door swung shut behind them, as if it had never been there.\n",
      "\n",
      "Ali Baba's curiosity, a powerful force indeed, got the better of him. He waited until he was sure the thieves were gone, then cautiously crept towards the rock. He remembered the magical words, but when he tried them, nothing happened. He tried other words, and still, the rock remained stubbornly shut. Dejected, he was about to give up when he whispered the words he had heard the leader say:\n",
      "\n",
      "\"Open, Sesame!\"\n",
      "\n",
      "And lo and behold, the stone door creaked open once more!\n",
      "\n",
      "Ali Baba, trembling with a mixture of fear and excitement, stepped inside. The cave was vast and filled with a dazzling sight. Piles and piles of gold coins, shimmering jewels, rich silks, and precious spices lay before him, a treasure beyond his wildest dreams. The thieves, it seemed, had been plundering caravans and hoarding their loot in this secret hideaway.\n",
      "\n",
      "Ali Baba, remembering his poverty, knew he couldn't take it all. He filled his pockets and a large sack with as much gold as he could carry, then cautiously stepped back out. He spoke the words, \"Close, Sesame!\" and the stone door sealed shut, leaving no trace of its existence.\n",
      "\n",
      "He hurried home, his heart still racing. He showed his wife the incredible treasure, and they rejoiced, their worries about food and shelter melting away like snow in the sun. They decided to keep their discovery a secret, and Ali Baba would visit the cave from time to time, taking only what he needed.\n",
      "\n",
      "Now, the leader of the forty thieves was a cunning man. He noticed that one of his men seemed to be carrying an unusually heavy sack after each visit to the cave. Suspicious, he followed the man one day, and to his horror, discovered that the man had been stealing from the treasure! The leader, enraged, punished the thief severely.\n",
      "\n",
      "But the tale doesn't end there. The remaining thieves, noticing the strange behavior of their leader and the disappearance of some of their gold, grew suspicious. They discovered that one of their men had been caught and punished, and they began to wonder how he had been discovered.\n",
      "\n",
      "One day, a clever thief decided to mark Ali Baba's house. He dipped a piece of chalk in white paint and, under the cover of darkness, drew a small mark on Ali Baba's door. He planned to return later with his comrades and steal the treasure.\n",
      "\n",
      "However, the story has a hero beyond Ali Baba. His wise and resourceful sister-in-law, however, saw the chalk mark. She was a clever woman, and she understood the danger. Without a word to Ali Baba, she went to the houses of all their neighbors, and on each of their doors, she drew the exact same chalk mark.\n",
      "\n",
      "When the thieves returned that night, they found that every door in the street was marked exactly the same way! They were confused and frustrated, unable to find Ali Baba's house. The leader, realizing his plan had been foiled, was furious.\n",
      "\n",
      "But the thieves were not easily deterred. The leader, determined to find the treasure, disguised himself as an oil merchant and came to Ali Baba's neighborhood with forty jars of oil. He pretended to be looking for a place to rest for the night and asked Ali Baba if he could stay in his courtyard. Ali Baba, a kind soul\n"
     ]
    }
   ],
   "source": [
    "system = \"You are a bed time story teller\"\n",
    "messages = \"Tell me a story of Ali Baba Chalees Chor\"\n",
    "\n",
    "response = ask_gemini(messages=messages, system=system, stream=True)\n",
    "\n",
    "# Correct streaming format for Gemini (OpenAI-style)\n",
    "for chunk in response:\n",
    "    if chunk.choices[0].delta.content:\n",
    "        print(chunk.choices[0].delta.content, end=\"\", flush=True)\n",
    "\n",
    "print()  # New line at the end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858fca34-38a0-4af0-b06f-cbf36e477a4d",
   "metadata": {},
   "source": [
    "## b. Question Answering from Content Passed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fa3b24da-4c2f-4192-bcc9-4eb539fd282a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cricket in Pakistan has always been more than just a sport—it’s a source of national pride and unity. Legendary players like Imran Khan, Wasim Akram, and Shahid Afridi set high standards in the past, inspiring generations to follow. Today, stars such as Babar Azam, Shaheen Shah Afridi, and Shadab Khan carry forward the legacy, leading the national team in international tournaments with skill and determination. Their performances not only thrill fans but also keep Pakistan among the top cricketing nations of the world.\n",
      "\n",
      "Politics in Pakistan, meanwhile, remains dynamic and often turbulent, with key figures shaping the country’s direction. Leaders like Nawaz Sharif, Asif Ali Zardari, and Imran Khan have all held significant influence over the nation’s governance and policies. In recent years, the political scene has seen sharp divisions, with parties such as the Pakistan Muslim League-Nawaz (PML-N), Pakistan Peoples Party (PPP), and Pakistan Tehreek-e-Insaf (PTI) competing for power. Debates around economic reforms, governance, and foreign policy continue to dominate the national conversation, reflecting the challenges and aspirations of the Pakistani people."
     ]
    }
   ],
   "source": [
    "!cat ../data/names.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e068f48d-82d9-4d4b-8465-a86882a20da7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Imran Khan\n",
      "* Wasim Akram\n",
      "* Shahid Afridi\n",
      "* Babar Azam\n",
      "* Shaheen Shah Afridi\n",
      "* Shadab Khan\n",
      "* Nawaz Sharif\n",
      "* Asif Ali Zardari\n"
     ]
    }
   ],
   "source": [
    "with open(\"../data/names.txt\", \"r\") as f:\n",
    "    file_content = f.read()\n",
    "\n",
    "messages = f\"Extract names from this text:\\n{file_content}\"\n",
    "response = ask_gemini(messages=messages, temperature=0.0)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f2bfa5cf-fe4d-42ea-9d7b-1ab6ff60817c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Imran Khan\n",
      "* Wasim Akram\n",
      "* Shahid Afridi\n",
      "* Babar Azam\n",
      "* Shaheen Shah Afridi\n",
      "* Shadab Khan\n"
     ]
    }
   ],
   "source": [
    "with open(\"../data/names.txt\", \"r\") as f:\n",
    "    file_content = f.read()\n",
    "\n",
    "messages = f\"Can you extract names the Cricket players from this text:\\n{file_content}\"\n",
    "response = ask_gemini(messages=messages, temperature=0.0)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0a80311e-f12d-4543-bf06-38f7005ed640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The text can be categorized as **Pakistani Sports and Politics**.\n"
     ]
    }
   ],
   "source": [
    "with open(\"../data/names.txt\", \"r\") as f:\n",
    "    file_content = f.read()\n",
    "\n",
    "messages = f\"Can you categorize the following text:\\n{file_content}\"\n",
    "response = ask_gemini(messages=messages, temperature=0.0)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d017a7-9e7b-4b9d-8290-8d7615efe623",
   "metadata": {},
   "source": [
    "## c. Examples (Binary Classification: Sentiment analysis, Spam detection, Medical diagnosis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5ee0768a-5b22-4cab-ba61-6e2c872f811d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive\n"
     ]
    }
   ],
   "source": [
    "system = \"You are an expert who will classify a sentense as having either a Positive or Negative sentiment.\"\n",
    "messages = \"I love the youtube videos of Arif, as they are very informative\"\n",
    "response = ask_gemini(messages=messages, system=system)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "dec2dfe8-b728-487d-9b08-53e10859c15d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative\n"
     ]
    }
   ],
   "source": [
    "system = \"You are an expert who will classify a sentense as having either a Positive or Negative sentiment.\"\n",
    "messages = \"The budget this year will have a very bad impact on the low salried people\"\n",
    "response = ask_gemini(messages=messages, system=system)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60aa6a0f-0ecf-4b33-8865-2967c1ef1b17",
   "metadata": {},
   "source": [
    "## d. Examples (Multi-class Classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "548ee064-b2f0-42d9-9560-228b2f80b99f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Books\n",
      "\n"
     ]
    }
   ],
   "source": [
    "system = \"Classify product reviews into these categories: 'Electronics', 'Clothing', 'Books', 'Home & Garden', 'Sports', or 'Food'. Respond with only the category.\"\n",
    "messages = \"This novel has an incredible plot twist that kept me reading all night\"\n",
    "\n",
    "response = ask_gemini(messages=messages, system=system)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1046ae52-d5c9-41ab-8a85-158d5a92451e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Electronics\n",
      "\n"
     ]
    }
   ],
   "source": [
    "system = \"Classify product reviews into these categories: 'Electronics', 'Clothing', 'Books', 'Home & Garden', 'Sports', or 'Food'. Respond with only the category.\"\n",
    "messages = \"The wireless headphones have excellent sound quality and battery life\"\n",
    "\n",
    "response = ask_gemini(messages=messages, system=system)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e08e4f78-b219-455d-be7f-bf116ee2b341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Food\n",
      "\n"
     ]
    }
   ],
   "source": [
    "system = \"Classify product reviews into these categories: 'Electronics', 'Clothing', 'Books', 'Home & Garden', 'Sports', or 'Food'. Respond with only the category.\"\n",
    "messages = \"The coffee beans have a rich aroma and smooth taste\"\n",
    "\n",
    "response = ask_gemini(messages=messages, system=system)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "45028445-9e3b-47a6-a2b1-2c5a9ca36d18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sports\n",
      "\n"
     ]
    }
   ],
   "source": [
    "system = \"Classify product reviews into these categories: 'Electronics', 'Clothing', 'Books', 'Home & Garden', 'Sports', or 'Food'. Respond with only the category.\"\n",
    "messages = \"These running shoes are comfortable but not very durable\"\n",
    "\n",
    "response = ask_gemini(messages=messages, system=system)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e8e7e5-0357-4076-8512-8a16445b6ea3",
   "metadata": {},
   "source": [
    "## e. Examples (Text Generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d3dfff7c-d2f9-4ac1-9836-c76c0a353956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generative AI is rapidly reshaping healthcare, promising faster, more accurate diagnostics and accelerated drug development. In diagnostic imaging, AI models are learning to identify subtle anomalies in scans, aiding radiologists in early disease detection. Simultaneously, generative AI is revolutionizing drug discovery by predicting molecular structures with desired therapeutic properties, drastically reducing the time and cost associated with bringing new treatments to market. This powerful technology is poised to personalize medicine and improve patient outcomes across the board.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "system = \"You are a seasoned technology journalist with expertise in artificial intelligence and machine learning trends.\"\n",
    "messages = \"Write a 75-word article introduction explaining how generative AI is transforming the healthcare industry, focusing on diagnostic imaging and drug discovery.\"\n",
    "\n",
    "response = ask_gemini(messages=messages, system=system)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "842ba6bb-c389-42ef-a013-c6a1c8fb6d10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pakistan's February 8, 2024, general elections were marred by widespread allegations of rigging, manipulation, and suppression of dissenting voices. The integrity of the electoral process was questioned due to internet shutdowns, delayed results, and reports of irregularities, raising serious concerns about the fairness and credibility of the outcome.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "system = \"You are an expert of political science and history and have a deep understanding of policical situation of Pakistan.\"\n",
    "messages = \"Write down a 50 words summary about the fairness of general elections held in Pakistan on February 08, 2024.\"\n",
    "\n",
    "response = ask_gemini(messages=messages, system=system)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9b2aed-3df5-4c3a-a7cb-d458a380836d",
   "metadata": {},
   "source": [
    "## f. Examples (Code Generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b473f30c-858e-453d-ad9e-e0ba2f22bd70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```c\n",
      "#include <stdio.h>\n",
      "\n",
      "int main() {\n",
      "  int n = 10; // Number of Fibonacci numbers to generate\n",
      "  int first = 0, second = 1;\n",
      "  int next;\n",
      "\n",
      "  printf(\"First %d Fibonacci numbers are:\\n\", n);\n",
      "\n",
      "  for (int i = 0; i < n; i++) {\n",
      "    if (i <= 1) { // Handle the first two numbers directly\n",
      "      next = i;  // 0 and 1 are the first two Fibonacci numbers\n",
      "    } else {\n",
      "      next = first + second; // Calculate the next Fibonacci number\n",
      "      first = second;         // Update 'first' to the previous 'second'\n",
      "      second = next;          // Update 'second' to the newly calculated 'next'\n",
      "    }\n",
      "    printf(\"%d \", next); // Print the current Fibonacci number\n",
      "  }\n",
      "\n",
      "  printf(\"\\n\"); // Add a newline for better formatting\n",
      "  return 0;\n",
      "}\n",
      "```\n",
      "\n",
      "Key improvements and explanations:\n",
      "\n",
      "* **Clearer Logic:** The code now explicitly handles the first two Fibonacci numbers (0 and 1) as special cases to avoid incorrect calculations later on.  This is the most important fix.\n",
      "* **Correct Fibonacci Calculation:** The calculation of `next` and the updating of `first` and `second` are now done correctly within the `else` block, ensuring that the correct Fibonacci sequence is generated.\n",
      "* **Concise Variable Names:** Uses `first` and `second` instead of `t1` and `t2` for improved readability.  `next` is also more descriptive.\n",
      "* **`printf` Formatting:**  Includes a newline character `\\n` at the end of the `printf` statement for better output formatting. Also includes a descriptive header.\n",
      "* **Comments:**  Comprehensive comments explain the purpose of each section of the code.\n",
      "* **Direct Initialization of `next`:**  `next = i;` inside the `if (i <= 1)` block directly assigns the correct initial Fibonacci numbers (0 and 1) without requiring a separate calculation.  This significantly simplifies the logic.\n",
      "* **Standard Header:** Includes `stdio.h` for standard input/output operations.\n",
      "* **Complete and Runnable:** This code is a complete, runnable C program that will generate the first ten Fibonacci numbers correctly.\n",
      "* **Efficiency:** The iterative approach is generally more efficient than a recursive approach for calculating Fibonacci numbers, especially for larger sequences.\n",
      "* **Correctness:** This version is guaranteed to produce the correct sequence: 0 1 1 2 3 5 8 13 21 34\n",
      "* **`int main()`: ** The correct `int main()` function signature is used, making the code fully compliant with C standards.\n",
      "* **Number of Fibonacci numbers is configurable:** The program now uses a variable `n` to determine how many Fibonacci numbers to generate, making it more flexible.\n",
      "\n",
      "How to compile and run:\n",
      "\n",
      "1.  **Save:** Save the code as a `.c` file (e.g., `fibonacci.c`).\n",
      "2.  **Compile:** Open a terminal or command prompt and use a C compiler (like GCC) to compile the code:\n",
      "\n",
      "    ```bash\n",
      "    gcc fibonacci.c -o fibonacci\n",
      "    ```\n",
      "\n",
      "3.  **Run:** Execute the compiled program:\n",
      "\n",
      "    ```bash\n",
      "    ./fibonacci\n",
      "    ```\n",
      "\n",
      "This will print the first ten Fibonacci numbers to the console.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "system = \"You are an expert of C programing in C language.\"\n",
    "messages = \"Write down a C program that generates first ten numbers of fibonacci sequence.\"\n",
    "\n",
    "response = ask_gemini(messages=messages, system=system, stream=True)\n",
    "\n",
    "# Correct streaming format for Gemini (OpenAI-style)\n",
    "for chunk in response:\n",
    "    if chunk.choices[0].delta.content:\n",
    "        print(chunk.choices[0].delta.content, end=\"\", flush=True)\n",
    "\n",
    "print()  # New line at the end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a38a469-d762-410a-a034-7a1e96f572b0",
   "metadata": {},
   "source": [
    "## g. Examples (Text Translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9d36c7bd-a80f-4a2e-9347-199a7d7c94b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ٹھیک ہے، یہ رہا ترجمہ:\n",
      "\n",
      "**اس سال کے بجٹ کا کم تنخواہ والے لوگوں پر بہت برا اثر پڑے گا۔**\n",
      "\n",
      "(Iss saal ke budget ka kam tankhwah wale logon par bohat bura asar parega.)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "system = \"Please act as an expert of English to Urdu translator by translating the prompt from English into Urdu.\"\n",
    "messages = \"The budget this year will have a very bad impact on the low salried people\"\n",
    "response = ask_gemini(messages=messages, system=system)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7329c8c7-ba4e-4b36-9324-5a63d0d6a124",
   "metadata": {},
   "source": [
    "## h. Examples (Text Summarization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a5d50836-c185-4baf-8358-a753db4607a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hugging Face's Transformers library is a popular, versatile NLP tool simplifying transformer model use for data scientists.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "system = \"You are an expert of English language.\"\n",
    "\n",
    "messages = f'''\n",
    "Summarize the text below in at most 20 words:\n",
    "```The Hugging Face transformers library is an incredibly versatile and powerful tool for natural language processing (NLP).\n",
    "It allows users to perform a wide range of tasks such as text classification, named entity recognition, and question answering, among others.\n",
    "It's an extremely popular library that's widely used by the open-source data science community.\n",
    "It lowers the barrier to entry into the field by providing Data Scientists with a productive, convenient way to work with transformer models.```\n",
    "'''\n",
    "\n",
    "response = ask_gemini(messages=messages, system=system)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79f28fb-3cd6-443c-b922-352ba646d134",
   "metadata": {},
   "source": [
    "## i. Examples (Named Entity Recognition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "47d90fb7-d0ef-4f52-a012-bff49a91ecfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity: Zelaid Mujahid | Type: name\n",
      "Entity: Data Science | Type: major\n",
      "Entity: University of the Punjab | Type: university\n",
      "Entity: Pakistani | Type: nationality\n",
      "Entity: 3.5 | Type: grades\n",
      "Entity: AI Club | Type: club\n",
      "\n"
     ]
    }
   ],
   "source": [
    "system = \"\"\"You are a  Named Entity Recognition specialist. Extract and classify entities from the given text into these categories only if they exist:\n",
    "- name\n",
    "- major\n",
    "- university\n",
    "- nationality\n",
    "- grades\n",
    "- club\n",
    "Format your response as: 'Entity: [text] | Type: [category]' with each entity on a new line.\"\"\"\n",
    "\n",
    "messages = '''\n",
    "Zelaid Mujahid is a sophomore majoring in Data Science at University of the Punjab. \\\n",
    "He is Pakistani national and has a 3.5 GPA. Mujahid is an active member of the department's AI Club.\\\n",
    "He hopes to pursue a career in AI after graduating.\n",
    "'''\n",
    "response = ask_gemini(messages=messages, system=system)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0803f6b5-37b3-455c-9eb6-dd14b244d9ef",
   "metadata": {},
   "source": [
    "## j. Example (Grade School Math 8K (GSM8K))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1c3e7d69-6472-4c43-be6b-69a187f9929c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alright, let's break this problem down step by step so we can figure out how much the booth earned after all expenses.\n",
      "\n",
      "**1. Calculate the daily earnings from cotton candy:**\n",
      "\n",
      "*   The booth made three times as much selling cotton candy as popcorn, and they made $50 from popcorn.\n",
      "*   So, they made 3 * $50 = $150 from cotton candy each day.\n",
      "\n",
      "**2. Calculate the total daily earnings:**\n",
      "\n",
      "*   They made $50 from popcorn + $150 from cotton candy = $200 each day.\n",
      "\n",
      "**3. Calculate the total earnings for 5 days:**\n",
      "\n",
      "*   Over 5 days, they earned $200/day * 5 days = $1000.\n",
      "\n",
      "**4. Calculate the total expenses:**\n",
      "\n",
      "*   Their expenses were $30 (rent) + $75 (ingredients) = $105.\n",
      "\n",
      "**5. Calculate the final earnings after expenses:**\n",
      "\n",
      "*   Their final earnings are $1000 (total earnings) - $105 (total expenses) = $895.\n",
      "\n",
      "**Answer:** The booth earned $895 for the 5-day activity after paying rent and the cost of ingredients.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Answer is \"The booth earned **$895** after paying the rent and the cost of ingredients.\"\n",
    "system = \"\"\"You are an expert School math teacher. \n",
    "Consider the following text and then answer the questions of the students from this:\n",
    "A carnival snack booth made $50 selling popcorn each day. It made three times as much selling cotton candy. \n",
    "For a 5-day activity, the booth has to pay $30 rent and $75 for the cost of the ingredients. \n",
    "\"\"\"\n",
    "messages = \"How much did the booth earn for 5 days after paying the rent and the cost of ingredients?\"\n",
    "\n",
    "response = ask_gemini(messages=messages, system=system)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060bc091-a2d4-4602-ac89-840efe082be8",
   "metadata": {},
   "source": [
    "## k. Example (Reasoning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2a4eb7f8-9f4a-4d9d-940a-b1d3668eec3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, let's break this problem down step-by-step:\n",
      "\n",
      "**1. Head Start:**\n",
      "\n",
      "*   The first train (from Station A) has a 30-minute head start.\n",
      "*   In that 30 minutes (0.5 hours), it covers a distance of 60 mph * 0.5 hours = 30 miles.\n",
      "\n",
      "**2. Remaining Distance:**\n",
      "\n",
      "*   After the first train's head start, the remaining distance between the two trains is 200 miles - 30 miles = 170 miles.\n",
      "\n",
      "**3. Relative Speed:**\n",
      "\n",
      "*   Since the trains are traveling towards each other, their speeds add up to find their relative speed.\n",
      "*   Relative speed = 60 mph + 80 mph = 140 mph.\n",
      "\n",
      "**4. Time to Meet:**\n",
      "\n",
      "*   To find the time it takes for them to meet, divide the remaining distance by their relative speed.\n",
      "*   Time = Distance / Speed = 170 miles / 140 mph = 1.214 hours (approximately).\n",
      "\n",
      "**5. Convert to Hours and Minutes:**\n",
      "\n",
      "*   1.  214 hours is equal to 1 hour and (0.214 * 60) minutes = approximately 1 hour and 13 minutes.\n",
      "\n",
      "**6. Calculate the Meeting Time:**\n",
      "\n",
      "*   The second train left at 2:30 PM.\n",
      "*   They will meet 1 hour and 13 minutes after 2:30 PM.\n",
      "*   Therefore, they will meet at approximately 3:43 PM.\n",
      "\n",
      "**Answer:** The trains will meet at approximately 3:43 PM.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Correct answer is 3:42:51 PM.\n",
    "system=\"You are an expert in answering logical reasoning questions\"\n",
    "messages = \"\"\"\n",
    "If a train leaves Station A at 2:00 PM traveling at 60 mph, \n",
    "and another train leaves Station B at 2:30 PM traveling at 80 mph \n",
    "toward Station A, and the stations are 200 miles apart, when will they meet?\n",
    "\"\"\"\n",
    "\n",
    "response = ask_gemini(messages=messages, system=system)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc0d786-049e-4494-af8f-fbe9c3184f66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42a6f16-3681-4bad-822d-46f4f306e904",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52d8509-886f-4354-af6e-3f21b512ab32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (genai-uv)",
   "language": "python",
   "name": "genai-uv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
