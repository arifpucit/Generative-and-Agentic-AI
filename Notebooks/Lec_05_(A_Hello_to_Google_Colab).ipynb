{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a41c76e0",
   "metadata": {
    "id": "a41c76e0"
   },
   "source": [
    "---   \n",
    " <img align=\"left\" width=\"75\" height=\"75\"  src=\"https://upload.wikimedia.org/wikipedia/en/c/c8/University_of_the_Punjab_logo.png\">\n",
    "\n",
    "<h1 align=\"center\">Department of Data Science</h1>\n",
    "\n",
    "---\n",
    "<h3><div align=\"right\">Instructor: Muhammad Arif Butt, Ph.D.</div></h3>    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b2fd9c-b3fd-42d1-b6de-4ecf48e89d36",
   "metadata": {
    "id": "a4b2fd9c-b3fd-42d1-b6de-4ecf48e89d36"
   },
   "source": [
    "<h1 align=\"center\">Lec-05: A Hello to Google Colab</h1>\n",
    "\n",
    "<div align=\"center\">\n",
    "  <a href=\"https://colab.research.google.com/github/arifpucit/Generative-and-Agentic-AI/blob/main/Notebooks/Lec_03_(A_Hello_to_Google_Colab).ipynb\" target=\"_blank\">\n",
    "    <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open in Colab\">\n",
    "  </a>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef15071",
   "metadata": {
    "id": "7ef15071"
   },
   "source": [
    "# Learning agenda of this notebook\n",
    "1. An Overview of GPU Platforms for LLM Training\n",
    "2. An Overview of Google Colab\n",
    "    - Free vs Paid Tiers\n",
    "    - What you can do on Google Colab?\n",
    "    - Accessing Google Colab\n",
    "3. Saving Your API Keys and Accessing OpenAI Models in Google Colab\n",
    "4. Accessing a Notebook File from a Public GitHub Repository in Google Colab\n",
    "5. Accessing a Notebook File from your Google Drive in Google Colab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exT6bcV_cEBN",
   "metadata": {
    "id": "exT6bcV_cEBN"
   },
   "source": [
    "# 1. An Overview of GPU Platforms for LLM Training\n",
    "\n",
    "| Category        | [Google Colab](https://colab.research.google.com) | [RunPod](https://www.runpod.io) | [Vast.ai](https://vast.ai) | [Lambda Labs](https://lambdalabs.com) | [Google Cloud](https://cloud.google.com) | [AWS](https://aws.amazon.com) |\n",
    "|-----------------|--------------------------------------------------|--------------------------------|----------------------------|--------------------------------------|------------------------------------------|--------------------------------|\n",
    "| **Description** | Free Jupyter Notebook service with GPU/TPU access. Great for students & beginners. **Gemini** integration enables features like “Generate Code,” “Explain Error,” “Chat in Notebook.” | On-demand GPU cloud with flexible hourly pricing. | Peer-to-peer GPU rentals with very low costs. | Premium GPU provider optimized for ML projects. | Enterprise cloud with scalable GPU/TPU options. | Industry-standard cloud with wide ML ecosystem. |\n",
    "| **Pricing (2025)** | Free tier (T4/P100, limited); Pro: \\\\\\$9.99/mo; Pro+: \\\\\\$49.99/mo | RTX 4090 (24 GB RAM): \\\\\\$0.34/hr; H100: \\\\\\$1.99/hr; A100: ~\\\\\\$2.25/hr | Spot GPUs from \\\\\\$0.05/hr (consumer) up to ~\\\\\\$2/hr (A100/H100) | RTX 4090: \\\\\\$0.60/hr; A100: \\\\\\$2.5–3.5/hr | A100: ~\\\\\\$2.5/hr; H100: ~\\\\\\$4–5/hr (spot up to 90% off) | A100: ~\\\\\\$3.1/hr; H100: ~\\\\\\$6–7/hr (spot cheaper) |\n",
    "| **Key Features** | ML libs pre-installed, Google Drive sync, collaborative, no setup | Multiple GPU types, Docker templates, persistent storage, scaling | Consumer + pro GPUs, flexible rentals, community-driven pricing | Optimized ML stack, high uptime, professional support | Global infra, TPU/GPU access, Vertex AI, GCP services | Broad GPU range, SageMaker ML, enterprise-grade security |\n",
    "| **Pros ✅**     | Free tier, quick start, beginner-friendly, good for tutorials | Low hourly rates, easy to scale, user-friendly | Cheapest option, large GPU variety, very flexible | Reliable, ML-optimized envs, pro support | Enterprise reliability, global infra, advanced ML services | Mature ecosystem, best docs, wide integrations |\n",
    "| **Cons ❌**     | Limited runtime, GPU shortages, not for production | No free tier, setup needed, costs add up hourly | Variable reliability, limited support, setup overhead | Expensive, overkill for beginners, best for pros | Complex pricing, steep learning curve, costly small-scale | Highest costs, complex billing, steep learning curve |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "VporK6Hib_j4",
   "metadata": {
    "id": "VporK6Hib_j4"
   },
   "source": [
    "# 2. An Overview of Google Colab\n",
    "<h3 align=\"center\"><div class=\"alert alert-success\" style=\"margin: 20px\">Google Colab is a free, cloud-based Python and machine-learning workspace (accessible with any Google account) that provides a ready-to-use environment with CPUs, ~12–13 GB RAM, and access to GPUs like NVIDIA K80 when available, along with preinstalled libraries such as Keras, PyTorch, TensorFlow, NumPy, and more.</h3>\n",
    "\n",
    "\n",
    "## a. Free vs Paid Tiers\n",
    "- On free tier, Google colab provides following:\n",
    "    - CPU (always available)\n",
    "    - GPU (limited, usually NVIDIA T4)\n",
    "    - TPU v2 / v3 (time-limited)\n",
    "- Other than the free tier (limits—lower-tier hardware and shorter sessions), Colab has paid versions: https://colab.research.google.com/signup\n",
    "  - **Pay as You Go:** (\\\\$9.99 for 100 compute units, each unit cost \\\\$0.1). Compute units are special credits used to run premium GPUs (T4, A100, L4), TPUs (v2-8, v5e-1, v6e-1), and or high RAM. Unused compute units will expire after 90 days.\n",
    "  - **Colab Pro** (\\$9.99 per month)\n",
    "  - **Colab Pro+** (\\$49.99 per month)\n",
    "  - **Colab Enterprise** (Pay for what you use)\n",
    "- As a thumb rule the cheapest T4 GPU will cost your around \\$0.2 per hour usage.\n",
    "\n",
    ">- **GPUs** (Graphics Processing Units) are primarily used for Graphics rendering, but now we use them for General-purpose parallel compute. A GPU may have thousands of small cores and are excellent at SIMD-style parallelism. Some examples are NVIDIA (T4, V100, A100, H100) and AMD (MI250). They works well for training, fine-tuning, inference tasks. However, they use high power and are not purpose-built for matrix math only.\n",
    ">- **TPUs** (Tensor Processing Units) are designed by Google specifically for ML. Some examples are TPU v2, v3, v4, v5e. They are excellent for large-scale training, however, have limited support for custom CUDA ops.\n",
    ">- **LPUs** (Language Processing Units) are designed specifically for LLM inference by Groq (Groq LPU). They are mostly used for inference and not suitable for training or fine-tuning.\n",
    "\n",
    "## b. What you can do on Google Colab?\n",
    "- Create, upload, download, and share Jupyter notebooks entirely in the browser.\n",
    "- Import notebooks from Google Drive and save your work back to Drive automatically.\n",
    "- Load notebooks from GitHub and publish your own notebooks back to your GitHub repositories.\n",
    "- Install and use any Python library (PyTorch, TensorFlow, Transformers, etc.) instantly.\n",
    "- Import datasets from Kaggle or any external URL for training or analysis.\n",
    "- Hugging Face Integration Inside Colab\n",
    "    - Access the Hugging Face Hub directly using huggingface_hub commands.\n",
    "    - Clone, test, or create Hugging Face Spaces (Streamlit, Gradio, FastAPI apps) from Colab.\n",
    "    - Download pretrained models for inference, fine-tuning, or experimentation.\n",
    "    - Upload your own models (LoRA, full models, tokenizers) to your Hugging Face profile.\n",
    "    - Load Hugging Face datasets instantly for NLP, vision, and multimodal tasks.\n",
    "    - Push training outputs (models / logs / datasets) to Hugging Face with a single command.\n",
    "\n",
    "## c. Accessing Google Colab\n",
    "- **Open Colab directly** by visiting https://colab.research.google.com/ and sign in with your Google account. (There is no separate Colab account)\n",
    "- **Open Colab through Google Drive:** Go to https://drive.google.com/ → New → More → Google Colaboratory\n",
    "\n",
    "<br><br>\n",
    "**Note:** In the middle of running a Colab, you might get an Runtime error saying CUDA is required but not available for bitsandbytes.On free tear, this actually happens because Google has switched out your Colab runtime, perhaps because Google Colab was too busy. The solution is:\n",
    ">- Kernel menu >> Disconnect and delete runtime\n",
    ">- Reload the colab from fresh and Edit menu >> Clear All Outputs\n",
    ">- Connect to a new T4 using the button at the top right\n",
    ">- Select \"View resources\" from the menu on the top right to confirm you have a GPU\n",
    ">- Rerun the cells in the colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3885f82-b402-40fa-83a4-fdb2999227be",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a3885f82-b402-40fa-83a4-fdb2999227be",
    "outputId": "d7d1acc6-f2d6-4eb8-fe34-5621f62b5fa2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "HARDWARE SPECIFICATIONS\n",
      "============================================================\n",
      "\n",
      "Operating System: Linux 6.6.105+\n",
      "Platform: Linux-6.6.105+-x86_64-with-glibc2.35\n",
      "\n",
      "============================================================\n",
      "CPU INFORMATION\n",
      "============================================================\n",
      "Processor: x86_64\n",
      "Machine:   x86_64\n",
      "Physical Cores: 1\n",
      "Logical Cores:  2\n",
      "Max Frequency:  0.00 MHz\n",
      "Current Freq:   2200.00 MHz\n",
      "\n",
      "============================================================\n",
      "RAM INFORMATION\n",
      "============================================================\n",
      "Total RAM:     12.67 GB\n",
      "Available RAM: 11.68 GB\n",
      "Used RAM:      0.68 GB\n",
      "RAM Usage:     7.8%\n",
      "\n",
      "============================================================\n",
      "GPU INFORMATION\n",
      "============================================================\n",
      "\n",
      "GPU #1:\n",
      "  Vendor: NVIDIA\n",
      "  Name:   Tesla T4\n",
      "  Memory: 15360 MiB\n",
      "  Driver: 550.54.15\n",
      "  Temp:   56°C\n"
     ]
    }
   ],
   "source": [
    "import platform\n",
    "import subprocess\n",
    "import psutil\n",
    "import re\n",
    "\n",
    "def get_gpu_info():\n",
    "    \"\"\"Detect and return information about available GPUs\"\"\"\n",
    "    gpus = []\n",
    "    os_type = platform.system()\n",
    "\n",
    "    # Try NVIDIA GPUs\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            [\"nvidia-smi\", \"--query-gpu=name,memory.total,driver_version,temperature.gpu\",\n",
    "             \"--format=csv,noheader\"],\n",
    "            stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True, timeout=5\n",
    "        )\n",
    "        if result.returncode == 0 and result.stdout.strip():\n",
    "            for line in result.stdout.strip().split('\\n'):\n",
    "                parts = [p.strip() for p in line.split(',')]\n",
    "                if len(parts) >= 2:\n",
    "                    gpus.append({\n",
    "                        'vendor': 'NVIDIA',\n",
    "                        'name': parts[0],\n",
    "                        'memory': parts[1] if len(parts) > 1 else 'N/A',\n",
    "                        'driver': parts[2] if len(parts) > 2 else 'N/A',\n",
    "                        'temp': parts[3] if len(parts) > 3 else 'N/A'\n",
    "                    })\n",
    "    except (FileNotFoundError, subprocess.TimeoutExpired):\n",
    "        pass\n",
    "\n",
    "    # Try AMD GPUs (Linux)\n",
    "    if os_type == \"Linux\":\n",
    "        try:\n",
    "            result = subprocess.run(\n",
    "                [\"rocm-smi\", \"--showproductname\"],\n",
    "                stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True, timeout=5\n",
    "            )\n",
    "            if result.returncode == 0 and result.stdout.strip():\n",
    "                lines = result.stdout.strip().split('\\n')\n",
    "                for line in lines:\n",
    "                    if 'GPU' in line or 'Card series' in line:\n",
    "                        name = re.sub(r'GPU\\[\\d+\\]\\s*:\\s*', '', line).strip()\n",
    "                        if name and 'Card series' not in name:\n",
    "                            gpus.append({'vendor': 'AMD', 'name': name})\n",
    "        except (FileNotFoundError, subprocess.TimeoutExpired):\n",
    "            pass\n",
    "\n",
    "        # Alternative: Check via lspci\n",
    "        try:\n",
    "            result = subprocess.run(\n",
    "                [\"lspci\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True, timeout=5\n",
    "            )\n",
    "            if result.returncode == 0:\n",
    "                for line in result.stdout.split('\\n'):\n",
    "                    if 'VGA compatible controller' in line or 'Display controller' in line:\n",
    "                        match = re.search(r':\\s*(.+)$', line)\n",
    "                        if match:\n",
    "                            gpu_name = match.group(1).strip()\n",
    "                            # Only add if not already detected by vendor-specific tools\n",
    "                            if not any(gpu['name'] in gpu_name for gpu in gpus):\n",
    "                                if 'AMD' in gpu_name or 'ATI' in gpu_name:\n",
    "                                    gpus.append({'vendor': 'AMD', 'name': gpu_name})\n",
    "                                elif 'Intel' in gpu_name:\n",
    "                                    gpus.append({'vendor': 'Intel', 'name': gpu_name})\n",
    "                                elif 'NVIDIA' not in gpu_name:  # Catch other vendors\n",
    "                                    gpus.append({'vendor': 'Unknown', 'name': gpu_name})\n",
    "        except (FileNotFoundError, subprocess.TimeoutExpired):\n",
    "            pass\n",
    "\n",
    "    # Windows detection\n",
    "    elif os_type == \"Windows\":\n",
    "        try:\n",
    "            result = subprocess.run(\n",
    "                [\"wmic\", \"path\", \"win32_VideoController\", \"get\", \"name,AdapterRAM,DriverVersion\"],\n",
    "                stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True, timeout=5\n",
    "            )\n",
    "            if result.returncode == 0:\n",
    "                lines = result.stdout.strip().split('\\n')[1:]  # Skip header\n",
    "                for line in lines:\n",
    "                    line = line.strip()\n",
    "                    if line:\n",
    "                        parts = line.split()\n",
    "                        if len(parts) >= 2:\n",
    "                            name = ' '.join(parts[:-2]) if len(parts) > 2 else parts[0]\n",
    "                            ram = parts[-2] if len(parts) > 2 else 'N/A'\n",
    "                            driver = parts[-1] if len(parts) > 1 else 'N/A'\n",
    "\n",
    "                            # Convert RAM from bytes to GB\n",
    "                            try:\n",
    "                                ram_gb = f\"{int(ram) / (1024**3):.2f} GB\"\n",
    "                            except:\n",
    "                                ram_gb = 'N/A'\n",
    "\n",
    "                            vendor = 'Unknown'\n",
    "                            if 'NVIDIA' in name.upper():\n",
    "                                vendor = 'NVIDIA'\n",
    "                            elif 'AMD' in name.upper() or 'ATI' in name.upper():\n",
    "                                vendor = 'AMD'\n",
    "                            elif 'INTEL' in name.upper():\n",
    "                                vendor = 'Intel'\n",
    "\n",
    "                            gpus.append({\n",
    "                                'vendor': vendor,\n",
    "                                'name': name,\n",
    "                                'memory': ram_gb,\n",
    "                                'driver': driver\n",
    "                            })\n",
    "        except (FileNotFoundError, subprocess.TimeoutExpired):\n",
    "            pass\n",
    "\n",
    "    # macOS detection\n",
    "    elif os_type == \"Darwin\":\n",
    "        try:\n",
    "            result = subprocess.run(\n",
    "                [\"system_profiler\", \"SPDisplaysDataType\"],\n",
    "                stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True, timeout=5\n",
    "            )\n",
    "            if result.returncode == 0:\n",
    "                lines = result.stdout.split('\\n')\n",
    "                current_gpu = {}\n",
    "                for line in lines:\n",
    "                    if 'Chipset Model:' in line:\n",
    "                        name = line.split(':', 1)[1].strip()\n",
    "                        current_gpu['name'] = name\n",
    "                        if 'AMD' in name or 'ATI' in name:\n",
    "                            current_gpu['vendor'] = 'AMD'\n",
    "                        elif 'Intel' in name:\n",
    "                            current_gpu['vendor'] = 'Intel'\n",
    "                        elif 'Apple' in name:\n",
    "                            current_gpu['vendor'] = 'Apple'\n",
    "                        else:\n",
    "                            current_gpu['vendor'] = 'Unknown'\n",
    "                    elif 'VRAM' in line and current_gpu:\n",
    "                        current_gpu['memory'] = line.split(':', 1)[1].strip()\n",
    "                        gpus.append(current_gpu.copy())\n",
    "                        current_gpu = {}\n",
    "        except (FileNotFoundError, subprocess.TimeoutExpired):\n",
    "            pass\n",
    "\n",
    "    return gpus\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# SYSTEM INFORMATION\n",
    "# ---------------------------------------------------------\n",
    "print(\"=\" * 60)\n",
    "print(\"HARDWARE SPECIFICATIONS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nOperating System: {platform.system()} {platform.release()}\")\n",
    "print(f\"Platform: {platform.platform()}\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# CPU INFORMATION\n",
    "# ---------------------------------------------------------\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"CPU INFORMATION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Processor: {platform.processor()}\")\n",
    "print(f\"Machine:   {platform.machine()}\")\n",
    "print(f\"Physical Cores: {psutil.cpu_count(logical=False)}\")\n",
    "print(f\"Logical Cores:  {psutil.cpu_count(logical=True)}\")\n",
    "\n",
    "# CPU Frequency\n",
    "cpu_freq = psutil.cpu_freq()\n",
    "if cpu_freq:\n",
    "    print(f\"Max Frequency:  {cpu_freq.max:.2f} MHz\")\n",
    "    print(f\"Current Freq:   {cpu_freq.current:.2f} MHz\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# RAM INFORMATION\n",
    "# ---------------------------------------------------------\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"RAM INFORMATION\")\n",
    "print(\"=\" * 60)\n",
    "ram = psutil.virtual_memory()\n",
    "print(f\"Total RAM:     {ram.total / (1024**3):.2f} GB\")\n",
    "print(f\"Available RAM: {ram.available / (1024**3):.2f} GB\")\n",
    "print(f\"Used RAM:      {ram.used / (1024**3):.2f} GB\")\n",
    "print(f\"RAM Usage:     {ram.percent}%\")\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# GPU INFORMATION\n",
    "# ---------------------------------------------------------\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"GPU INFORMATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "gpus = get_gpu_info()\n",
    "if gpus:\n",
    "    for idx, gpu in enumerate(gpus, 1):\n",
    "        print(f\"\\nGPU #{idx}:\")\n",
    "        print(f\"  Vendor: {gpu['vendor']}\")\n",
    "        print(f\"  Name:   {gpu['name']}\")\n",
    "        if 'memory' in gpu:\n",
    "            print(f\"  Memory: {gpu['memory']}\")\n",
    "        if 'driver' in gpu:\n",
    "            print(f\"  Driver: {gpu['driver']}\")\n",
    "        if 'temp' in gpu and gpu['temp'] != 'N/A':\n",
    "            print(f\"  Temp:   {gpu['temp']}°C\")\n",
    "else:\n",
    "    print(\"No GPU detected or unable to retrieve GPU information.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e-qrU-2E8Anq",
   "metadata": {
    "id": "e-qrU-2E8Anq"
   },
   "source": [
    "# 3. Saving Your API Keys and Accessing OpenAI Models in Google Colab\n",
    "- **Where to Save the API Keys when working in Google Colab?**\n",
    "- Never directly copy paste the secret key inside the code cell, if you do so then make sure that you donot share the notebook with anyone.\n",
    "- When working inside Google Colab, save your secrets by click the key icon on the side panel on the left and add all your secret keys over there, e.g., the OpenAI API key.\n",
    "    - In the name field put `OPENAI_API_KEY`\n",
    "    - In the value field put your actual token: `sk-proj-xyz...`\n",
    "    - Ensure the notebook access switch is turned ON.\n",
    "- API Keys in Colab “Secrets” / Environment Variables only lives in RAM of that runtime, or your browser session or your account secrets storage (if using the UI).\n",
    "- Anyone opening/downloading your notebook (hosted in your public GitHub repo or your Google Drive) can see the code, markdown, outputs and notebook metadata.\n",
    "- They CANNOT see environment variables created at runtime and secrets stored through Colab UI\n",
    "- Before running the following code cell (that accesses the  secret `OPEN_API_KEY`, you must first set your own OpenAI API Key as mentioned above. You'll need to do this on each of the course colab notebook files.\n",
    "- It's a really useful way to manage your secrets without needing to type them into colab.  Do not directly copy paste the secret key inside the code cell, if you do so then make sure that you donot share the notebook with anyone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "rzwtevF17_Vk",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rzwtevF17_Vk",
    "outputId": "aed9c7d4-f16b-45f7-c90e-4544de7c60fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ada Lovelace was an English mathematician and writer, often considered the first computer programmer. She worked with Charles Babbage on the Analytical Engine and created what is regarded as the first algorithm intended for implementation on a machine. Her contributions to computing were recognized long after her death in 1852.\n"
     ]
    }
   ],
   "source": [
    "# This code will run only if the notebook is running on colab\n",
    "from google.colab import userdata\n",
    "from openai import OpenAI\n",
    "# Store your key in Colab's secrets (left sidebar > key icon) and access it using userdata.get() method\n",
    "openai_api_key = userdata.get( 'OPENAI_API_KEY')\n",
    "\n",
    "# Create client\n",
    "client = OpenAI(base_url=\"https://api.openai.com/v1\", api_key=openai_api_key)\n",
    "\n",
    "# Responses API call with list-based input\n",
    "response = client.responses.create(\n",
    "                                model=\"gpt-4o-mini\",\n",
    "                                #input= \"What is the color of the sky? Tell me in a single line.\"   # 'input' replaces 'messages' for the Responses API and can be a simple string or can be a list specifying the developer and user role\n",
    "                                input=[{\"role\": \"developer\", \"content\": \"Always answer in a short and clear way.\"}, {\"role\": \"user\", \"content\": \"Who is Ada Lovelace?\"}])\n",
    "\n",
    "# Extract and print the output text\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "s9lmS12U8W4q",
   "metadata": {
    "id": "s9lmS12U8W4q"
   },
   "source": [
    "# 4. Accessing a Notebook File from a Public GitHub Repository in Google Colab\n",
    "- If you have saved\n",
    "## a. Method 1: Use Google Colab Interface (Open from GitHub Tab)\n",
    "- Go to [https://colab.research.google.com](https://colab.research.google.com)\n",
    "- Click on the **“GitHub”** tab on the left side of the dialog\n",
    "- Enter GitHub repository name `https://github.com/arifpucit/Generative-and-Agentic-AI` and Colab will display all `.ipynb` files it can find. Click the notebook you want → Click **Open**\n",
    "\n",
    "## b. Method 2: Use the “Open in Colab” Badge / Link\n",
    "- For this course notbooks hosted at https://github.com/arifpucit/Generative-and-Agentic-AI, I am using a badge under the Lecture title in every notebook file and clicking that will open the notebook from GitHub repository to your Google Colab for execution.\n",
    "- An example batch link looks like this:\n",
    "\n",
    "```html\n",
    "<a href=\"https://colab.research.google.com/github/USERNAME/REPO/blob/main/notebook.ipynb\" target=\"_blank\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open in Colab\">\n",
    "</a>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uFn4OzAGWUJf",
   "metadata": {
    "id": "uFn4OzAGWUJf"
   },
   "source": [
    "# 5. Accessing a Notebook File from your Google Drive in Google Colab\n",
    "- **Method 1: Use Google Colab Interface**\n",
    "  - Go to https://colab.research.google.com\n",
    "  - Click on the \"Google Drive\" tab on the left.\n",
    "  - You may be asked to authorize access to your Drive.\n",
    "  - Once authorized, you will see a list of .ipynb files in your Drive.\n",
    "  - Select the notebook you want and click Open.\n",
    "\n",
    "- **Method 2: From Google Drive UI (Right Click Method)**\n",
    "  - Open Google Drive\n",
    "  - Find your .ipynb file.\n",
    "  - Right-click the file → Click \"Open with\" → Choose \"Google Colaboratory\"\n",
    "  - If \"Google Colaboratory\" doesn’t appear:\n",
    "    - Click \"Connect more apps\"\n",
    "    - Search for \"Colaboratory\"\n",
    "    - Click Install\n",
    "\n",
    "\n",
    "- **Method 3: Mount Google Drive in Colab (Programmatically)**\n",
    "  - Open a new notebook in Colab\n",
    "  - Run the following code to mount your Google Drive:\n",
    "```\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "```\n",
    "- After authorization, navigate to your file and open it in Colab.\n",
    "- An example code snippet that accesses the advertising.csv dataset from the instructor Google Drive is shown below:\n",
    "```\n",
    "import pandas as pd\n",
    "data = pd.read_csv('/content/drive/MyDrive/LLM/Datasets/advertising.csv')\n",
    "data.head()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "VNo2b0RA7ACy",
   "metadata": {
    "id": "VNo2b0RA7ACy"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (genai-uv)",
   "language": "python",
   "name": "genai-uv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
