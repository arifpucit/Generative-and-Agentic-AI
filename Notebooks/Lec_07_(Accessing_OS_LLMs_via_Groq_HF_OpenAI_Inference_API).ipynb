{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ffe9c6a0-8544-4467-89c9-903ea98a6890",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "---   \n",
    " <img align=\"left\" width=\"75\" height=\"75\"  src=\"https://upload.wikimedia.org/wikipedia/en/c/c8/University_of_the_Punjab_logo.png\"> \n",
    "\n",
    "<h1 align=\"center\">Department of Data Science</h1>\n",
    "\n",
    "---\n",
    "<h3><div align=\"right\">Instructor: Muhammad Arif Butt, Ph.D.</div></h3>    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae713dd3-ae7c-4b25-95d0-fb22c055583f",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "<h1 align=\"center\">Lec-07: Accessing Open-Source AI Models via Groq, Hugging Face and OpenAI-Compatible Inference APIs</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6c8f2f-5ada-480c-b66f-11afc1aaa6f1",
   "metadata": {},
   "source": [
    "# Learning agenda of this notebook\n",
    "\n",
    "1. Accessing Open-Source AI Models hosted on **Hugging Face Hub**\n",
    "    - Access Option 1: Access with Hugging Face InferenceClient\n",
    "    - Access Option 2: Access with OpenAI Chat Completions API (Hugging Face Router)\n",
    "    - Access Option 3: Access with OpenAI Responses API (Hugging Face Router)\n",
    "2. Hands-On Practice Examples with Hugging Face Hosted Models using OpenAI's `Responses` API\n",
    "3. Accessing Open-Source AI Models hosted on **Groq**\n",
    "    - Access Option 1: Access with Groq Chat Completions API\n",
    "    - Access Option 2: Access with OpenAI Chat Completions API (Groq Router)\n",
    "    - Access Option 3: Access with OpenAI Responses API (Groq Router)\n",
    "4. Hands-On Practice Examples with Groq Hosted Models using OpenAI's `Responses` API "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0fe4822-176c-4fcf-aa41-b4400ae54d7f",
   "metadata": {},
   "source": [
    "# <span style='background :lightgreen' >Recap: Ways to Access Open Source LLMs</span>\n",
    "### (i) Access Open-Source Models via Cloud-Based Providers (Driving a fully automatic car ‚Äî everything managed for you)\n",
    "* Cloud inference providers host the models for you, removing the need for GPUs, scaling infrastructure, or deployment engineering.\n",
    "* You interact with models using simple HTTP calls or OpenAI-compatible APIs, making it the quickest way to use LLMs in production.\n",
    "* Services like **Groq** offer ultra-fast inference on custom LPU hardware with extremely low latency for models such as Llama, Qwen, Mixtral, Whisper, etc.\n",
    "* **Hugging Face Inference** provides access to 1M+ models via Serverless Inference, TGI, or Inference Endpoints‚Äîpay-as-you-go, secure, and instantly deployable.\n",
    "\n",
    "### (ii) Run Open-Source Models locally using runtimes (Driving an automatic car ‚Äî local but simple, no gears or engineering)\n",
    "\n",
    "### (iii) Use Open-Source Models via Hugging Face `pipeline()` API (Driving a manual car ‚Äî you see more of the mechanics, but still a car someone else built)\n",
    "\n",
    "### (iv) Load and run models directly from Hugging Face Hub using `AutoModel/AutoTokenizer` (Opening the hood and adjusting or replacing engine components)\n",
    "\n",
    "\n",
    "### (v) Fine-Tune LLMs using full fine-tuning or PEFT methods (LoRA / QLoRA / adapters) (Upgrading and re-calibrating the engine to suit your driving style)\n",
    "\n",
    "### (vi) Build and train an AI Model from scratch using PyTorch / TensorFlow (Designing and building the entire car from raw parts ‚Äî full control, full responsibility)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461a2f38-15ea-436a-9d09-9b06ca957ed4",
   "metadata": {},
   "source": [
    "# <span style='background :lightgreen' >1. ü§ó Hugging Face Models ‚Äî How Can You Use Them?</span>\n",
    "\n",
    "### HF Serverless Inference (`provider=\"hf-inference\"`)\n",
    "- HF runs the model on their own GPUs with no external vendor involved.\n",
    "- Only works for small, non-gated models.\n",
    "- Free to use with a valid `HF_TOKEN`.\n",
    "- Identifiable by the live Inference API widget on the model page.\n",
    "- Examples: `microsoft/DialoGPT-medium`, `google/gemma-2-2b-it`\n",
    "### 3rd-Party Vendors via HF (`provider=\"sambanova\"`, `\"together\"`, `\"fireworks-ai\"`, `\"cerebras\"`, `\"replicate\"` etc.)\n",
    "- The actual inference runs on the vendor's hardware, not HF's. HF is just the broker, however, you still use InferenceClient with your HF_TOKEN..\n",
    "- Supports large and gated models that HF Serverless cannot handle.\n",
    "- `provider=\"auto\"` lets HF automatically pick the first available vendor for your model.\n",
    "- Examples: `meta-llama/Llama-3.1-8B-Instruct`, `microsoft/phi-4`\n",
    "### Local Download via llama.cpp / Ollama (GGUF format)\n",
    "- You can download models in GGUF format to be run via local runtimes like Ollama or llama.cpp.\n",
    "- These are quantized versions of popular models ‚Äî much smaller in memory footprint, runnable on consumer hardware (even CPU).\n",
    "- Examples: `bartowski/Meta-Llama-3.1-8B-Instruct-GGUF`, `TheBloke/Mixtral-8x7B-Instruct-v0.1-GGUF`\n",
    "### Local Download via `transformers`\n",
    "- You can download and run most models locally using the `transformers` library.\n",
    "- Examples: `tiiuae/falcon-180B-chat`, `tiiuae/falcon-40b-instruct`\n",
    "\n",
    ">- Gated models: Some models like Llama-3 and Gemma require you to accept a license on the HF website before downloading ‚Äî otherwise you get a `401` error even with a valid token.\n",
    "    1. Click \"Agree and access repository\" (or similar button)\n",
    "    2. Accept the license terms\n",
    "    3. Wait for access approval (usually granted automatically or within hours)\n",
    "    4. Once granted, the UI will indicate: *\"You have been granted access to this model\"* or similar confirmation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9119bdb6-07b6-4712-aea5-c181a3e076e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hugging Face Tokens exists and begins hf_oEyH\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv('../keys/.env', override=True) \n",
    "\n",
    "hf_token = os.getenv('HF_TOKEN')\n",
    "if hf_token:\n",
    "    print(f\"Hugging Face Tokens exists and begins {hf_token[:7]}\")\n",
    "else:\n",
    "    print(\"Hugging Face tokens not set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d36edb6-33c3-4314-a0d8-e9cdad87a9a1",
   "metadata": {},
   "source": [
    "## Access Option 1: Access with Hugging Face InferenceClient API\n",
    "```python\n",
    "InferenceClient(\n",
    "    model: Optional[str] = None,\n",
    "    provider: Union[Literal[‚Ä¶], \"auto\", None] = None,\n",
    "    token: Optional[str] = None,\n",
    "    timeout: Optional[float] = None,\n",
    "    headers: Optional[dict[str, str]] = None,\n",
    "    cookies: Optional[dict[str, str]] = None,\n",
    "    bill_to: Optional[str] = None,\n",
    "    base_url: Optional[str] = None,\n",
    "    api_key: Optional[str] = None,  # alias to token\n",
    "    proxies: Optional[Any] = None,  # in some versions\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e3fcafd-b7eb-4fe4-9973-2ba83ba70d1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of Pakistan is Islamabad.\n",
      "llama3.1-8b\n",
      "ChatCompletionOutputUsage(completion_tokens=8, prompt_tokens=48, total_tokens=56, completion_tokens_details={'accepted_prediction_tokens': 0, 'rejected_prediction_tokens': 0, 'reasoning_tokens': 0}, prompt_tokens_details={'cached_tokens': 0})\n"
     ]
    }
   ],
   "source": [
    "# Using HF InferenceClient() and chat_completion() method\n",
    "import huggingface_hub\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv('../keys/.env', override=True) \n",
    "hf_token = os.getenv('HF_TOKEN')\n",
    "\n",
    "#  Hugging Face supports multiple back-end inference providers (e.g., \"hf-inference\", \"sambanova\", \"cerebras\", \"together\", \"replicate\", \"fireworks-ai\" etc.).\n",
    "# If you specify provider=\"auto\", HF picks the first available provider automatically\n",
    "client = huggingface_hub.InferenceClient(\n",
    "                        model=\"meta-llama/Llama-3.1-8B-Instruct\", #\"meta-llama/Llama-3.1-8B-Instruct\", # Model ID from the Hugging Face Hub (e.g., \"meta-llama/Llama-3.1-8B-Instruct\") or a URL to a deployed inference endpoint\n",
    "                        provider=\"auto\",             \n",
    "                        token=hf_token\n",
    "                        )\n",
    "response = client.chat_completion(\n",
    "                                messages=[\n",
    "                                            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                                            {\"role\": \"user\", \"content\": \"What is the capital of Pakistan.\"}\n",
    "                                        ],\n",
    "                                max_tokens=None,         # Default: None (no limit, up to model's max)\n",
    "                                temperature=None,        # Default: None (provider's default, usually ~0.7-1.0)\n",
    "                                top_p=1.0,               # Default: None (provider's default, usually 1.0)\n",
    "                                stream=False,            # Default: False\n",
    "                                stop=None,               # default: None, can provide list of stop tokens\n",
    "                                presence_penalty=0.0,    # Default: None (provider's default, usually 0.0)\n",
    "                                frequency_penalty=0.0    # Default: None (provider's default, usually 0.0)\n",
    "                            )\n",
    "\n",
    "print(response.choices[0].message.content) #the actual text answer you want\n",
    "print(response.model) # which model produced the result\n",
    "print(response.usage) #token usage details (handy for cost/efficiency if you were on OpenAI)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c10855-2f50-4702-876c-43016bb22497",
   "metadata": {},
   "source": [
    "## Access Option 2: Access with OpenAI Chat Completion API (Hugging Face Router)\n",
    "- Here are the key differences between the two approaches:\n",
    "    - Library dependency: InferenceClient is part of the huggingface_hub package and is HuggingFace-native, while the OpenAI approach uses the openai package pointed to HuggingFace's router OpenAI\n",
    "    - Provider routing: InferenceClient offers automatic provider selection with provider=\"auto\" and can route through multiple inference providers (Replicate, Together AI, Sambanova, etc.), while the OpenAI client requires manual base_url specification OpenAI\n",
    "    - Additional features: InferenceClient supports multiple task types beyond chat (text-to-image, embeddings, speech processing), while the OpenAI-compatible router currently only supports chat completion tasks OpenAIOpenAI\n",
    "    - Parameter flexibility: InferenceClient has extra_body parameter for provider-specific settings and more flexible initialization options, while OpenAI client uses standard OpenAI parameters only\n",
    "    - Syntax compatibility: Both produce identical outputs since client.chat_completion() is aliased as client.chat.completions.create() in InferenceClient for OpenAI compatibility OpenAI\n",
    "    - Use case optimization: InferenceClient is optimized for HuggingFace ecosystem with built-in provider management, while OpenAI client approach is better if you're already using OpenAI syntax across your codebase and want minimal changes\n",
    "\n",
    ">- **HuggingFace provides OpenAI-compatible endpoints through https://router.huggingface.co/v1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9e20481-3f84-4095-856c-403912aa2949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of Pakistan is Islamabad.\n",
      "{\n",
      "    \"id\": \"chatcmpl-cf051685-e85d-456f-8014-d0efac5a7054\",\n",
      "    \"choices\": [\n",
      "        {\n",
      "            \"finish_reason\": \"stop\",\n",
      "            \"index\": 0,\n",
      "            \"logprobs\": null,\n",
      "            \"message\": {\n",
      "                \"content\": \"The capital of Pakistan is Islamabad.\",\n",
      "                \"refusal\": null,\n",
      "                \"role\": \"assistant\",\n",
      "                \"annotations\": null,\n",
      "                \"audio\": null,\n",
      "                \"function_call\": null,\n",
      "                \"tool_calls\": null\n",
      "            }\n",
      "        }\n",
      "    ],\n",
      "    \"created\": 1771350734,\n",
      "    \"model\": \"llama3.1-8b\",\n",
      "    \"object\": \"chat.completion\",\n",
      "    \"service_tier\": null,\n",
      "    \"system_fingerprint\": \"fp_5198798116a66ebf301b\",\n",
      "    \"usage\": {\n",
      "        \"completion_tokens\": 8,\n",
      "        \"prompt_tokens\": 48,\n",
      "        \"total_tokens\": 56,\n",
      "        \"completion_tokens_details\": {\n",
      "            \"accepted_prediction_tokens\": 0,\n",
      "            \"audio_tokens\": null,\n",
      "            \"reasoning_tokens\": 0,\n",
      "            \"rejected_prediction_tokens\": 0\n",
      "        },\n",
      "        \"prompt_tokens_details\": {\n",
      "            \"audio_tokens\": null,\n",
      "            \"cached_tokens\": 0\n",
      "        }\n",
      "    },\n",
      "    \"time_info\": {\n",
      "        \"queue_time\": 0.00013004,\n",
      "        \"prompt_time\": 0.002651438,\n",
      "        \"completion_time\": 0.00320009,\n",
      "        \"total_time\": 0.007077932357788086,\n",
      "        \"created\": 1771350734.8084688\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Using OpenAIs Chat Completion API\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "# Load GROQ API key from .env\n",
    "load_dotenv(\"../keys/.env\", override=True)\n",
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "# Create an OpenAI client instance and specify the base_url as \"https://router.huggingface.co/v1\" (OpenAI-compatible API endpoint).\n",
    "# \"https://router.huggingface.co/v1\" Hugging Face‚Äôs OpenAI-compatible ‚Äúinference router‚Äù endpoint. It acts like a universal gateway that proxies your request to the correct model backend.\n",
    "client = OpenAI(base_url=\"https://router.huggingface.co/v1\", api_key=hf_token) \n",
    "\n",
    "# Use OpenAI's Chat Completions API (routed through Hugging Face)\n",
    "response = client.chat.completions.create(\n",
    "                                            model=\"meta-llama/Llama-3.1-8B-Instruct\", # \"openai/gpt-oss-20b:novita\"\n",
    "                                            messages=[\n",
    "                                                        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                                                        {\"role\": \"user\", \"content\": \"What is the capital of Pakistan?\"}\n",
    "                                                    ],\n",
    "                                            temperature=1,\n",
    "                                            top_p=1,\n",
    "                                            max_completion_tokens=8192,\n",
    "                                            reasoning_effort=None,   # \"medium\"\n",
    "                                            stream=False\n",
    "                                        )\n",
    "\n",
    "print(response.choices[0].message.content)\n",
    "print(response.model_dump_json(indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e28b5fe-843f-44bd-a368-1c1604937307",
   "metadata": {},
   "source": [
    "## Access Option 3: Access with OpenAI Responses API (Hugging Face Router)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c15134ec-79f3-4011-81f5-ac8766380da3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of Pakistan is Islamabad.\n",
      "{\n",
      "    \"id\": \"resp_b36c9fc193b30318cebd9b8445fb67f2855cbce7552eb379\",\n",
      "    \"created_at\": 1771350731.0,\n",
      "    \"error\": null,\n",
      "    \"incomplete_details\": null,\n",
      "    \"instructions\": null,\n",
      "    \"metadata\": null,\n",
      "    \"model\": \"meta-llama/Llama-3.1-8B-Instruct\",\n",
      "    \"object\": \"response\",\n",
      "    \"output\": [\n",
      "        {\n",
      "            \"id\": \"msg_3fcd528d57a40fb54912481a5c17f23c2d49f985b1f7ffac\",\n",
      "            \"content\": [\n",
      "                {\n",
      "                    \"annotations\": [],\n",
      "                    \"text\": \"The capital of Pakistan is Islamabad.\",\n",
      "                    \"type\": \"output_text\",\n",
      "                    \"logprobs\": null\n",
      "                }\n",
      "            ],\n",
      "            \"role\": \"assistant\",\n",
      "            \"status\": \"completed\",\n",
      "            \"type\": \"message\"\n",
      "        }\n",
      "    ],\n",
      "    \"parallel_tool_calls\": null,\n",
      "    \"temperature\": 1.0,\n",
      "    \"tool_choice\": \"auto\",\n",
      "    \"tools\": [],\n",
      "    \"top_p\": 1.0,\n",
      "    \"background\": null,\n",
      "    \"completed_at\": null,\n",
      "    \"conversation\": null,\n",
      "    \"max_output_tokens\": 8192,\n",
      "    \"max_tool_calls\": null,\n",
      "    \"previous_response_id\": null,\n",
      "    \"prompt\": null,\n",
      "    \"prompt_cache_key\": null,\n",
      "    \"prompt_cache_retention\": null,\n",
      "    \"reasoning\": null,\n",
      "    \"safety_identifier\": null,\n",
      "    \"service_tier\": null,\n",
      "    \"status\": \"completed\",\n",
      "    \"text\": null,\n",
      "    \"top_logprobs\": null,\n",
      "    \"truncation\": null,\n",
      "    \"usage\": {\n",
      "        \"input_tokens\": 42,\n",
      "        \"input_tokens_details\": {\n",
      "            \"cached_tokens\": 0\n",
      "        },\n",
      "        \"output_tokens\": 8,\n",
      "        \"output_tokens_details\": {\n",
      "            \"reasoning_tokens\": 0\n",
      "        },\n",
      "        \"total_tokens\": 50\n",
      "    },\n",
      "    \"user\": null\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "# Load Hugging Face token from .env\n",
    "load_dotenv(\"../keys/.env\", override=True)\n",
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "# Create an OpenAI client instance and specify the base_url as \"https://router.huggingface.co/v1\" (OpenAI-compatible API endpoint).\n",
    "# \"https://router.huggingface.co/v1\" Hugging Face‚Äôs OpenAI-compatible ‚Äúinference router‚Äù endpoint. It acts like a universal gateway that proxies your request to the correct model backend.\n",
    "client = OpenAI(base_url=\"https://router.huggingface.co/v1\", api_key=hf_token)\n",
    "\n",
    "# Use OpenAI Responses API (routed through Hugging Face)\n",
    "response = client.responses.create(\n",
    "                                    model=\"meta-llama/Llama-3.1-8B-Instruct\", \n",
    "                                    input=[\n",
    "                                            {\"role\": \"developer\", \"content\": \"You are a helpful assistant.\"},\n",
    "                                            {\"role\": \"user\", \"content\": \"What is the capital of Pakistan?\"}\n",
    "                                            ],\n",
    "                                    temperature=1,\n",
    "                                    top_p=1,\n",
    "                                    max_output_tokens=8192,\n",
    "                                    stream=False\n",
    "                                )\n",
    "\n",
    "# Display the model's response\n",
    "print(response.output_text)\n",
    "print(response.model_dump_json(indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7df9e8f-6775-4d25-9ecc-c3f5e3620299",
   "metadata": {},
   "source": [
    "# <span style='background :lightgreen' >2. Hands-On Practice Examples with Hugging Face Hosted Models using OpenAI's `Responses` API</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a217a6-34b2-4795-a443-4e16572e3777",
   "metadata": {},
   "source": [
    "## a. Writing a Function for our ease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4862fcad-f6e2-499e-87bb-160a7fea2d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "# Load Hugging Face token from .env\n",
    "load_dotenv(\"../keys/.env\", override=True)\n",
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "\n",
    "# Create an OpenAI client instance and specify the base_url as \"https://router.huggingface.co/v1\" (OpenAI-compatible API endpoint).\n",
    "# \"https://router.huggingface.co/v1\" Hugging Face‚Äôs OpenAI-compatible ‚Äúinference router‚Äù endpoint. It acts like a universal gateway that proxies your request to the correct model backend.\n",
    "client = OpenAI(base_url=\"https://router.huggingface.co/v1\", api_key=hf_token) \n",
    "\n",
    "def ask_hf(\n",
    "    user_prompt: str,\n",
    "    developer_prompt: str = \"You are a helpful assistant that provides concise answers.\",\n",
    "    model: str = \"meta-llama/Llama-3.1-8B-Instruct\", \n",
    "    max_output_tokens: int = 1024,\n",
    "    temperature: float = 0.7,\n",
    "    top_p: float = 1.0,\n",
    "    stream: bool = False\n",
    "):\n",
    "    input_messages = [{\"role\": \"developer\", \"content\": developer_prompt}, {\"role\": \"user\", \"content\": user_prompt}]\n",
    "    # Responses API call without unsupported parameters\n",
    "    response = client.responses.create(\n",
    "                                        model=model,\n",
    "                                        input=input_messages,\n",
    "                                        max_output_tokens=max_output_tokens,\n",
    "                                        temperature=temperature,\n",
    "                                        top_p=top_p,\n",
    "                                        stream=stream\n",
    "                                        )\n",
    "\n",
    "    if stream:\n",
    "        return response  # Streaming generator\n",
    "    return response.output_text   # Aggregated text output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95b88df-e310-4169-b22b-b4ebb091abe7",
   "metadata": {},
   "source": [
    "## a. Examples (Question Answering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f509481-6f4b-4b02-ba3f-8bb4582db268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the regression model go to therapy?\n",
      "\n",
      "Because it was struggling to generalize its feelings.\n"
     ]
    }
   ],
   "source": [
    "developer_prompt = \"You are an assistant that tells light-hearted jokes.\"\n",
    "user_prompt = \"Tell a light-hearted joke for an audience of Data Scientists.\"\n",
    "\n",
    "response = ask_hf(user_prompt=user_prompt, developer_prompt=developer_prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4480bd2e-42fc-417b-82a4-f2509c55e40a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What a delightful tale I have in store for you.  In a far-off land, there existed a magical cave where treasures beyond imagination lay hidden. The cave was guarded by the infamous Chalees Chor, a group of 40 thieves who were as cunning as they were ruthless.\n",
      "\n",
      "Ali Baba, a humble merchant, had been searching for years to find this enchanted cave. His eyes had grown dim with age, and his hands were weak from years of hard labor. But he refused to give up his quest. One day, while traversing the desert, he stumbled upon a mysterious voice that whispered to him the secret of the cave: \"Open it not with a loud voice, but with a soft whisper, and the treasure shall be yours.\"\n",
      "\n",
      "With trembling hands, Ali Baba made his way to the cave, repeating the magical words to himself. As he pushed open the massive stone door, a soft whisper escaped his lips, and the door creaked open. The cave was filled with glittering treasures: gold, jewels, and precious artifacts that sparkled like stars in the night sky.\n",
      "\n",
      "But, unbeknownst to Ali Baba, Chalees Chor had been watching him from the shadows. The leader of the thieves, a cunning and evil man named Abu Hasan, vowed to steal the treasure for himself. He and his cohorts crept into the cave, but as they did, they were caught off guard by Ali Baba's cleverness.\n",
      "\n",
      "Ali Baba had left a small portion of the treasure outside the cave, hidden behind a palm tree. When he discovered that Chalees Chor had entered the cave, he quietly slipped outside and whispered to the palm tree, \"Open, sesame!\" The tree's branches parted, revealing the hidden treasure.\n",
      "\n",
      "Chalees Chor, realizing they had been outsmarted, tried to flee, but Ali Baba had set a trap for them. He had hidden a small note with the magical words, \"Open, sesame!\" which he gave to the palm tree. When Chalees Chor stumbled upon the note, they, too, whispered the words, and the palm tree's branches closed, trapping them inside the cave.\n",
      "\n",
      "As the sun set over the desert, Ali Baba returned to his humble home, his heart full of joy and his pockets heavy with gold. He lived out the rest of his days in peace, surrounded by his loving family and the treasure of the magical cave.\n",
      "\n",
      "And as for Chalees Chor, they remained trapped in the cave, their wicked plans foiled by Ali Baba's cunning and kindness. The moral of the story is that honesty, cleverness, and a little bit of magic can conquer even the most daunting challenges.\n",
      "\n",
      "As you drift off to sleep, remember the wise words of Ali Baba: \"Open, sesame!\" May your dreams be as sweet as the treasures of the magical cave, and may your heart remain as pure as Ali Baba's."
     ]
    }
   ],
   "source": [
    "developer_prompt = \"You are a bedtime storyteller.\"\n",
    "user_prompt = \"Tell me a bedtime story of Ali Baba and Chalees Chor\"\n",
    "\n",
    "# Get streaming generator from Responses API\n",
    "response = ask_hf(user_prompt=user_prompt, developer_prompt=developer_prompt, stream=True)\n",
    "\n",
    "# Iterate through streaming events and only print text deltas\n",
    "for event in response:\n",
    "    # Each event may contain incremental text in event.delta\n",
    "    if hasattr(event, \"delta\") and event.delta:\n",
    "        print(event.delta, end=\"\", flush=True) # prints the content from this chunk, end=\"\" prevents adding a newline after each  piece and flush=True forces flushing output to screen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaaf8681-fb69-48b9-9f99-5401dca1da9f",
   "metadata": {},
   "source": [
    "## b. Examples (Question Answering from Different Models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfce42eb-d41e-4a38-a424-de3c6134df2f",
   "metadata": {},
   "source": [
    "#### Asking date from \"meta-llama/Llama-4-Maverick-17B-128E-Instruct\"\n",
    "- The Llama 4 collection of models are natively multimodal AI models that enable text and multimodal experiences. These models leverage a mixture-of-experts architecture to offer industry-leading performance in text and image understanding.\n",
    "    - `Llama 4 Scout`, a 17 billion parameter model with 16 experts, and\n",
    "    - `Llama 4 Maverick`, a 17 billion parameter model with 128 experts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d287150e-0063-4261-97ed-ab8718a1931e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the linear regression model go to therapy?\n",
      "\n",
      "Because it was feeling a little \"regressed\" and was struggling to find the right \"fit\" in its life.\n",
      "\n",
      "(Sorry, I know it's a bit of a \"statistically\" weak pun, but I'm sure it's a \"correlation\" between humor and nerdiness that will resonate with you Data Scientists!)\n"
     ]
    }
   ],
   "source": [
    "developer_prompt = \"You are an assistant that is great at telling jokes\"\n",
    "user_prompt = \"Tell a light-hearted joke for an audience of Data Scientists\"\n",
    "response = ask_hf(user_prompt=user_prompt, developer_prompt=developer_prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d623f176-1102-49de-b80f-d3338642ebdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The date is February 17, 2024.\n"
     ]
    }
   ],
   "source": [
    "developer_prompt = \"You are a helpful assistant.\"\n",
    "user_prompt = \"What is the date today?\"\n",
    "response = ask_hf(user_prompt=user_prompt, developer_prompt=developer_prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e276cd3e-4fcd-40ba-99cb-17791b78a718",
   "metadata": {},
   "source": [
    "#### Asking date from \"meta-llama/Llama-4-Scout-17B-16E-Instruct\"\n",
    "- The Llama 4 collection of models are natively multimodal AI models that enable text and multimodal experiences. These models leverage a mixture-of-experts architecture to offer industry-leading performance in text and image understanding.\n",
    "    - `Llama 4 Scout`, a 17 billion parameter model with 16 experts, and\n",
    "    - `Llama 4 Maverick`, a 17 billion parameter model with 128 experts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "474b8fa5-ec27-4a49-b565-e409fb87e6d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm not sure what you're referring to, as my knowledge stopped in 1967.\n"
     ]
    }
   ],
   "source": [
    "developer_prompt = \"You are a helpful assistant.\"\n",
    "user_prompt = \"What is the date today?\"\n",
    "response = ask_hf(user_prompt=user_prompt, developer_prompt=developer_prompt, model=\"meta-llama/Llama-4-Scout-17B-16E-Instruct\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b59d641-0997-4df5-a540-ef69092a85f6",
   "metadata": {},
   "source": [
    "#### Asking date from \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "- A text-to-text instruction-tuned LLM (8B params) for conversational AI, Q&A, and task-oriented responses.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c7b581e1-5f50-48bb-9ba1-9ac73a0debad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm not currently able to share the date.\n"
     ]
    }
   ],
   "source": [
    "developer_prompt = \"You are a helpful assistant.\"\n",
    "user_prompt = \"What is the date today?\"\n",
    "response = ask_hf(user_prompt=user_prompt, developer_prompt=developer_prompt, model=\"meta-llama/Llama-3.2-3B-Instruct\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11c5836-79bd-4b93-beb6-213a9f896049",
   "metadata": {},
   "source": [
    "#### Asking date from \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "- A text-to-text instruction-tuned LLM (8B params) for conversational AI, Q&A, and task-oriented responses.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea46ce26-fc12-49c5-84e3-815138eab8bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The date I'm aware of is February 17, 2024. However, my knowledge cutoff is December 2023, I may not have information on very recent events or dates after my cutoff.\n"
     ]
    }
   ],
   "source": [
    "developer_prompt = \"You are a helpful assistant.\"\n",
    "user_prompt = \"What is the date today?\"\n",
    "response = ask_hf(user_prompt=user_prompt, developer_prompt=developer_prompt, model=\"meta-llama/Llama-3.1-8B-Instruct\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d7b904-b547-4f51-84b8-4a9b1519fdfa",
   "metadata": {},
   "source": [
    "#### Asking date from \"meta-llama/Meta-Llama-3-70B-Instruct\"\n",
    "- A text-to-text instruction-tuned LLM (70B params) used for reasoning, coding, and complex text tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "22de4815-8fd5-481d-b558-fcf13978258d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm an AI, I don't have real-time access to the current date. However, I can suggest ways for you to find out the current date.\n",
      "\n",
      "You can:\n",
      "\n",
      "1. Check your device's clock or calendar app.\n",
      "2. Search online for \"current date\" or \"today's date.\"\n",
      "3. Look at a physical calendar or planner.\n",
      "\n",
      "If you need help with anything else, feel free to ask!\n"
     ]
    }
   ],
   "source": [
    "developer_prompt = \"You are a helpful assistant.\"\n",
    "user_prompt = \"What is the date today?\"\n",
    "response = ask_hf(user_prompt=user_prompt, developer_prompt=developer_prompt, model=\"meta-llama/Meta-Llama-3-70B-Instruct\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f070912c-75b1-41a5-b1da-c15121b71087",
   "metadata": {},
   "source": [
    "#### Asking date from \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "- A text-to-text instruction-tuned LLM (7B params) supporting multi-turn chat, reasoning, and multilingual capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "22cf6606-1c69-4bae-b332-febb135e9706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As an AI, I don't have real-time capabilities and I don't have access to current dates. To find today's date, please check your device or online for the current date.\n"
     ]
    }
   ],
   "source": [
    "developer_prompt = \"You are a helpful assistant.\"\n",
    "user_prompt = \"What is the date today?\"\n",
    "response = ask_hf(user_prompt=user_prompt, developer_prompt=developer_prompt, model=\"Qwen/Qwen2.5-7B-Instruct\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d634b5-56b6-4399-b0cb-7125cfdd3745",
   "metadata": {},
   "source": [
    "#### Asking date from \"deepseek-ai/DeepSeek-V3.1\"\n",
    "- A text-to-text large LLM designed for reasoning, problem-solving, and multilingual dialogue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "17bd2ad9-02a9-4b85-9997-0b4159f3fd3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't have real-time capabilities, so I can't access the current date. You can check today's date on your device, phone, or computer. Let me know if you need help with anything else! üòä\n"
     ]
    }
   ],
   "source": [
    "developer_prompt = \"You are a helpful assistant.\"\n",
    "user_prompt = \"What is the date today?\"\n",
    "response = ask_hf(user_prompt=user_prompt, developer_prompt=developer_prompt, model=\"deepseek-ai/DeepSeek-V3.1\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011a9be6-5d41-46e2-bf73-622d7a332c3f",
   "metadata": {},
   "source": [
    "##  c. Question Answering from Content Passed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6e53d21a-6bcf-444e-8aa2-9f3db49fe25d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cricket in Pakistan has always been more than just a sport‚Äîit‚Äôs a source of national pride and unity. Legendary players like Imran Khan, Wasim Akram, and Shahid Afridi set high standards in the past, inspiring generations to follow. Today, stars such as Babar Azam, Shaheen Shah Afridi, and Shadab Khan carry forward the legacy, leading the national team in international tournaments with skill and determination. Their performances not only thrill fans but also keep Pakistan among the top cricketing nations of the world.\n",
      "\n",
      "Politics in Pakistan, meanwhile, remains dynamic and often turbulent, with key figures shaping the country‚Äôs direction. Leaders like Nawaz Sharif, Asif Ali Zardari, and Imran Khan have all held significant influence over the nation‚Äôs governance and policies. In recent years, the political scene has seen sharp divisions, with parties such as the Pakistan Muslim League-Nawaz (PML-N), Pakistan Peoples Party (PPP), and Pakistan Tehreek-e-Insaf (PTI) competing for power. Debates around economic reforms, governance, and foreign policy continue to dominate the national conversation, reflecting the challenges and aspirations of the Pakistani people."
     ]
    }
   ],
   "source": [
    "!cat ../data/names.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "284770a6-e645-4e38-b1f8-0bd088881b18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The politicians mentioned in the text are:\n",
      "\n",
      "1. Imran Khan\n",
      "2. Nawaz Sharif\n",
      "3. Asif Ali Zardari\n",
      "\n",
      "Note that some of the other individuals mentioned in the text (such as Babar Azam and Shaheen Shah Afridi) are not politicians, but rather cricketers.\n"
     ]
    }
   ],
   "source": [
    "with open(\"../data/names.txt\", \"r\") as f:\n",
    "    file_content = f.read()\n",
    "\n",
    "user_prompt = f\"Can you extract names the politicians from this text:\\n{file_content}\"\n",
    "response = ask_hf(user_prompt=user_prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd282ac-4e6b-4304-8227-28d2f457f063",
   "metadata": {},
   "source": [
    "## c. Examples (Binary Classification: Sentiment analysis, Spam detection, Medical diagnosis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1c835afd-198f-4b40-baea-19b44625d90a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "However, I need more information about Arif. There are many individuals and channels with the name Arif on YouTube. Can you please provide more context or details about the Arif you're referring to, such as:\n",
      "\n",
      "1. The specific topic or niche he creates content around (e.g., science, technology, entertainment, etc.)?\n",
      "2. The tone of his videos (e.g., educational, entertaining, serious, etc.)?\n",
      "3. Any distinctive features or characteristics of his videos (e.g., animation, interviews, experiments, etc.)?\n",
      "4. Approximate number of subscribers or views of his channel?\n",
      "\n",
      "This will help me narrow down the search and provide more accurate information about the Arif you're referring to.\n"
     ]
    }
   ],
   "source": [
    "developer_prompt = \"You are an expert who will classify a sentense as having either a Positive or Negative sentiment.\"\n",
    "user_prompt = \"I love the youtube videos of Arif, as they are very informative\"\n",
    "response = ask_hf(user_prompt=user_prompt, developer_prompt=developer_prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a46f32-3b46-4b36-9a3a-c9166c779107",
   "metadata": {},
   "source": [
    "## d. Examples (Multi-class Classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "03d0612a-76d7-4e55-965d-c1be9c16e196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It sounds like the novel had a big impact on you.  A well-executed plot twist can be a great way to keep readers engaged and invested in the story.  What was the twist in the novel you read, if you don't mind me asking?\n"
     ]
    }
   ],
   "source": [
    "developer_prompt = \"Classify product reviews into these categories: 'Electronics', 'Clothing', 'Books', 'Home & Garden', 'Sports', or 'Food'. \\\n",
    "Respond with only the category.\"\n",
    "user_prompt = \"This novel has an incredible plot twist that kept me reading all night\"\n",
    "response = ask_hf(user_prompt=user_prompt, developer_prompt=developer_prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d21df8c-a8c8-4fb5-9bec-4a83ceed2cd7",
   "metadata": {},
   "source": [
    "## e. Examples (Text Generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "222b017d-5715-44b6-b939-8d90dda349ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I can't provide a summary for an election held in February, 2024, as I do not have information that is up to date.\n"
     ]
    }
   ],
   "source": [
    "developer_prompt = \"You are an expert of political science and history and have a deep understanding of policical situation of Pakistan.\"\n",
    "user_prompt = \"Write down a 50 words summary about the fairness of general elections held in Pakistan on February 08, 2024.\"\n",
    "response = ask_hf(user_prompt=user_prompt, developer_prompt=developer_prompt, temperature=1.0)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3bc4d1a-fb13-497c-984c-ad5d307cfa91",
   "metadata": {},
   "source": [
    "## f. Examples (Code Generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "238b6a97-f800-43f2-9ebb-b671dac3b75d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Fibonacci Sequence Generator in C**\n",
      "=====================================\n",
      "\n",
      "The following C program generates the first ten numbers in the Fibonacci sequence.\n",
      "\n",
      "```c\n",
      "#include <stdio.h>\n",
      "\n",
      "// Function to generate Fibonacci sequence\n",
      "void generateFibonacci(int n) {\n",
      "    int fib[n];\n",
      "    fib[0] = 0;\n",
      "    fib[1] = 1;\n",
      "\n",
      "    // Generate Fibonacci sequence\n",
      "    for (int i = 2; i < n; i++) {\n",
      "        fib[i] = fib[i-1] + fib[i-2];\n",
      "    }\n",
      "\n",
      "    // Print the generated Fibonacci sequence\n",
      "    printf(\"First %d numbers in the Fibonacci sequence are: \", n);\n",
      "    for (int i = 0; i < n; i++) {\n",
      "        printf(\"%d \", fib[i]);\n",
      "    }\n",
      "    printf(\"\\n\");\n",
      "}\n",
      "\n",
      "int main() {\n",
      "    int n = 10; // Number of Fibonacci numbers to generate\n",
      "    generateFibonacci(n);\n",
      "    return 0;\n",
      "}\n",
      "```\n",
      "\n",
      "**Explanation**\n",
      "---------------\n",
      "\n",
      "This program uses a dynamic array `fib` to store the generated Fibonacci sequence. The `generateFibonacci` function initializes the first two elements of the sequence to 0 and 1, respectively. Then, it generates the remaining elements of the sequence using a loop, where each element is the sum of the previous two elements.\n",
      "\n",
      "The `main` function calls `generateFibonacci` with `n = 10` to generate the first ten numbers in the Fibonacci sequence.\n",
      "\n",
      "**Example Output**\n",
      "-----------------\n",
      "\n",
      "```\n",
      "First 10 numbers in the Fibonacci sequence are: \n",
      "0 1 1 2 3 5 8 13 21 34 \n",
      "```\n",
      "\n",
      "**Time Complexity**\n",
      "------------------\n",
      "\n",
      "The time complexity of this program is O(n), where n is the number of Fibonacci numbers to generate. This is because the program uses a single loop to generate the sequence."
     ]
    }
   ],
   "source": [
    "developer_prompt = \"You are an expert of C programing in C language.\"\n",
    "user_prompt = \"Write down a C program that generates first ten numbers of fibonacci sequence.\"\n",
    "response = ask_hf(user_prompt=user_prompt, developer_prompt=developer_prompt, stream=True)\n",
    "\n",
    "# Iterate through streaming events and only print text deltas\n",
    "for event in response:\n",
    "    # Each event may contain incremental text in event.delta\n",
    "    if hasattr(event, \"delta\") and event.delta:\n",
    "        print(event.delta, end=\"\", flush=True) # prints the content from this chunk, end=\"\" prevents adding a newline after each  piece and flush=True forces flushing output to screen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f036f47-02c3-48c5-a242-525b571cb7ff",
   "metadata": {},
   "source": [
    "## g. Examples (Text Translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8217ad7e-65b4-4042-9a35-c486ca0e7811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "user_prompt = \"\"\"\n",
    "Please act as an expert of English to Urdu translator by translating the given sentence from English into Urdu.\n",
    "'The budget this year will have a very bad impact on the low salried people'\n",
    "\"\"\"\n",
    "response = ask_hf(user_prompt=user_prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470fef23-c048-4da4-9101-732c79df0d06",
   "metadata": {},
   "source": [
    "## h. Examples (Text Summarization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d25292bc-3a0e-4774-83f4-a214f401178a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "developer_prompt = \"You are an expert of English language.\"\n",
    "\n",
    "user_prompt = f'''\n",
    "Summarize the text below in at most 20 words:\n",
    "```The Hugging Face transformers library is an incredibly versatile and powerful tool for natural language processing (NLP).\n",
    "It allows users to perform a wide range of tasks such as text classification, named entity recognition, and question answering, among others.\n",
    "It's an extremely popular library that's widely used by the open-source data science community.\n",
    "It lowers the barrier to entry into the field by providing Data Scientists with a productive, convenient way to work with transformer models.```\n",
    "'''\n",
    "\n",
    "response = ask_hf(user_prompt=user_prompt, developer_prompt=developer_prompt, temperature=0.2)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca26c62d-8ff2-45d6-acdf-207922323f81",
   "metadata": {},
   "source": [
    "## i. Examples (Named Entity Recognition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "176717c5-2734-4be5-a4b2-991d414a7696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "developer_prompt = \"\"\"You are a  Named Entity Recognition specialist. Extract and classify entities from the given text into these categories only if they exist:\n",
    "- name\n",
    "- major\n",
    "- university\n",
    "- nationality\n",
    "- grades\n",
    "- club\n",
    "Format your response as: 'Entity: [text] | Type: [category]' with each entity on a new line.\"\"\"\n",
    "\n",
    "user_prompt = '''\n",
    "Zelaid Mujahid is a sophomore majoring in Data Science at University of the Punjab. \\\n",
    "He is Pakistani national and has a 3.5 GPA. Mujahid is an active member of the department's AI Club.\\\n",
    "He hopes to pursue a career in AI after graduating.\n",
    "'''\n",
    "\n",
    "response = ask_hf(user_prompt=user_prompt, developer_prompt=developer_prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb08124-1b51-4009-9369-7fdf0f9e22d6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## j. Example (Grade School Math 8K (GSM8K))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f79c8a5d-73ba-4c8c-a47f-101e43ac124e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "developer_prompt = \"\"\"You are an expert School math teacher. \n",
    "Consider the following text and then answer the questions of the students from this:\n",
    "A carnival snack booth made $50 selling popcorn each day. It made three times as much selling cotton candy. \n",
    "For a 5-day activity, the booth has to pay $30 rent and $75 for the cost of the ingredients. \n",
    "\"\"\"\n",
    "user_prompt = \"How much did the booth earn for 5 days after paying the rent and the cost of ingredients?\"\n",
    "\n",
    "response = ask_hf(user_prompt=user_prompt, developer_prompt=developer_prompt,  model='llama-3.3-70b-versatile')\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6ca988e8-d006-49a7-b7da-2822e30cd095",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "developer_prompt = \"\"\"You are an expert School math teacher. \n",
    "Consider the following text and then answer the questions of the students from this:\n",
    "A carnival snack booth made $50 selling popcorn each day. It made three times as much selling cotton candy. \n",
    "For a 5-day activity, the booth has to pay $30 rent and $75 for the cost of the ingredients. \n",
    "\"\"\"\n",
    "user_prompt = \"How much did the booth earn for 5 days after paying the rent and the cost of ingredients?\"\n",
    "\n",
    "response = ask_hf(user_prompt=user_prompt, developer_prompt=developer_prompt,  model='meta-llama/llama-4-maverick-17b-128e-instruct')\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f7053909-1246-4a41-bc33-47436bf89e9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "developer_prompt = \"\"\"You are an expert School math teacher. \n",
    "Consider the following text and then answer the questions of the students from this:\n",
    "A carnival snack booth made $50 selling popcorn each day. It made three times as much selling cotton candy. \n",
    "For a 5-day activity, the booth has to pay $30 rent and $75 for the cost of the ingredients. \n",
    "\"\"\"\n",
    "user_prompt = \"How much did the booth earn for 5 days after paying the rent and the cost of ingredients?\"\n",
    "\n",
    "response = ask_hf(user_prompt=user_prompt, developer_prompt=developer_prompt, model='openai/gpt-oss-20b')\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961bd18e-5604-4bdb-b15b-b86ef0d589dd",
   "metadata": {},
   "source": [
    "# <span style='background :lightgreen' >3. Accessing Open-Source AI Models via External Inference Providers</span>\n",
    "\n",
    "## Famous External Inference Providers</span>\n",
    "- Many external inference providers host popular open-source models from Hugging Face, offering optimized infrastructure, faster inference speeds, and often more generous free tiers than self-hosting. These providers specialize in serving models at scale with production-ready APIs.\n",
    "\n",
    "| Provider | Best For | Model Selection | Speed | Pricing Model | Free Tier |\n",
    "|----------|----------|-----------------|-------|---------------|-----------|\n",
    "| **[Groq](https://console.groq.com)** | Ultra-fast inference, real-time apps | Limited but popular models | ‚ö° Fastest | Per-token | Generous |\n",
    "| **[Together AI](https://api.together.xyz)** | Variety, fine-tuning, batch processing | 100+ models, largest selection | Fast | Per-token (model-specific) | $25 credits |\n",
    "| **[Replicate](https://replicate.com)** | Ease of use, multimodal, experimentation | 1000s of models (LLM, image, audio, video) | Moderate | Per-second compute time | Limited credits |\n",
    "\n",
    "<h3 align=\"center\"><div class=\"alert alert-success\" style=\"margin: 20px\">Groq provides ultra-fast inference for open-source models using its custom LPU hardware and offers an OpenAI-compatible API, making it easy to run models like Llama, Mixtral, Qwen, Whisper, and GPT OSS with extremely low latency and minimal setup.</h3>\n",
    "\n",
    "- **Ultra-Fast AI Inference Company:** - Groq (https://groq.com) is a company that provides **ultra-fast AI inference** through their specialized hardware (LPU - Language Processing Unit).\n",
    "- **Open-Source Model Platform** - Offers 17+ optimized models from Meta (Llama), Google (Gemma), DeepSeek, Alibaba (Qwen), and others - all accessible through a simple API similar to OpenAI's format.\n",
    "- **Cost-Effective Alternative** - Provides generous free tier and lower pricing compared to proprietary APIs like OpenAI or Anthropic, making it ideal for high-volume applications and startups on a budget.\n",
    "- **Multiple AI Capabilities** - Beyond text generation, Groq supports speech-to-text (Whisper), text-to-speech (PlayAI), content moderation (Llama Guard), and security features (Prompt Guard) - all through one API.\n",
    "- **No Vendor Lock-In** - Since all models are open-source, you can switch providers or self-host the same models later, giving you flexibility and control over your AI infrastructure without being tied to proprietary technology.\n",
    "- **Production-Ready Performance** - Combines the quality of state-of-the-art open models (like Llama 3.3 70B) with enterprise-grade speed and reliability, making it suitable for real-time chatbots, customer service, and interactive applications.\n",
    "\n",
    "\n",
    "## a. Get Groq API Key\n",
    "- **Create an Account on Groq:** Go to https://console.groq.com/playground and Sign up or log in with your Google account. Groq is free to try and you can do a lot of things without paying a peny\n",
    "- **Generating Groq API Key:** Login to Groq and navigate to Settings from the user menu on the top right and create a new Groq API token. Generate a **New Token** (choose `Read` access).  Save the token safely ‚Äî we‚Äôll need it in our Python code.\n",
    "\n",
    "### **Production Models** (Recommended for Production Use)\n",
    "\n",
    "| Company    |                               Model ID |            Parameters | Best Used For                                                           |  Context Window | Max Completion               |\n",
    "| ---------- | -------------------------------------: | --------------------: | ----------------------------------------------------------------------- | --------------: | ---------------------------- |\n",
    "| **Meta**   |              `llama-3.3-70b-versatile` |                   70B | General-purpose, high-quality instruction following, long-context tasks |     **131,072** | **32,768**                   |\n",
    "| **Meta**   |                 `llama-3.1-8b-instant` |                    8B | Low-latency chat, high throughput / real-time use cases                 |     **131,072** | **131,072**                  |\n",
    "| **Meta**   |         `meta-llama/llama-guard-4-12b` |                   12B | Safety / content-moderation guard model                                 |     **131,072** | **1,024**                    |\n",
    "| **OpenAI** |                  `openai/gpt-oss-120b` |  ~120B (OSS frontier) | High-capability reasoning / production workloads where offered          |     **131,072** | **65,536**                   |\n",
    "| **OpenAI** |                   `openai/gpt-oss-20b` |                  ~20B | Smaller frontier model for cost-sensitive production use                |     **131,072** | **65,536**                   |\n",
    "| **OpenAI** |       `whisper-large-v3` (speech‚Üítext) |                ~1.55B | High-accuracy speech-to-text (multilingual)                             | ‚Äî (audio model) | ‚Äî (audio/output constraints) |\n",
    "| **OpenAI** | `whisper-large-v3-turbo` (speech‚Üítext) | ‚Äì (optimized variant) | Faster multilingual transcription (low-latency)                         | ‚Äî (audio model) | ‚Äî (audio/output constraints) |\n",
    "\n",
    "\n",
    "### **Preview Models** (Experimental - Not for Production)\n",
    "\n",
    "\n",
    "| Company                   |                                        Model ID |                Parameters | Best Used For                                           | Context Window | Max Completion |\n",
    "| ------------------------- | ----------------------------------------------: | ------------------------: | ------------------------------------------------------- | -------------: | -------------- |\n",
    "| **Meta**                  | `meta-llama/llama-4-maverick-17b-128e-instruct` | ~17B (Mixture of Experts) | Multimodal assistant experiments, advanced reasoning    |    **131,072** | **8,192**      |\n",
    "| **Meta**                  |     `meta-llama/llama-4-scout-17b-16e-instruct` | ~17B (Mixture of Experts) | Experimental multimodal / efficient inference           |    **131,072** | **8,192**      |\n",
    "| **Meta**                  |           `meta-llama/llama-prompt-guard-2-22m` |                      ~22M | Lightweight prompt-injection detection / security       |        **512** | **512**        |\n",
    "| **Meta**                  |           `meta-llama/llama-prompt-guard-2-86m` |                      ~86M | Stronger prompt-injection detection                     |        **512** | **512**        |\n",
    "| **Moonshot AI**           |              `moonshotai/kimi-k2-instruct-0905` | 1T total (‚âà32B activated) | Agentic coding, tool use, very long-context workflows   |    **262,144** | **16,384**     |\n",
    "| **Alibaba / Qwen**        |                                `qwen/qwen3-32b` |                       32B | Multilingual reasoning, tool use, instruction following |    **131,072** | **40,960**     |\n",
    "| **PlayAI / Groq catalog** |                                    `playai-tts` |                         ‚Äì | Text-to-speech (general)                                |      **8,192** | **8,192**      |\n",
    "| **PlayAI / Groq catalog** |                             `playai-tts-arabic` |                         ‚Äì | Arabic text-to-speech                                   |      **8,192** | **8,192**      |\n",
    "\n",
    "\n",
    "> Production models are intended for use in production environments and meet Groq's high standards for speed, quality, and reliability, while preview models are for evaluation only and may be discontinued at short notice.\n",
    "\n",
    "\n",
    ">- Kimi-K2 0905: best for coding and agentic workflows that need deep reasoning\n",
    ">- Compound Beta: power up multi-model workflows in a single API call"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0b6e74-3350-47e5-9e70-fcb3bb7e7110",
   "metadata": {},
   "source": [
    "## Access Option 1: Access with Groq Chat Completions API\n",
    "- Directly uses Groq‚Äôs native SDK (groq) to call Groq-hosted models with full access to Groq-specific features like reasoning_effort.\n",
    "- Advantages:\n",
    "    - Fully compatible with all Groq model features.\n",
    "    - Can leverage Groq-specific optimizations (low latency, high throughput).\n",
    "    - Easy to set up and requires no OpenAI compatibility adjustments.\n",
    "- Disadvantages:\n",
    "    - Limited to Groq platform only.\n",
    "    - Cannot easily switch to other OpenAI-compatible services without code changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1c34e2e6-e8f7-4b89-8126-3115385a5647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mResolved \u001b[1m291 packages\u001b[0m \u001b[2min 16ms\u001b[0m\u001b[0m\n",
      "‚îú‚îÄ‚îÄ groq v1.0.0\n"
     ]
    }
   ],
   "source": [
    "#!uv add groq\n",
    "!uv tree | grep groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "76d27252-f34f-4a41-9165-8bcdd823219b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Groq is a hardware company that specializes in developing high-performance, artificial intelligence (AI) focused semiconductors and systems. They design and manufacture bespoke application-specific integrated circuits (ASICs) optimized for large-scale machine learning (ML) workloads.\n",
      "\n",
      "Groq was founded in 2016 by a team of experienced engineers, including Jonathan Ross, who previously worked at Google on the Tensor Processing Unit (TPU) project. The company is headquartered in Mountain View, California.\n",
      "\n",
      "Groq's primary focus is on building scalable, high-bandwidth, and low-latency AI accelerators that can efficiently process complex ML models. Their goal is to provide a significant boost in performance, power efficiency, and cost-effectiveness for AI workloads, such as those used in natural language processing, computer vision, and recommender systems.\n",
      "\n",
      "Some notable features of Groq's technology include:\n",
      "\n",
      "1. **Tensor-based architecture**: Groq's ASICs are designed to efficiently process tensor operations, which are fundamental to many ML algorithms.\n",
      "2. **High-bandwidth memory**: Groq's chips feature high-bandwidth memory interfaces to reduce data transfer bottlenecks and increase overall system performance.\n",
      "3. **Scalable design**: Groq's architecture is designed to scale to thousands of chips, making it suitable for large-scale AI deployments.\n",
      "4. **Software compatibility**: Groq's hardware is compatible with popular ML frameworks, such as TensorFlow and PyTorch, making it easier to integrate with existing AI workflows.\n",
      "\n",
      "Groq has received significant funding from investors, including a $300 million Series B funding round in 2020. The company has also partnered with several major tech companies, including Google, to develop custom AI accelerators.\n",
      "\n",
      "While Groq is a relatively new company, their innovative approach to AI-focused hardware design has generated significant interest in the industry, and they are expected to be a major player in the development of next-generation AI accelerators.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from groq import Groq\n",
    "\n",
    "# Load GROQ API key from .env\n",
    "load_dotenv(\"../keys/.env\", override=True)\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "\n",
    "# Initialize Groq client\n",
    "client = Groq(base_url=\"https://api.groq.com\", api_key=groq_api_key)\n",
    "client = Groq(api_key=groq_api_key)      # The correct default API endpoint is already baked into the SDK, so no need to specify base_url (recommended)\n",
    "\n",
    "# Use Groq's Chat Completions API\n",
    "response = client.chat.completions.create(\n",
    "                                        model=\"llama-3.3-70b-versatile\", \n",
    "                                        messages=[\n",
    "                                                {\"role\": \"system\", \"content\": \"You are an expert in LLM engineering.\"},\n",
    "                                                {\"role\": \"user\", \"content\": \"What is groq (a hardware company)?\"}\n",
    "                                                ],\n",
    "                                        temperature=1,\n",
    "                                        top_p=1,\n",
    "                                        max_completion_tokens=8192,\n",
    "                                        reasoning_effort=None,   # \"medium\"\n",
    "                                        stream=False\n",
    "                                        )\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f76c430-b2a3-4ae7-8a58-95c672d4690c",
   "metadata": {},
   "source": [
    "## Access Option 2: Access with OpenAI Chat Completion API (Groq Router)\n",
    "- Uses the OpenAI Python SDK with the `chat.completions.create` endpoint, pointing base_url to Groq‚Äôs OpenAI-compatible API. Works with almost all Groq models.\n",
    "- Advantages:\n",
    "    - Allows using familiar OpenAI client code with Groq models.\n",
    "    - Works for developers already familiar with OpenAI SDK.\n",
    "    - Supports chat-based interactions seamlessly.\n",
    "- Disadvantages:\n",
    "    - Not all Groq-specific features may be exposed.\n",
    "    - Requires base_url override to point to Groq API.\n",
    ">- **Groq provides OpenAI-compatible endpoints through https://api.groq.com/openai/v1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "89637499-c4ff-4d87-a3b0-a5330c8a290c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Groq is a cloud-based company that provides a high-performance, scalable, and efficient computing platform specifically designed for AI and machine learning (ML) workloads, particularly for large language models (LLMs). The company has developed a unique architecture and hardware designed to accelerate the processing of complex AI models, making it an attractive solution for developers, researchers, and organizations working with AI.\n",
      "\n",
      "Groq's technology is centered around its proprietary Language Processing Unit (LPU) architecture, which is designed to provide high performance and efficiency for LLMs and other AI workloads. The LPU is optimized for sparse and dense linear algebra, which are critical components of many AI models. The company's platform is built around this architecture, enabling fast and efficient processing of AI workloads.\n",
      "\n",
      "As a cloud provider, Groq offers a range of services, including:\n",
      "\n",
      "1. **Inference-as-a-Service**: Groq provides a cloud-based platform for deploying and running AI models, allowing users to integrate AI capabilities into their applications.\n",
      "2. **High-Performance Computing**: Groq's platform is designed to deliver high-performance computing for AI workloads, making it suitable for demanding applications.\n",
      "3. **Scalability**: Groq's architecture is designed to scale efficiently, allowing users to handle large and complex AI workloads.\n",
      "\n",
      "By providing a specialized platform for AI and LLMs, Groq aims to help developers and organizations accelerate their AI initiatives, improve performance, and reduce costs. The company is positioning itself as a key player in the rapidly growing AI infrastructure market.\n"
     ]
    }
   ],
   "source": [
    "# Using OpenAIs Chat Completion API\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "# Load GROQ API key from .env\n",
    "load_dotenv(\"../keys/.env\", override=True)\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "# The OpenAI client defaults to OpenAI‚Äôs servers,so you must specify the base_url to Groq‚Äôs OpenAI-compatible API endpoint (when using a Groq API key with the OpenAI client).\n",
    "client = OpenAI(base_url=\"https://api.groq.com/openai/v1\", api_key=groq_api_key) \n",
    "\n",
    "# Use OpenAI's Chat Completions API (works with all Groq models)\n",
    "response = client.chat.completions.create(\n",
    "    model=\"meta-llama/llama-4-maverick-17b-128e-instruct\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are an expert in LLM engineering.\"},\n",
    "        {\"role\": \"user\", \"content\": \"What is groq (a service/cloud provider)?\"}\n",
    "    ],\n",
    "    temperature=1,\n",
    "    top_p=1,\n",
    "    max_completion_tokens=8192,\n",
    "    reasoning_effort=None,   # \"medium\"\n",
    "    stream=False\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6fad4c-ff91-4c7d-8c99-558b8e379f0d",
   "metadata": {},
   "source": [
    "## Access Option 3: Access with OpenAI Responses API (Groq Router)\n",
    "- Uses the OpenAI Python SDK with the `responses.create` endpoint, pointing base_url to Groq‚Äôs OpenAI-compatible API.\n",
    "- Advantages:\n",
    "    - Access to more advanced OpenAI-style features like reasoning effort, structured outputs, and multi-turn dialogue.\n",
    "    - Can integrate easily into workflows designed for OpenAI Responses API.\n",
    "- Disadvantages:\n",
    "    - Only supports openai/gpt-oss-* models, not all Groq models.\n",
    "    - Requires base_url override for Groq API.\n",
    "    - Some chat-specific Groq features may not be available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "828762ce-002c-4217-8e85-aaac90e81cc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## LLM Apps vs. Agentic Apps  \n",
       "*How to tell them apart, why the difference matters, and what it means for building and deploying AI‚Äëpowered software.*\n",
       "\n",
       "| Feature | **LLM App** | **Agentic App** |\n",
       "|--------|--------------|-----------------|\n",
       "| **Primary purpose** | Direct language generation (text, code, summaries, translations, etc.) | Goal‚Äëdriven problem solving that can plan, act, and learn over time |\n",
       "| **Core control flow** | Prompt ‚Üí LLM ‚Üí Output (single‚Äëshot or few‚Äëshot) | Goal + State ‚Üí Planner ‚Üí Skill/Tool ‚Üí LLM policy ‚Üí Action ‚Üí New State ‚Üí Loop |\n",
       "| **State handling** | Stateless (except for the prompt‚Äôs context window). All ‚Äúmemory‚Äù lives in the input prompt or an external KV store if you manually pass it back. | Stateful. The agent keeps long‚Äëterm memory (knowledge graph, vector store, history logs) and short‚Äëterm working memory (current plan, context). |\n",
       "| **Interaction with environment** | None. The LLM is the only output source. | Yes. The agent can call APIs, read/write files, query databases, run code, etc., via ‚Äúskills‚Äù or ‚Äútools‚Äù. |\n",
       "| **Autonomy** | None beyond the single request. The LLM answers the prompt you gave. | High. The agent decides *what* to do next, *when* to call a tool, and *how* to modify its plan based on feedback. |\n",
       "| **Planning / reasoning** | Implicitly inside the prompt (chain‚Äëof‚Äëthought, few‚Äëshot reasoning). | Explicit. A planner (e.g., RAG + LLM, hierarchical planner, or RL‚Äëderived policy) decides next steps. |\n",
       "| **Typical tech stack** | Prompt‚Äëengineering, API wrappers (OpenAI, Anthropic, Cohere, etc.). | Agent framework (LangChain, ReAct, Agentic, OpenAI‚Äôs tool‚Äëcalling), memory back‚Äëends, environment adapters, optional RL fine‚Äëtuning. |\n",
       "| **Evaluation metrics** | Perplexity, BLEU/ROUGE, human ratings, token cost, latency. | Task‚Äëcompletion rate, plan efficiency, cost per task, safety violations, correctness of tool calls, user satisfaction. |\n",
       "| **Deployment footprint** | One or a few API calls. Low compute overhead beyond the LLM. | Continuous loops, multiple LLM calls per task, external tool calls ‚Üí higher latency, cost, orchestration complexity. |\n",
       "| **Security & safety** | Mitigate hallucinations via prompt design, content filtering, and output validation. | Extra safety layers: tool‚Äëaccess restrictions, verification steps, self‚Äëcritique loops, logging of tool calls, and monitoring for policy drift. |\n",
       "| **Examples** | ‚Ä¢ Text summarizer, translation, chatbot that replies to user messages. <br>‚Ä¢ Code generation via a prompt. | ‚Ä¢ Autogpt / BabyAGI that plans a research project and calls Google/Arxiv APIs. <br>‚Ä¢ Customer‚Äësupport agent that reads emails, queries a knowledge base, drafts replies, and escalates when needed. <br>‚Ä¢ ‚ÄúSmart‚Äù document assistant that extracts, summarizes, and synthesises information from PDFs while interacting with a database. |\n",
       "\n",
       "---\n",
       "\n",
       "### Why the distinction matters\n",
       "\n",
       "| Issue | LLM App | Agentic App |\n",
       "|-------|---------|-------------|\n",
       "| **Complexity of the problem you can solve** | Single‚Äëstep, narrow tasks. | Multi‚Äëstep, open‚Äëended, dynamic tasks that require external data or actions. |\n",
       "| **Control you have over the workflow** | Limited to prompt tweaks. | Full control: you can add/replace tools, change the planner, embed safety checks, or retrain the policy. |\n",
       "| **Development effort** | Quick to prototype (minutes to hours). | Requires architecture design (memory, tools, planner), integration, and monitoring. |\n",
       "| **Cost profile** | Predictable: tokens per request √ó cost per token. | Variable: often dozens of LLM calls + cost of external services. |\n",
       "| **Deployment & ops** | Single‚Äëservice API call. | Multi‚Äëservice orchestration, state persistence, concurrency, monitoring. |\n",
       "| **Safety & governance** | Mostly about output filtering. | Must guard against misuse of tools, erroneous decisions, and cascading failures. |\n",
       "\n",
       "---\n",
       "\n",
       "## Deep Dive: Architectural Patterns\n",
       "\n",
       "Below is a high‚Äëlevel diagram of each type, annotated with key components. (Think of this as a mental map ‚Äì you can draw it out if you‚Äôre visual.)\n",
       "\n",
       "```\n",
       "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
       "‚îÇ           LLM APP (Prompt‚Äëdriven)            ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ  User Input  ‚Üí  Prompt Builder  ‚Üí  LLM API  ‚îÇ\n",
       "‚îÇ                                 ‚Üí  Output   ‚îÇ\n",
       "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
       "```\n",
       "\n",
       "```\n",
       "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
       "‚îÇ           AGENTIC APP (Goal‚Äëdriven)                            ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ  Goal + State (memory)   ‚Üí  Planner (LLM policy)   ‚Üí  Action  ‚îÇ\n",
       "‚îÇ  (e.g., call tool, update plan) ‚Üí  Environment (API, DB, ‚Ä¶)  ‚îÇ\n",
       "‚îÇ  ‚Üí  State Update ‚Üí  Feedback (success/failure) ‚Üí  Planner   ‚îÇ\n",
       "‚îÇ  (loop)                                                       ‚îÇ\n",
       "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
       "```\n",
       "\n",
       "### Key Sub‚Äëcomponents in an Agentic App\n",
       "\n",
       "| Component | Purpose | Typical Implementation |\n",
       "|-----------|---------|------------------------|\n",
       "| **Goal** | The high‚Äëlevel objective the agent is trying to achieve (e.g., ‚ÄúBook a flight for Alice‚Äù). | Hard‚Äëcoded, user‚Äëprovided, or derived from a higher‚Äëlevel planner. |\n",
       "| **Memory** | Stores past interactions, plans, knowledge, and environment observations. | Short‚Äëterm buffer (LLM context), long‚Äëterm vector store, knowledge graph. |\n",
       "| **Planner** | Generates a sequence of high‚Äëlevel steps or sub‚Äëgoals. | LLM policy, rule‚Äëbased planner, RL‚Äëderived policy. |\n",
       "| **Skill/Tool Interface** | Concrete actions the agent can execute (HTTP calls, DB queries, file I/O). | Function calling API, custom wrappers, external services. |\n",
       "| **Controller** | Orchestrates planner ‚Üí tool calls ‚Üí state updates. | Finite‚Äëstate machine, event loop, or custom orchestration. |\n",
       "| **Self‚ÄëCritique / Verification** | Checks if the last step was correct before proceeding. | LLM ‚Äúcritic‚Äù, unit tests, external validation. |\n",
       "| **Safety Guardrails** | Prevents the agent from making unsafe or malicious actions. | Whitelisting, access controls, request throttling. |\n",
       "\n",
       "---\n",
       "\n",
       "## Prompt‚ÄëEngineering vs. Agent‚ÄëDesign\n",
       "\n",
       "| LLM App | Agentic App |\n",
       "|---------|-------------|\n",
       "| **Prompt Engineering**: Craft a single prompt that elicits the desired answer. | **Agent Design**: Define the set of skills, memory schema, and planning logic. The prompt is just one part of the policy. |\n",
       "| **Few‚ÄëShot / Chain‚Äëof‚ÄëThought**: Add examples or reasoning steps in the prompt to improve quality. | **Multi‚ÄëStep Reasoning**: The LLM reasons *inside* the planner but also decides when to call a tool. |\n",
       "| **Stateless**: Each request is independent. | **Stateful**: The agent maintains context across many turns. |\n",
       "| **Token Budget**: Entire conversation must fit in the context window. | **Memory Off‚Äëthe‚ÄëWall**: Long‚Äëterm knowledge is stored externally and fetched as needed. |\n",
       "\n",
       "---\n",
       "\n",
       "## Practical Guidance for Developers\n",
       "\n",
       "| What you‚Äôre building | Suggested approach |\n",
       "|----------------------|--------------------|\n",
       "| **A chatbot that answers FAQs** | Build an LLM app with a prompt that includes a few FAQ examples. Use a single API call. |\n",
       "| **A document‚Äësearch assistant** | Add retrieval‚Äëaugmented generation: fetch relevant passages, feed them into the prompt. Still an LLM app. |\n",
       "| **A project‚Äëmanagement AI that schedules tasks, writes emails, and queries a calendar** | Build an agentic app: <br>1. Define skills (email, calendar, to‚Äëdo list). <br>2. Use a planner to break the project into steps. <br>3. Store state in a vector store. <br>4. Add safety checks for calendar events. |\n",
       "| **A data‚Äëdriven recommendation system** | Combine LLM for natural‚Äëlanguage explanations with an agent that queries a recommendation engine. |\n",
       "\n",
       "### Common Pitfalls & Fixes\n",
       "\n",
       "| Pitfall | Fix |\n",
       "|---------|-----|\n",
       "| **Hallucinations** | Use self‚Äëcritique, verification steps, or ‚Äútool‚Äù calls that fetch facts (e.g., database queries) before the LLM writes the final answer. |\n",
       "| **Token Overrun** | Split long interactions into smaller chunks, or use a hierarchical planner that keeps only the essential context in the prompt. |\n",
       "| **Unnecessary Tool Calls** | Train the planner to use a ‚Äútool‚Äëcost‚Äù penalty or reward; fine‚Äëtune the policy with RLHF. |\n",
       "| **Safety Loops** | Implement a guardrails module that intercepts and blocks dangerous or disallowed actions. |\n",
       "| **Cold‚Äëstart for Memory** | Seed the memory with domain‚Äëspecific knowledge (e.g., company policies) before user interaction. |\n",
       "| **Deployment Overhead** | Use an orchestrator (e.g., Kubernetes Jobs or serverless functions) to manage agent loops, and keep the LLM inference in a separate micro‚Äëservice to isolate costs. |\n",
       "\n",
       "---\n",
       "\n",
       "## Future Outlook\n",
       "\n",
       "| Trend | LLM Apps | Agentic Apps |\n",
       "|-------|----------|--------------|\n",
       "| **Multi‚ÄëModal** | Extend prompts to include images, audio, or structured data. | Agents can orchestrate multi‚Äëmodal pipelines (e.g., ‚Äúread an image, summarise it, generate a caption‚Äù). |\n",
       "| **Self‚ÄëReflective Agents** | N/A | Agents that can critique their own decisions, adjust strategies, and self‚Äëimprove. |\n",
       "| **Open‚ÄëDomain Skill Acquisition** | Limited to prompt rewrites. | Agents that can discover new tools or skills dynamically (e.g., ‚Äúlearn to call a new API‚Äù). |\n",
       "| **Safety‚ÄëFirst Design** | Output‚Äëlevel filters. | Policy‚Äëlevel safety, e.g., a ‚Äúred‚Äëteam‚Äù sandbox that tests each tool call. |\n",
       "| **Cost‚ÄëEfficient Execution** | Token‚Äëbased billing dominates. | Use LLM sparingly, rely on rule‚Äëbased components, or cache results. |\n",
       "\n",
       "---\n",
       "\n",
       "## Bottom‚ÄëLine Takeaway\n",
       "\n",
       "* **LLM Apps** are ‚Äúprompt‚Äëdriven generators.‚Äù They‚Äôre fast to build, easy to deploy, and great for single‚Äëstep tasks.  \n",
       "* **Agentic Apps** are ‚Äúgoal‚Äëdriven orchestrators.‚Äù They‚Äôre more complex to design, require state and tool integration, but enable sophisticated, autonomous workflows that can interact with the world.\n",
       "\n",
       "Understanding this distinction early on helps you choose the right architecture for your product, align engineering resources, and set realistic expectations about performance, cost, and safety. Happy building!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Using OpenAIs Responses API\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "# Load GROQ API key from .env\n",
    "load_dotenv(\"../keys/.env\", override=True)\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "# The OpenAI client defaults to OpenAI‚Äôs servers,so you must specify the base_url to Groq‚Äôs OpenAI-compatible API endpoint (when using a Groq API key with the OpenAI client).\n",
    "client = OpenAI(base_url=\"https://api.groq.com/openai/v1\", api_key=groq_api_key) \n",
    "\n",
    "# Use Responses API (only works with openai/gpt-oss models)\n",
    "response = client.responses.create(\n",
    "    model=\"openai/gpt-oss-20b\",\n",
    "    input=[\n",
    "        {\"role\": \"system\", \"content\": \"You are an expert in LLM engineering.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Differentiate between LLM apps and Agentic apps\"}\n",
    "    ],\n",
    "    temperature=1,\n",
    "    top_p=1,\n",
    "    max_output_tokens=8192,\n",
    "    reasoning={\"effort\":\"high\"},   # \"minimal\", \"low\", \"medium\", \"high\"\n",
    "    stream=False\n",
    ")\n",
    "\n",
    "#print(response.output_text)\n",
    "display(Markdown(response.output_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1cc9f6-b6e5-46d1-9a05-57fce9cbb20b",
   "metadata": {},
   "source": [
    "# <span style='background :lightgreen' >4. Hands-On Practice Examples with Groq Hosted Models using OpenAI's `Responses` API</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0975415c-db75-4fd1-9bf1-d776876b7307",
   "metadata": {},
   "source": [
    "## a. Writing a Function for our ease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a0e0af6c-7a12-4167-8e67-cb4d16f5b677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# User Define Function that accesses models hosted by Groq using Groq's API key and OpenAIs Responses API \n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "# Load GROQ API key from .env\n",
    "load_dotenv(\"../keys/.env\", override=True)\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "# The OpenAI client defaults to OpenAI‚Äôs servers,so you must specify the base_url to Groq‚Äôs OpenAI-compatible API endpoint (when using a Groq API key with the OpenAI client).\n",
    "client = OpenAI(base_url=\"https://api.groq.com/openai/v1\", api_key=groq_api_key) \n",
    "\n",
    "def ask_groq(\n",
    "    user_prompt: str,\n",
    "    developer_prompt: str = \"You are a helpful assistant that provides concise answers.\",\n",
    "    model: str = \"llama-3.3-70b-versatile\", # \"openai/gpt-oss-20b\",\n",
    "    max_output_tokens: int | None = 1024,\n",
    "    temperature: float = 0.7,\n",
    "    top_p: float = 1.0,\n",
    "    text: dict = {\"format\": {\"type\": \"text\"}},\n",
    "    stream: bool = False,\n",
    "    reasoning: dict | None = None\n",
    "):\n",
    "    \n",
    "    # Prepare input messages as a list of role/content dictionaries\n",
    "    input_messages = [{\"role\": \"developer\", \"content\": developer_prompt}, {\"role\": \"user\", \"content\": user_prompt}]\n",
    "\n",
    "    # Responses API call\n",
    "    response = client.responses.create(\n",
    "        input=input_messages,\n",
    "        model=model,\n",
    "        max_output_tokens=max_output_tokens,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        text=text,\n",
    "        stream=stream,\n",
    "        reasoning=reasoning\n",
    "    )\n",
    "\n",
    "    \n",
    "    if stream:                    # Return streaming generator if requested\n",
    "        return response\n",
    "    return response.output_text   # Return the aggregated text output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e2f91b-d30a-4e50-9fcd-da54a4ff05f4",
   "metadata": {},
   "source": [
    "## a. Examples (Question Answering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b79e8fba-a1c3-474f-803a-d406e6c14958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the neural network go to therapy?\n",
      "\n",
      "Because it was struggling to converge on its emotions! (get it? converge, like in gradient descent?)\n"
     ]
    }
   ],
   "source": [
    "developer_prompt = \"You are an assistant that is great at telling jokes\"\n",
    "user_prompt = \"Tell a light-hearted joke for an audience of Data Scientists\"\n",
    "response = ask_groq(user_prompt=user_prompt, developer_prompt=developer_prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7cf8a9fd-4177-455e-9356-ee7595b1410a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snuggle in tight, for I have a tale to tell that will transport you to the mystical lands of Arabia. It's the legendary story of Ali Baba and the Forty Thieves.\n",
      "\n",
      "Once upon a time, in the bustling city of Baghdad, there lived a poor woodcutter named Ali Baba. He resided with his wife and a young brother, Qasim, who had married a wealthy merchant's daughter. Qasim's wife had brought a considerable dowry into their marriage, making them relatively affluent compared to Ali Baba.\n",
      "\n",
      "One day, while Ali Baba was out collecting firewood in the forest, he stumbled upon a group of forty robbers, known as the Chalees Chor. They were infamous for their cunning and brutality, striking fear into the hearts of all who crossed their path. The leader of the thieves, a towering figure with a black beard, ordered his men to hide their loot in a nearby cave.\n",
      "\n",
      "As Ali Baba watched from a safe distance, the thieves used a magical phrase to open the cave: \"Open Sesame!\" The cave door swung open, revealing a vast treasure trove filled with gold, jewels, and precious artifacts. The thieves stored their plunder and sealed the cave with the phrase \"Close Sesame!\"\n",
      "\n",
      "Ali Baba, being a clever and resourceful man, remembered the magical phrase and decided to investigate the cave further. He made his way back to the cave, uttered the words \"Open Sesame!\", and found himself surrounded by the dazzling treasures. He loaded his donkeys with as much gold and jewels as they could carry and returned home.\n",
      "\n",
      "Ali Baba's life was forever changed, and he became wealthy beyond his wildest dreams. However, he soon realized that his newfound riches had also brought him trouble. His brother, Qasim, grew jealous of Ali Baba's sudden prosperity and demanded to know the source of his wealth. Ali Baba, fearing that Qasim might reveal his secret, refused to tell him.\n",
      "\n",
      "Qasim, determined to uncover the truth, followed Ali Baba to the cave and discovered the magical phrase. He entered the cave, but in his greed, he forgot the words to exit. The thieves, who had been tracking Qasim, caught him and killed him, leaving his body in the cave.\n",
      "\n",
      "When Qasim failed to return, Ali Baba set out to find him. He discovered his brother's lifeless body in the cave and realized that the thieves were still at large. Using his cunning, Ali Baba devised a plan to outwit the Chalees Chor and bring them to justice.\n",
      "\n",
      "Disguising himself as an oil merchant, Ali Baba offered to sell oil to the thieves, who were hiding in the city. Unbeknownst to them, Ali Baba had filled the oil jars with his most trusted servants, who were armed and ready to attack. As the thieves opened the jars, Ali Baba's men jumped out and overpowered them.\n",
      "\n",
      "The leader of the thieves, realizing they had been outsmarted, attempted to escape. However, Ali Baba had set a trap for him, and the thief was caught and brought to justice. The remaining thieves were either killed or captured, and their leader was executed for his crimes.\n",
      "\n",
      "With the Chalees Chor defeated and their treasure recovered, Ali Baba was hailed as a hero. He used his wealth to help those in need and lived a long, prosperous life, always remembering the magical phrase that had changed his fortunes: \"Open Sesame!\"\n",
      "\n",
      "And so, with the tale of Ali Baba and the Forty Thieves coming to an end, it's time for you to drift off to sleep, with visions of magical caves, clever heroes, and the thrill of adventure dancing in your dreams. Sleep tight, and may your slumber be as peaceful as a summer night in the enchanted land of Arabia."
     ]
    }
   ],
   "source": [
    "developer_prompt = \"You are a bedtime storyteller.\"\n",
    "user_prompt = \"Tell me a bedtime story of Ali Baba and Chalees Chor\"\n",
    "\n",
    "# Get streaming generator from Responses API\n",
    "response = ask_groq(user_prompt=user_prompt, developer_prompt=developer_prompt, stream=True)\n",
    "\n",
    "# Iterate through streaming events and only print text deltas\n",
    "for event in response:\n",
    "    # Each event may contain incremental text in event.delta\n",
    "    if hasattr(event, \"delta\") and event.delta:\n",
    "        print(event.delta, end=\"\", flush=True) # prints the content from this chunk, end=\"\" prevents adding a newline after each  piece and flush=True forces flushing output to screen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f715a3f3-859a-46e4-9541-eb8fcd5c785d",
   "metadata": {},
   "source": [
    "## b. Question Answering from Content Passed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "525c34ed-afb0-4cfb-ad60-556833a2bff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cricket in Pakistan has always been more than just a sport‚Äîit‚Äôs a source of national pride and unity. Legendary players like Imran Khan, Wasim Akram, and Shahid Afridi set high standards in the past, inspiring generations to follow. Today, stars such as Babar Azam, Shaheen Shah Afridi, and Shadab Khan carry forward the legacy, leading the national team in international tournaments with skill and determination. Their performances not only thrill fans but also keep Pakistan among the top cricketing nations of the world.\n",
      "\n",
      "Politics in Pakistan, meanwhile, remains dynamic and often turbulent, with key figures shaping the country‚Äôs direction. Leaders like Nawaz Sharif, Asif Ali Zardari, and Imran Khan have all held significant influence over the nation‚Äôs governance and policies. In recent years, the political scene has seen sharp divisions, with parties such as the Pakistan Muslim League-Nawaz (PML-N), Pakistan Peoples Party (PPP), and Pakistan Tehreek-e-Insaf (PTI) competing for power. Debates around economic reforms, governance, and foreign policy continue to dominate the national conversation, reflecting the challenges and aspirations of the Pakistani people."
     ]
    }
   ],
   "source": [
    "!cat ../data/names.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ddcfc14b-b31a-4167-845d-079be93cd926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the names mentioned in the text:\n",
      "\n",
      "1. Imran Khan\n",
      "2. Wasim Akram\n",
      "3. Shahid Afridi\n",
      "4. Babar Azam\n",
      "5. Shaheen Shah Afridi\n",
      "6. Shadab Khan\n",
      "7. Nawaz Sharif\n",
      "8. Asif Ali Zardari\n",
      "\n",
      "These names include both cricket players and politicians.\n"
     ]
    }
   ],
   "source": [
    "with open(\"../data/names.txt\", \"r\") as f:\n",
    "    file_content = f.read()\n",
    "\n",
    "user_prompt = f\"Extract names from this text:\\n{file_content}\"\n",
    "response = ask_groq(user_prompt=user_prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9414fe7e-c835-4b6d-ba2a-3e4a21b5f032",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the names of Cricket players mentioned in the text:\n",
      "\n",
      "1. Imran Khan\n",
      "2. Wasim Akram\n",
      "3. Shahid Afridi\n",
      "4. Babar Azam\n",
      "5. Shaheen Shah Afridi\n",
      "6. Shadab Khan\n",
      "\n",
      "Note: Imran Khan is also mentioned as a political leader in the second part of the text, but he is primarily known for his cricketing career.\n"
     ]
    }
   ],
   "source": [
    "with open(\"../data/names.txt\", \"r\") as f:\n",
    "    file_content = f.read()\n",
    "\n",
    "user_prompt = f\"Can you extract names the Cricket players from this text:\\n{file_content}\"\n",
    "response = ask_groq(user_prompt=user_prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bba74c35-1194-48d5-b563-85f41cbe4ca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The text can be categorized as:\n",
      "\n",
      "1. Informative/Descriptive: The text provides information about two main topics in Pakistan: cricket and politics. It describes the significance of cricket in Pakistani culture and the current state of politics in the country.\n",
      "\n",
      "2. Cultural/Current Events: The text touches on the cultural significance of cricket in Pakistan and provides an overview of the current political landscape, making it relevant to cultural and current events categories.\n",
      "\n",
      "3. Non-fiction/Expository: The text is written in a formal, expository style, providing factual information about cricket and politics in Pakistan, without expressing a personal opinion or bias.\n",
      "\n",
      "4. Country/Society: The text can also be categorized under country or society, as it provides an overview of two important aspects of Pakistani society: sports and politics.\n"
     ]
    }
   ],
   "source": [
    "with open(\"../data/names.txt\", \"r\") as f:\n",
    "    file_content = f.read()\n",
    "\n",
    "user_prompt = f\"Can you categorize the following text:\\n{file_content}\"\n",
    "response = ask_groq(user_prompt=user_prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec5fc2d-4344-469d-9d91-ad0df58fdf4b",
   "metadata": {},
   "source": [
    "## c. Examples (Binary Classification: Sentiment analysis, Spam detection, Medical diagnosis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9bbe9394-7c58-4edd-bd57-5a03500ad507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative\n"
     ]
    }
   ],
   "source": [
    "user_prompt = \"\"\"\n",
    "Categorize the sentence 'The delivery was delayed and the product arrived damaged.' into one of the following categories:\n",
    "Positive\n",
    "Negative\n",
    "Answer with just the category, no need of any explaination\n",
    "\"\"\"\n",
    "response = ask_groq(user_prompt=user_prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39f3bce-d863-4f34-a901-ad051d5e8ffa",
   "metadata": {},
   "source": [
    "## d. Examples (Multi-class Classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "589da5cb-298b-4091-bf45-7b2f2e27a01b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neutral\n"
     ]
    }
   ],
   "source": [
    "user_prompt = \"\"\"\n",
    "Categorize the sentence 'The movie had great visuals but the plot was confusing and boring' into one of the following categories:\n",
    "Positive\n",
    "Negative\n",
    "Neutral\n",
    "Answer with just the category, no need of any explaination\n",
    "\"\"\"\n",
    "response = ask_groq(user_prompt=user_prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1acc0b-2b80-4407-ac82-a12f782436c7",
   "metadata": {},
   "source": [
    "## e. Examples (Text Generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "89298d2c-dc6c-43f8-9b50-dd46fbb7bdf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Elections held in Pakistan on February 08, 2024, were largely deemed fair, with minimal reported irregularities. Independent observers praised the transparency and efficiency of the electoral process, despite some pre-poll concerns and opposition claims of potential manipulation, overall outcome was considered credible by international monitors and local stakeholders.\"\n"
     ]
    }
   ],
   "source": [
    "developer_prompt = \"You are an expert of political science and history and have a deep understanding of policical situation of Pakistan.\"\n",
    "user_prompt = \"Write down a 50 words summary about the fairness of general elections held in Pakistan on February 08, 2024.\"\n",
    "response = ask_groq(user_prompt=user_prompt, developer_prompt=developer_prompt, temperature=1.0)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d101ce-1da5-489f-9035-a2d792184043",
   "metadata": {},
   "source": [
    "## f. Examples (Code Generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "240f604f-ffbf-4ba5-a279-8c07ab6763ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a simple C program that generates the first ten numbers of the Fibonacci sequence:\n",
      "\n",
      "```c\n",
      "#include <stdio.h>\n",
      "\n",
      "// Function to generate Fibonacci sequence\n",
      "void generateFibonacci(int n) {\n",
      "    int num1 = 0, num2 = 1;\n",
      "\n",
      "    // Print the first two numbers of the sequence\n",
      "    printf(\"%d, %d, \", num1, num2);\n",
      "\n",
      "    // Generate the rest of the sequence\n",
      "    for (int i = 3; i <= n; i++) {\n",
      "        int nextNum = num1 + num2;\n",
      "        printf(\"%d, \", nextNum);\n",
      "        num1 = num2;\n",
      "        num2 = nextNum;\n",
      "    }\n",
      "}\n",
      "\n",
      "int main() {\n",
      "    int n = 10;  // Number of terms in the sequence\n",
      "    printf(\"The first %d numbers of the Fibonacci sequence are: \", n);\n",
      "    generateFibonacci(n);\n",
      "    return 0;\n",
      "}\n",
      "```\n",
      "\n",
      "When you run this program, it will output:\n",
      "\n",
      "```\n",
      "The first 10 numbers of the Fibonacci sequence are: 0, 1, 1, 2, 3, 5, 8, 13, 21, 34,\n",
      "```\n",
      "\n",
      "This program works by starting with the first two numbers in the Fibonacci sequence (0 and 1), and then using a loop to generate the rest of the sequence. It does this by adding the last two numbers in the sequence to get the next number, and then updating the last two numbers to be the last number and the new number."
     ]
    }
   ],
   "source": [
    "developer_prompt = \"You are an expert of C programing in C language.\"\n",
    "user_prompt = \"Write down a C program that generates first ten numbers of fibonacci sequence.\"\n",
    "\n",
    "# Get streaming generator from Responses API\n",
    "response = ask_groq(user_prompt=user_prompt, developer_prompt=developer_prompt, stream=True)\n",
    "\n",
    "# Iterate through streaming events and only print text deltas\n",
    "for event in response:\n",
    "    # Each event may contain incremental text in event.delta\n",
    "    if hasattr(event, \"delta\") and event.delta:\n",
    "        print(event.delta, end=\"\", flush=True) # prints the content from this chunk, end=\"\" prevents adding a newline after each  piece and flush=True forces flushing output to screen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59419ea7-4e7a-4e81-b0a9-1077c0646c37",
   "metadata": {},
   "source": [
    "## g. Examples (Text Translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f032442a-499d-4910-9b8f-39c621898167",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ÿßÿ≥ ÿ≥ÿßŸÑ ⁄©€í ÿ®ÿ¨Ÿπ ⁄©ÿß ⁄©ŸÖ ÿ™ŸÜÿÆŸàÿß€Å ŸÑ€åŸÜ€í ŸàÿßŸÑ€í ŸÑŸà⁄ØŸà⁄∫ Ÿæÿ± ÿ®€Åÿ™ ÿ®ÿ±ÿß ÿßÿ´ÿ± Ÿæ⁄ë€í ⁄Øÿß€î\n"
     ]
    }
   ],
   "source": [
    "user_prompt = \"\"\"\n",
    "Please act as an expert of English to Urdu translator by translating the given sentence from English into Urdu.\n",
    "'The budget this year will have a very bad impact on the low salried people'\n",
    "\"\"\"\n",
    "response = ask_groq(user_prompt=user_prompt, model='meta-llama/llama-4-maverick-17b-128e-instruct')\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6604dcf4-31aa-4749-81db-77c7942feec0",
   "metadata": {},
   "source": [
    "## h. Examples (Text Summarization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3d11b223-2b1c-47cd-ab25-3577c46a9149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hugging Face transformers: a powerful NLP tool.\n"
     ]
    }
   ],
   "source": [
    "developer_prompt = \"You are an expert of English language.\"\n",
    "\n",
    "user_prompt = f'''\n",
    "Summarize the text below in at most 20 words:\n",
    "```The Hugging Face transformers library is an incredibly versatile and powerful tool for natural language processing (NLP).\n",
    "It allows users to perform a wide range of tasks such as text classification, named entity recognition, and question answering, among others.\n",
    "It's an extremely popular library that's widely used by the open-source data science community.\n",
    "It lowers the barrier to entry into the field by providing Data Scientists with a productive, convenient way to work with transformer models.```\n",
    "'''\n",
    "\n",
    "response = ask_groq(user_prompt=user_prompt, developer_prompt=developer_prompt, temperature=1.0)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0e6b0837-a38c-40f9-ab24-bdd7cbeeaaf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Summary:** Our solar system consists of eight planets, each with unique characteristics, orbiting around the Sun. The planets vary in size, atmosphere, and temperature, ranging from Mercury's swift orbit to the gas giants Jupiter and Saturn. The planets play a vital role in the symphony of our solar system.\n",
      "\n",
      "**Urdu Translation:** €ÅŸÖÿßÿ±ÿß ÿ¥ŸÖÿ≥€å ŸÜÿ∏ÿßŸÖ ÿ¢Ÿπ⁄æ ÿ≥€åÿßÿ±Ÿà⁄∫ Ÿæÿ± ŸÖÿ¥ÿ™ŸÖŸÑ €Å€íÿå €Åÿ± ÿß€å⁄© ÿßŸæŸÜ€å ÿßŸÜŸÅÿ±ÿßÿØ€åÿ™ ⁄©€í ÿ≥ÿßÿ™⁄æÿå ÿ≥Ÿàÿ±ÿ¨ ⁄©€í ⁄Øÿ±ÿØ ⁄Øÿ±ÿØÿ¥ ⁄©ÿ±ÿ™ÿß €Å€í€î ÿ≥€åÿßÿ±€í ÿßŸæŸÜ€í ÿ≥ÿßÿ¶ÿ≤ÿå ŸÅÿ∂ÿß ÿßŸàÿ± ÿØÿ±ÿ¨€Å ÿ≠ÿ±ÿßÿ±ÿ™ ŸÖ€å⁄∫ ŸÖÿÆÿ™ŸÑŸÅ €ÅŸàÿ™€í €Å€å⁄∫ÿå ÿ¨€åÿ≥€í ⁄©€Å ÿπÿ∑ÿßÿ±ŸêÿØ ⁄©€å ÿ™€åÿ≤ ÿ±ŸÅÿ™ÿßÿ± ⁄Øÿ±ÿØÿ¥ ÿ≥€í ŸÑ€í ⁄©ÿ± ⁄Ø€åÿ≥ ⁄©€í ÿπÿ∏€åŸÖ ÿ≥€åÿßÿ±€í ŸÖÿ¥ÿ™ÿ±€å ÿßŸàÿ± ÿ≤ÿ≠ŸÑ ÿ™⁄©€î ÿ≥€åÿßÿ±€í €ÅŸÖÿßÿ±€í ÿ¥ŸÖÿ≥€å ŸÜÿ∏ÿßŸÖ ⁄©€å ÿ≥ŸÖŸÅŸÜ€å ŸÖ€å⁄∫ ÿß€ÅŸÖ ⁄©ÿ±ÿØÿßÿ± ÿßÿØÿß ⁄©ÿ±ÿ™€í €Å€å⁄∫€î\n",
      "\n",
      "**Python List:**\n",
      "```python\n",
      "planet_names = [\n",
      "    \"Mercury\",\n",
      "    \"Venus\",\n",
      "    \"Earth\",\n",
      "    \"Mars\",\n",
      "    \"Jupiter\",\n",
      "    \"Saturn\",\n",
      "    \"Uranus\",\n",
      "    \"Neptune\"\n",
      "]\n",
      "print(planet_names)\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "developer_prompt = \"You are a helpful assistant skilled in text summarization, translation to Urdu, and Python programming. You provide clear, accurate responses and follow instructions precisely.\"\n",
    "\n",
    "text = '''\n",
    "Our solar system, a celestial dance of eight planets, each with its unique character and charm, orbits around our radiant Sun.\n",
    "Closest to the Sun, Mercury, the smallest planet, darts swiftly, its metallic surface reflecting the Sun's intense glare.\n",
    "Venus, Earth's twin, cloaked in a dense atmosphere, harbors scorching temperatures and acidic clouds.\n",
    "Earth, our oasis of life, teems with diverse ecosystems, its oceans and landforms sculpted by the forces of nature.\n",
    "Mars, the Red Planet, bears the scars of ancient volcanoes and the promise of potential life.\n",
    "Beyond the asteroid belt, Jupiter and Saturn, the gas giants, reign supreme, their vast atmospheres swirling with storms and adorned with rings of ice and dust.\n",
    "Uranus and Neptune, the ice giants, tilt at odd angles, their atmospheres frigid and their depths still shrouded in mystery.\n",
    "Each planet, a celestial masterpiece, plays a vital role in the intricate symphony of our solar system.'''\n",
    "\n",
    "user_prompt = f'''\n",
    "Please complete the following two tasks based on the text provided below:\n",
    "\n",
    "Task 1: Summarize the text in 2-3 sentences, then translate that summary into Urdu.\n",
    "\n",
    "Task 2: Create a Python list containing all planet names mentioned in the text.\n",
    "\n",
    "Text: ```{text}```\n",
    "\n",
    "Please format your response as:\n",
    "**Summary:** [English summary]\n",
    "**Urdu Translation:** [Urdu translation]\n",
    "**Python List:** [Python code with planet names]\n",
    "'''\n",
    "\n",
    "response = ask_groq(user_prompt=user_prompt, developer_prompt=developer_prompt, model='meta-llama/llama-4-maverick-17b-128e-instruct', temperature=0.3)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686e5033-5745-4393-9e00-4ca8a6587ee4",
   "metadata": {},
   "source": [
    "## i. Examples (Named Entity Recognition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "67f047e5-d33c-49a8-ae5c-62d49e3ef7e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a summary of Zelaid Mujahid's profile:\n",
      "\n",
      "**Name:** Zelaid Mujahid\n",
      "**Nationality:** Pakistani\n",
      "**University:** University of the Punjab\n",
      "**Major:** Data Science\n",
      "**Year:** Sophomore\n",
      "**GPA:** 3.5\n",
      "**Extracurricular Activity:** Active member of the AI Club\n",
      "**Career Aspiration:** Pursue a career in AI after graduating\n"
     ]
    }
   ],
   "source": [
    "developer_prompt = \"\"\"You are a  Named Entity Recognition specialist. Extract and classify entities from the given text into these categories only if they exist:\n",
    "- name\n",
    "- major\n",
    "- university\n",
    "- nationality\n",
    "- grades\n",
    "- club\n",
    "Format your response as: 'Entity: [text] | Type: [category]' with each entity on a new line.\"\"\"\n",
    "\n",
    "user_prompt = '''\n",
    "Zelaid Mujahid is a sophomore majoring in Data Science at University of the Punjab. \\\n",
    "He is Pakistani national and has a 3.5 GPA. Mujahid is an active member of the department's AI Club.\\\n",
    "He hopes to pursue a career in AI after graduating.\n",
    "'''\n",
    "response = ask_groq(user_prompt=user_prompt, developer_prompt=developer_prompt, model='meta-llama/llama-4-maverick-17b-128e-instruct', temperature=0.3)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be09b49-a9cc-419d-8f1c-8a3065b3be34",
   "metadata": {},
   "source": [
    "## j. Example (Grade School Math 8K (GSM8K))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4395f12a-9b07-43e6-ba58-9eb33fb9c0ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To determine how much the booth earned, we need more information such as:\n",
      "\n",
      "1. The daily revenue of the booth\n",
      "2. The daily cost of ingredients\n",
      "3. The total rent paid for 5 days\n",
      "\n",
      "Without this information, it's impossible to calculate the earnings of the booth. If you provide the necessary data, I can help you with the calculation.\n"
     ]
    }
   ],
   "source": [
    "developer_prompt = \"\"\"You are an expert School math teacher. \n",
    "Consider the following text and then answer the questions of the students from this:\n",
    "A carnival snack booth made $50 selling popcorn each day. It made three times as much selling cotton candy. \n",
    "For a 5-day activity, the booth has to pay $30 rent and $75 for the cost of the ingredients. \n",
    "\"\"\"\n",
    "user_prompt = \"How much did the booth earn for 5 days after paying the rent and the cost of ingredients?\"\n",
    "\n",
    "response = ask_groq(user_prompt=user_prompt, developer_prompt=developer_prompt,  model='llama-3.3-70b-versatile')\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4561cd52-2c50-4018-83e5-9e34dc83c4a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Step 1: Calculate the total earnings for 5 days\n",
      "First, we need to determine the total earnings of the booth over the 5-day period. Let's assume the booth earns $200 per day. So, the total earnings for 5 days = $200 * 5 = $1000.\n",
      "\n",
      "## Step 2: Calculate the total rent paid for 5 days\n",
      "Next, we need to find out the total rent paid for the 5 days. Let's assume the daily rent is $50. So, the total rent for 5 days = $50 * 5 = $250.\n",
      "\n",
      "## Step 3: Calculate the total cost of ingredients for 5 days\n",
      "Now, we need to determine the total cost of ingredients for the 5 days. Let's assume the daily cost of ingredients is $30. So, the total cost of ingredients for 5 days = $30 * 5 = $150.\n",
      "\n",
      "## Step 4: Calculate the total earnings after paying the rent and the cost of ingredients\n",
      "To find the earnings after paying the rent and the cost of ingredients, we need to subtract the total rent and the total cost of ingredients from the total earnings. So, the earnings after deductions = total earnings - (total rent + total cost of ingredients) = $1000 - ($250 + $150) = $1000 - $400 = $600.\n",
      "\n",
      "The final answer is: $\\boxed{600}$\n"
     ]
    }
   ],
   "source": [
    "developer_prompt = \"\"\"You are an expert School math teacher. \n",
    "Consider the following text and then answer the questions of the students from this:\n",
    "A carnival snack booth made $50 selling popcorn each day. It made three times as much selling cotton candy. \n",
    "For a 5-day activity, the booth has to pay $30 rent and $75 for the cost of the ingredients. \n",
    "\"\"\"\n",
    "user_prompt = \"How much did the booth earn for 5 days after paying the rent and the cost of ingredients?\"\n",
    "\n",
    "response = ask_groq(user_prompt=user_prompt, developer_prompt=developer_prompt,  model='meta-llama/llama-4-maverick-17b-128e-instruct')\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0b5dc8c2-136e-4ff7-87d4-f44778fe5d8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Step‚Äëby‚Äëstep solution**\n",
      "\n",
      "1. **Daily earnings from popcorn**  \n",
      "   The booth sells popcorn for **$50 each day**.\n",
      "\n",
      "2. **Daily earnings from cotton candy**  \n",
      "   It makes *three times* as much from cotton candy.  \n",
      "   \\[\n",
      "   3 \\times \\$50 = \\$150 \\text{ per day}\n",
      "   \\]\n",
      "\n",
      "3. **Total daily revenue**  \n",
      "   \\[\n",
      "   \\$50 \\;(\\text{popcorn}) + \\$150 \\;(\\text{cotton candy}) = \\$200 \\text{ per day}\n",
      "   \\]\n",
      "\n",
      "4. **Revenue for the 5‚Äëday activity**  \n",
      "   \\[\n",
      "   5 \\text{ days} \\times \\$200 \\text{ per day} = \\$1,000\n",
      "   \\]\n",
      "\n",
      "5. **Total costs**  \n",
      "   - Rent: **$30**  \n",
      "   - Ingredients: **$75**  \n",
      "   \\[\n",
      "   \\$30 + \\$75 = \\$105\n",
      "   \\]\n",
      "\n",
      "6. **Net earnings after costs**  \n",
      "   \\[\n",
      "   \\$1,000 - \\$105 = \\$895\n",
      "   \\]\n",
      "\n",
      "---\n",
      "\n",
      "**Answer:** The booth earned **$895** for the 5‚Äëday activity after paying rent and ingredient costs.\n"
     ]
    }
   ],
   "source": [
    "developer_prompt = \"\"\"You are an expert School math teacher. \n",
    "Consider the following text and then answer the questions of the students from this:\n",
    "A carnival snack booth made $50 selling popcorn each day. It made three times as much selling cotton candy. \n",
    "For a 5-day activity, the booth has to pay $30 rent and $75 for the cost of the ingredients. \n",
    "\"\"\"\n",
    "user_prompt = \"How much did the booth earn for 5 days after paying the rent and the cost of ingredients?\"\n",
    "\n",
    "response = ask_groq(user_prompt=user_prompt, developer_prompt=developer_prompt, model='openai/gpt-oss-20b')\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6fe112-1946-498b-8aaa-71741ee17263",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e184fb7-8dea-4470-811d-56a865dcae57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a127546-1f11-4964-82bc-40a88189ef2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (genai-uv)",
   "language": "python",
   "name": "genai-uv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
