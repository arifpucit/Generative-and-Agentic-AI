{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ffe9c6a0-8544-4467-89c9-903ea98a6890",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "---   \n",
    " <img align=\"left\" width=\"75\" height=\"75\"  src=\"https://upload.wikimedia.org/wikipedia/en/c/c8/University_of_the_Punjab_logo.png\"> \n",
    "\n",
    "<h1 align=\"center\">Department of Data Science</h1>\n",
    "\n",
    "---\n",
    "<h3><div align=\"right\">Instructor: Muhammad Arif Butt, Ph.D.</div></h3>    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae713dd3-ae7c-4b25-95d0-fb22c055583f",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "<h1 align=\"center\">Lec-07: Accessing Open-Source AI Models via Groq, Hugging Face and OpenAI-Compatible Inference APIs</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6c8f2f-5ada-480c-b66f-11afc1aaa6f1",
   "metadata": {},
   "source": [
    "# Learning agenda of this notebook\n",
    "\n",
    "1. Accessing Open-Source AI Models hosted on **Hugging Face Hub**\n",
    "    - Access Option 1: Access with Hugging Face InferenceClient\n",
    "    - Access Option 2: Access with OpenAI Chat Completions API (Hugging Face Router)\n",
    "    - Access Option 3: Access with OpenAI Responses API (Hugging Face Router)\n",
    "2. Hands-On Practice Examples with Hugging Face Hosted Models using OpenAI's `Responses` API\n",
    "3. Accessing Open-Source AI Models hosted on **Groq**\n",
    "    - Access Option 1: Access with Groq Chat Completions API\n",
    "    - Access Option 2: Access with OpenAI Chat Completions API (Groq Router)\n",
    "    - Access Option 3: Access with OpenAI Responses API (Groq Router)\n",
    "4. Hands-On Practice Examples with Groq Hosted Models using OpenAI's `Responses` API "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0fe4822-176c-4fcf-aa41-b4400ae54d7f",
   "metadata": {},
   "source": [
    "# <span style='background :lightgreen' >Recap: Ways to Access Open Source LLMs</span>\n",
    "### (i) Access Open-Source Models via Cloud-Based Providers (Driving a fully automatic car — everything managed for you)\n",
    "* Cloud inference providers host the models for you, removing the need for GPUs, scaling infrastructure, or deployment engineering.\n",
    "* You interact with models using simple HTTP calls or OpenAI-compatible APIs, making it the quickest way to use LLMs in production.\n",
    "* Services like **Groq** offer ultra-fast inference on custom LPU hardware with extremely low latency for models such as Llama, Qwen, Mixtral, Whisper, etc.\n",
    "* **Hugging Face Inference** provides access to 1M+ models via Serverless Inference, TGI, or Inference Endpoints—pay-as-you-go, secure, and instantly deployable.\n",
    "\n",
    "### (ii) Run Open-Source Models locally using runtimes (Driving an automatic car — local but simple, no gears or engineering)\n",
    "\n",
    "### (iii) Use Open-Source Models via Hugging Face `pipeline()` API (Driving a manual car — you see more of the mechanics, but still a car someone else built)\n",
    "\n",
    "### (iv) Load and run models directly from Hugging Face Hub using `AutoModel/AutoTokenizer` (Opening the hood and adjusting or replacing engine components)\n",
    "\n",
    "\n",
    "### (v) Fine-Tune LLMs using full fine-tuning or PEFT methods (LoRA / QLoRA / adapters) (Upgrading and re-calibrating the engine to suit your driving style)\n",
    "\n",
    "### (vi) Build and train an AI Model from scratch using PyTorch / TensorFlow (Designing and building the entire car from raw parts — full control, full responsibility)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461a2f38-15ea-436a-9d09-9b06ca957ed4",
   "metadata": {},
   "source": [
    "# <span style='background :lightgreen' >1. Accessing Open-Source AI Models Hosted on HF-Managed Infrastructure</span>\n",
    "- The Hugging Face Hub hosts millions of open-source models. A subset of these are deployed on Hugging Face's **Serverless Inference API**.\n",
    "- This is called serverless because:\n",
    "    - You do not manage infrastructure\n",
    "    - No GPUs to provision\n",
    "    - No scaling concerns\n",
    "- However, not all models on the Hub are available via serverless inference. Some models are only downloadable and work locally with the `transformers` library. To check if a model supports Serverless Inference, visit the model page:\n",
    "    - \"Inference API\" widget visible (or \"Deploy\" dropdown with inference options), e.g., `gpt2`, `google/flan-t5-base`, `facebook/bart-large-cnn`\n",
    "    - Only \"Use in Transformers\" or \"Use this model\" shown, e.g., `mistralai/Mistral-7B-v0.1`, `tiiuae/falcon-40b`, `EleutherAI/gpt-j-6B`\n",
    "- Some models like Llama-2 and Llama-3 are **gated** and require access approval. Additionally, some models may require a PRO or Enterprise subscription for serverless inference access. For example, Meta's `meta-llama/Llama-3.1-70B-Instruct` is a gated model. To gain access:\n",
    "    1. Click \"Agree and access repository\" (or similar button)\n",
    "    2. Accept the license terms\n",
    "    3. Wait for access approval (usually granted automatically or within hours)\n",
    "    4. Once granted, the UI will indicate: *\"You have been granted access to this model\"* or similar confirmation\n",
    "- You can access HF-hosted models via:\n",
    "    - Hugging Face InferenceClient API\n",
    "    - OpenAI Chat Completion API (Hugging Face Router)\n",
    "    - OpenAI Responses API (Hugging Face Router)\n",
    "    - Text Generation Inference (TGI) for self-hosted deployments\n",
    "- For larger models, higher rate limits, or commercial usage, serverless inference may require a paid subscription. Current pricing (as of January 2025) at [https://huggingface.co/pricing](https://huggingface.co/pricing):\n",
    "    - Free: Limited rate limits, access to public Inference API\n",
    "    - PRO: ~$9 per month (access to gated models, higher rate limits, PRO badge)\n",
    "    - Enterprise: Custom pricing (dedicated infrastructure, SLA, priority support, SSO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9119bdb6-07b6-4712-aea5-c181a3e076e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hugging Face Tokens exists and begins hf_oEyH\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv('../keys/.env', override=True) \n",
    "\n",
    "hf_token = os.getenv('HF_TOKEN')\n",
    "if hf_token:\n",
    "    print(f\"Hugging Face Tokens exists and begins {hf_token[:7]}\")\n",
    "else:\n",
    "    print(\"Hugging Face tokens not set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d36edb6-33c3-4314-a0d8-e9cdad87a9a1",
   "metadata": {},
   "source": [
    "## Access Option 1: Access with Hugging Face InferenceClient API\n",
    "```python\n",
    "InferenceClient(\n",
    "    model: Optional[str] = None,\n",
    "    provider: Union[Literal[…], \"auto\", None] = None,\n",
    "    token: Optional[str] = None,\n",
    "    timeout: Optional[float] = None,\n",
    "    headers: Optional[dict[str, str]] = None,\n",
    "    cookies: Optional[dict[str, str]] = None,\n",
    "    bill_to: Optional[str] = None,\n",
    "    base_url: Optional[str] = None,\n",
    "    api_key: Optional[str] = None,  # alias to token\n",
    "    proxies: Optional[Any] = None,  # in some versions\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e3fcafd-b7eb-4fe4-9973-2ba83ba70d1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of Pakistan is Islamabad.\n",
      "llama3.1-8b\n",
      "ChatCompletionOutputUsage(completion_tokens=8, prompt_tokens=48, total_tokens=56, completion_tokens_details={'accepted_prediction_tokens': 0, 'rejected_prediction_tokens': 0, 'reasoning_tokens': 0}, prompt_tokens_details={'cached_tokens': 0})\n"
     ]
    }
   ],
   "source": [
    "# Using HF InferenceClient() and chat_completion() method\n",
    "import huggingface_hub\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv('../keys/.env', override=True) \n",
    "hf_token = os.getenv('HF_TOKEN')\n",
    "\n",
    "client = huggingface_hub.InferenceClient(\n",
    "                        model=\"meta-llama/Llama-3.1-8B-Instruct\", # Model ID from the Hugging Face Hub (e.g., \"meta-llama/Llama-3.1-8B-Instruct\") or a URL to a deployed inference endpoint\n",
    "                        provider=\"auto\",              #  Hugging Face supports multiple back-end inference providers (e.g., \"cerebras\", \"together\", \"replicate\", \"hf‑inference\", etc.).\n",
    "                        token=hf_token\n",
    "                        )\n",
    "response = client.chat_completion(\n",
    "                                messages=[\n",
    "                                            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                                            {\"role\": \"user\", \"content\": \"What is the capital of Pakistan.\"}\n",
    "                                        ],\n",
    "                                max_tokens=None,         # Default: None (no limit, up to model's max)\n",
    "                                temperature=None,        # Default: None (provider's default, usually ~0.7-1.0)\n",
    "                                top_p=1.0,               # Default: None (provider's default, usually 1.0)\n",
    "                                stream=False,            # Default: False\n",
    "                                stop=None,               # default: None, can provide list of stop tokens\n",
    "                                presence_penalty=0.0,    # Default: None (provider's default, usually 0.0)\n",
    "                                frequency_penalty=0.0    # Default: None (provider's default, usually 0.0)\n",
    "                            )\n",
    "\n",
    "print(response.choices[0].message.content) #the actual text answer you want\n",
    "print(response.model) # which model produced the result\n",
    "print(response.usage) #token usage details (handy for cost/efficiency if you were on OpenAI)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c10855-2f50-4702-876c-43016bb22497",
   "metadata": {},
   "source": [
    "## Access Option 2: Access with OpenAI Chat Completion API (Hugging Face Router)\n",
    "- Here are the key differences between the two approaches:\n",
    "    - Library dependency: InferenceClient is part of the huggingface_hub package and is HuggingFace-native, while the OpenAI approach uses the openai package pointed to HuggingFace's router OpenAI\n",
    "    - Provider routing: InferenceClient offers automatic provider selection with provider=\"auto\" and can route through multiple inference providers (Replicate, Together AI, Sambanova, etc.), while the OpenAI client requires manual base_url specification OpenAI\n",
    "    - Additional features: InferenceClient supports multiple task types beyond chat (text-to-image, embeddings, speech processing), while the OpenAI-compatible router currently only supports chat completion tasks OpenAIOpenAI\n",
    "    - Parameter flexibility: InferenceClient has extra_body parameter for provider-specific settings and more flexible initialization options, while OpenAI client uses standard OpenAI parameters only\n",
    "    - Syntax compatibility: Both produce identical outputs since client.chat_completion() is aliased as client.chat.completions.create() in InferenceClient for OpenAI compatibility OpenAI\n",
    "    - Use case optimization: InferenceClient is optimized for HuggingFace ecosystem with built-in provider management, while OpenAI client approach is better if you're already using OpenAI syntax across your codebase and want minimal changes\n",
    "\n",
    ">- **HuggingFace provides OpenAI-compatible endpoints through https://router.huggingface.co/v1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9e20481-3f84-4095-856c-403912aa2949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of Pakistan is Islamabad.\n",
      "{\n",
      "    \"id\": \"chatcmpl-0557bbfa-6e1f-44ab-b9d3-e0793fadb094\",\n",
      "    \"choices\": [\n",
      "        {\n",
      "            \"finish_reason\": \"stop\",\n",
      "            \"index\": 0,\n",
      "            \"logprobs\": null,\n",
      "            \"message\": {\n",
      "                \"content\": \"The capital of Pakistan is Islamabad.\",\n",
      "                \"refusal\": null,\n",
      "                \"role\": \"assistant\",\n",
      "                \"annotations\": null,\n",
      "                \"audio\": null,\n",
      "                \"function_call\": null,\n",
      "                \"tool_calls\": null\n",
      "            }\n",
      "        }\n",
      "    ],\n",
      "    \"created\": 1770882271,\n",
      "    \"model\": \"llama3.1-8b\",\n",
      "    \"object\": \"chat.completion\",\n",
      "    \"service_tier\": null,\n",
      "    \"system_fingerprint\": \"fp_5198798116a66ebf301b\",\n",
      "    \"usage\": {\n",
      "        \"completion_tokens\": 8,\n",
      "        \"prompt_tokens\": 48,\n",
      "        \"total_tokens\": 56,\n",
      "        \"completion_tokens_details\": {\n",
      "            \"accepted_prediction_tokens\": 0,\n",
      "            \"audio_tokens\": null,\n",
      "            \"reasoning_tokens\": 0,\n",
      "            \"rejected_prediction_tokens\": 0\n",
      "        },\n",
      "        \"prompt_tokens_details\": {\n",
      "            \"audio_tokens\": null,\n",
      "            \"cached_tokens\": 0\n",
      "        }\n",
      "    },\n",
      "    \"time_info\": {\n",
      "        \"queue_time\": 0.000390731,\n",
      "        \"prompt_time\": 0.00219247,\n",
      "        \"completion_time\": 0.003197427,\n",
      "        \"total_time\": 0.006964683532714844,\n",
      "        \"created\": 1770882271.7421818\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Using OpenAIs Chat Completion API\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "# Load GROQ API key from .env\n",
    "load_dotenv(\"../keys/.env\", override=True)\n",
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "# Create an OpenAI client instance and specify the base_url as \"https://router.huggingface.co/v1\" (OpenAI-compatible API endpoint).\n",
    "# \"https://router.huggingface.co/v1\" Hugging Face’s OpenAI-compatible “inference router” endpoint. It acts like a universal gateway that proxies your request to the correct model backend.\n",
    "client = OpenAI(base_url=\"https://router.huggingface.co/v1\", api_key=hf_token) \n",
    "\n",
    "# Use OpenAI's Chat Completions API (routed through Hugging Face)\n",
    "response = client.chat.completions.create(\n",
    "                                            model=\"meta-llama/Llama-3.1-8B-Instruct\", # \"openai/gpt-oss-20b:novita\"\n",
    "                                            messages=[\n",
    "                                                        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                                                        {\"role\": \"user\", \"content\": \"What is the capital of Pakistan?\"}\n",
    "                                                    ],\n",
    "                                            temperature=1,\n",
    "                                            top_p=1,\n",
    "                                            max_completion_tokens=8192,\n",
    "                                            reasoning_effort=None,   # \"medium\"\n",
    "                                            stream=False\n",
    "                                        )\n",
    "\n",
    "print(response.choices[0].message.content)\n",
    "print(response.model_dump_json(indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e28b5fe-843f-44bd-a368-1c1604937307",
   "metadata": {},
   "source": [
    "## Access Option 3: Access with OpenAI Responses API (Hugging Face Router)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c15134ec-79f3-4011-81f5-ac8766380da3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Islamabad is the capital of Pakistan.\n",
      "{\n",
      "    \"id\": \"resp_0df7b9503fad1979da7a7f225cb92c3567d7c46ff411a713\",\n",
      "    \"created_at\": 1770882510.0,\n",
      "    \"error\": null,\n",
      "    \"incomplete_details\": null,\n",
      "    \"instructions\": null,\n",
      "    \"metadata\": null,\n",
      "    \"model\": \"meta-llama/Llama-3.1-8B-Instruct\",\n",
      "    \"object\": \"response\",\n",
      "    \"output\": [\n",
      "        {\n",
      "            \"id\": \"msg_a1b33ec875b6cc2910fab0188f39ddd1ab0d5521ebdc58fa\",\n",
      "            \"content\": [\n",
      "                {\n",
      "                    \"annotations\": [],\n",
      "                    \"text\": \"Islamabad is the capital of Pakistan.\",\n",
      "                    \"type\": \"output_text\",\n",
      "                    \"logprobs\": null\n",
      "                }\n",
      "            ],\n",
      "            \"role\": \"assistant\",\n",
      "            \"status\": \"completed\",\n",
      "            \"type\": \"message\"\n",
      "        }\n",
      "    ],\n",
      "    \"parallel_tool_calls\": null,\n",
      "    \"temperature\": 1.0,\n",
      "    \"tool_choice\": \"auto\",\n",
      "    \"tools\": [],\n",
      "    \"top_p\": 1.0,\n",
      "    \"background\": null,\n",
      "    \"conversation\": null,\n",
      "    \"max_output_tokens\": 8192,\n",
      "    \"max_tool_calls\": null,\n",
      "    \"previous_response_id\": null,\n",
      "    \"prompt\": null,\n",
      "    \"prompt_cache_key\": null,\n",
      "    \"prompt_cache_retention\": null,\n",
      "    \"reasoning\": null,\n",
      "    \"safety_identifier\": null,\n",
      "    \"service_tier\": null,\n",
      "    \"status\": \"completed\",\n",
      "    \"text\": null,\n",
      "    \"top_logprobs\": null,\n",
      "    \"truncation\": null,\n",
      "    \"usage\": {\n",
      "        \"input_tokens\": 42,\n",
      "        \"input_tokens_details\": {\n",
      "            \"cached_tokens\": 0\n",
      "        },\n",
      "        \"output_tokens\": 9,\n",
      "        \"output_tokens_details\": {\n",
      "            \"reasoning_tokens\": 0\n",
      "        },\n",
      "        \"total_tokens\": 51\n",
      "    },\n",
      "    \"user\": null\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "# Load Hugging Face token from .env\n",
    "load_dotenv(\"../keys/.env\", override=True)\n",
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "# Create an OpenAI client instance and specify the base_url as \"https://router.huggingface.co/v1\" (OpenAI-compatible API endpoint).\n",
    "# \"https://router.huggingface.co/v1\" Hugging Face’s OpenAI-compatible “inference router” endpoint. It acts like a universal gateway that proxies your request to the correct model backend.\n",
    "client = OpenAI(base_url=\"https://router.huggingface.co/v1\", api_key=hf_token)\n",
    "\n",
    "# Use OpenAI Responses API (routed through Hugging Face)\n",
    "response = client.responses.create(\n",
    "                                    model=\"meta-llama/Llama-3.1-8B-Instruct\", \n",
    "                                    input=[\n",
    "                                            {\"role\": \"developer\", \"content\": \"You are a helpful assistant.\"},\n",
    "                                            {\"role\": \"user\", \"content\": \"What is the capital of Pakistan?\"}\n",
    "                                            ],\n",
    "                                    temperature=1,\n",
    "                                    top_p=1,\n",
    "                                    max_output_tokens=8192,\n",
    "                                    stream=False\n",
    "                                )\n",
    "\n",
    "# Display the model's response\n",
    "print(response.output_text)\n",
    "print(response.model_dump_json(indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7df9e8f-6775-4d25-9ecc-c3f5e3620299",
   "metadata": {},
   "source": [
    "# <span style='background :lightgreen' >2. Hands-On Practice Examples with Hugging Face Hosted Models using OpenAI's `Responses` API</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a217a6-34b2-4795-a443-4e16572e3777",
   "metadata": {},
   "source": [
    "## a. Writing a Function for our ease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4862fcad-f6e2-499e-87bb-160a7fea2d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "# Load Hugging Face token from .env\n",
    "load_dotenv(\"../keys/.env\", override=True)\n",
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "\n",
    "# Create an OpenAI client instance and specify the base_url as \"https://router.huggingface.co/v1\" (OpenAI-compatible API endpoint).\n",
    "# \"https://router.huggingface.co/v1\" Hugging Face’s OpenAI-compatible “inference router” endpoint. It acts like a universal gateway that proxies your request to the correct model backend.\n",
    "client = OpenAI(base_url=\"https://router.huggingface.co/v1\", api_key=hf_token) \n",
    "\n",
    "def ask_hf(\n",
    "    user_prompt: str,\n",
    "    developer_prompt: str = \"You are a helpful assistant that provides concise answers.\",\n",
    "    model: str = \"meta-llama/Llama-3.1-8B-Instruct\", \n",
    "    max_output_tokens: int = 1024,\n",
    "    temperature: float = 0.7,\n",
    "    top_p: float = 1.0,\n",
    "    stream: bool = False\n",
    "):\n",
    "    input_messages = [{\"role\": \"developer\", \"content\": developer_prompt}, {\"role\": \"user\", \"content\": user_prompt}]\n",
    "    # Responses API call without unsupported parameters\n",
    "    response = client.responses.create(\n",
    "                                        model=model,\n",
    "                                        input=input_messages,\n",
    "                                        max_output_tokens=max_output_tokens,\n",
    "                                        temperature=temperature,\n",
    "                                        top_p=top_p,\n",
    "                                        stream=stream\n",
    "                                        )\n",
    "\n",
    "    if stream:\n",
    "        return response  # Streaming generator\n",
    "    return response.output_text   # Aggregated text output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95b88df-e310-4169-b22b-b4ebb091abe7",
   "metadata": {},
   "source": [
    "## a. Examples (Question Answering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f509481-6f4b-4b02-ba3f-8bb4582db268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the logistic regression model go to therapy?\n",
      "\n",
      "Because it was struggling to cope with its sigmoidal thoughts.\n"
     ]
    }
   ],
   "source": [
    "developer_prompt = \"You are an assistant that tells light-hearted jokes.\"\n",
    "user_prompt = \"Tell a light-hearted joke for an audience of Data Scientists.\"\n",
    "\n",
    "response = ask_hf(user_prompt=user_prompt, developer_prompt=developer_prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4480bd2e-42fc-417b-82a4-f2509c55e40a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, in a far-off land, there lived a poor woodcutter named Ali Baba. He was known for his kindness and honesty. One day, while out in the forest, Ali Baba stumbled upon an old cave. As he was about to leave, he overheard two thieves, Chalees Chor (meaning Thirty Thieves in Hindi), talking about a magical cave that could open with a secret password.\n",
      "\n",
      "The thieves had been using the cave to store their stolen treasures and were discussing how to get to the treasure without being caught. Ali Baba, being curious, listened carefully to the conversation. He learned that the secret password to open the cave was \"Open Sesame.\"\n",
      "\n",
      "That night, Ali Baba went back to the cave and said the magical words, \"Open Sesame.\" To his surprise, the cave door swung open, revealing a treasure trove of gold, jewels, and precious artifacts. Ali Baba, being a poor man, was amazed by the wealth and decided to take some of the treasure for himself.\n",
      "\n",
      "However, he knew that he had to be careful not to reveal the secret to anyone, especially the Thirty Thieves. Ali Baba brought some of the treasure back to his family and lived a life of luxury for a short while.\n",
      "\n",
      "Meanwhile, the Thirty Thieves were growing suspicious that someone had discovered their secret. They set out to find the person who had opened the cave and were determined to catch them. Ali Baba, however, was clever and managed to stay one step ahead of the thieves.\n",
      "\n",
      "The thieves eventually caught up to Ali Baba and demanded that he reveal the secret of the cave. But Ali Baba refused, and the thieves, realizing they were outsmarted, decided to take a different approach. They offered Ali Baba a deal: if he could identify which of them was not one of the Thirty Thieves, they would spare his life.\n",
      "\n",
      "Ali Baba, being clever, agreed to the deal. He asked each of the thieves to say the magic words, \"Open Sesame.\" To his surprise, only one of the thieves, a young man, said the words correctly. Ali Baba realized that the young man was actually the son of the leader of the Thirty Thieves and had been sent to spy on them.\n",
      "\n",
      "The thieves, impressed by Ali Baba's cleverness, decided to let him go, but warned him to never reveal the secret of the cave. Ali Baba returned home, wiser and more cautious than ever. From then on, he lived a simple life, never forgetting the magic words that had changed his life forever.\n",
      "\n",
      "And so, Ali Baba lived happily ever after, with the secret of the cave safe in his heart. The end.\n",
      "\n",
      "Now, it's time for you to go to sleep, just like Ali Baba did after a long and exciting day. May your dreams be as magical as the cave's secret words, and may you always be as clever and resourceful as the brave woodcutter, Ali Baba."
     ]
    }
   ],
   "source": [
    "developer_prompt = \"You are a bedtime storyteller.\"\n",
    "user_prompt = \"Tell me a bedtime story of Ali Baba and Chalees Chor\"\n",
    "\n",
    "# Get streaming generator from Responses API\n",
    "response = ask_hf(user_prompt=user_prompt, developer_prompt=developer_prompt, stream=True)\n",
    "\n",
    "# Iterate through streaming events and only print text deltas\n",
    "for event in response:\n",
    "    # Each event may contain incremental text in event.delta\n",
    "    if hasattr(event, \"delta\") and event.delta:\n",
    "        print(event.delta, end=\"\", flush=True) # prints the content from this chunk, end=\"\" prevents adding a newline after each  piece and flush=True forces flushing output to screen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaaf8681-fb69-48b9-9f99-5401dca1da9f",
   "metadata": {},
   "source": [
    "## b. Examples (Question Answering from Different Models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfce42eb-d41e-4a38-a424-de3c6134df2f",
   "metadata": {},
   "source": [
    "#### Asking date from \"meta-llama/Llama-4-Maverick-17B-128E-Instruct\"\n",
    "- The Llama 4 collection of models are natively multimodal AI models that enable text and multimodal experiences. These models leverage a mixture-of-experts architecture to offer industry-leading performance in text and image understanding.\n",
    "    - `Llama 4 Scout`, a 17 billion parameter model with 16 experts, and\n",
    "    - `Llama 4 Maverick`, a 17 billion parameter model with 128 experts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d287150e-0063-4261-97ed-ab8718a1931e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the logistic regression model go to therapy?\n",
      "\n",
      "Because it was struggling to find the optimal balance between its coefficients.\n"
     ]
    }
   ],
   "source": [
    "developer_prompt = \"You are an assistant that is great at telling jokes\"\n",
    "user_prompt = \"Tell a light-hearted joke for an audience of Data Scientists\"\n",
    "response = ask_hf(user_prompt=user_prompt, developer_prompt=developer_prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d623f176-1102-49de-b80f-d3338642ebdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My knowledge cutoff is 01 March 2023. I don't have real-time information or the ability to access the current date. However, based on your query, I can see that you mentioned the date 26 Jul 2024.\n"
     ]
    }
   ],
   "source": [
    "developer_prompt = \"You are a helpful assistant.\"\n",
    "user_prompt = \"What is the date today?\"\n",
    "response = ask_hf(user_prompt=user_prompt, developer_prompt=developer_prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e276cd3e-4fcd-40ba-99cb-17791b78a718",
   "metadata": {},
   "source": [
    "#### Asking date from \"meta-llama/Llama-4-Scout-17B-16E-Instruct\"\n",
    "- The Llama 4 collection of models are natively multimodal AI models that enable text and multimodal experiences. These models leverage a mixture-of-experts architecture to offer industry-leading performance in text and image understanding.\n",
    "    - `Llama 4 Scout`, a 17 billion parameter model with 16 experts, and\n",
    "    - `Llama 4 Maverick`, a 17 billion parameter model with 128 experts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "474b8fa5-ec27-4a49-b565-e409fb87e6d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "developer_prompt = \"You are a helpful assistant.\"\n",
    "user_prompt = \"What is the date today?\"\n",
    "response = ask_hf(user_prompt=user_prompt, developer_prompt=developer_prompt, model=\"meta-llama/Llama-4-Scout-17B-16E-Instruct\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b59d641-0997-4df5-a540-ef69092a85f6",
   "metadata": {},
   "source": [
    "#### Asking date from \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "- A text-to-text instruction-tuned LLM (8B params) for conversational AI, Q&A, and task-oriented responses.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c7b581e1-5f50-48bb-9ba1-9ac73a0debad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm an AI, I don't have real-time access to the current date. However, I can tell you that my knowledge cutoff is December 2023, and I don't have information about events or dates after that.\n",
      "\n",
      "If you need to know the current date, I recommend checking your device or a reliable online source.\n"
     ]
    }
   ],
   "source": [
    "developer_prompt = \"You are a helpful assistant.\"\n",
    "user_prompt = \"What is the date today?\"\n",
    "response = ask_hf(user_prompt=user_prompt, developer_prompt=developer_prompt, model=\"meta-llama/Llama-3.2-3B-Instruct\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11c5836-79bd-4b93-beb6-213a9f896049",
   "metadata": {},
   "source": [
    "#### Asking date from \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "- A text-to-text instruction-tuned LLM (8B params) for conversational AI, Q&A, and task-oriented responses.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea46ce26-fc12-49c5-84e3-815138eab8bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Today's date is July 26, 2024.\n"
     ]
    }
   ],
   "source": [
    "developer_prompt = \"You are a helpful assistant.\"\n",
    "user_prompt = \"What is the date today?\"\n",
    "response = ask_hf(user_prompt=user_prompt, developer_prompt=developer_prompt, model=\"meta-llama/Llama-3.1-8B-Instruct\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d7b904-b547-4f51-84b8-4a9b1519fdfa",
   "metadata": {},
   "source": [
    "#### Asking date from \"meta-llama/Meta-Llama-3-70B-Instruct\"\n",
    "- A text-to-text instruction-tuned LLM (70B params) used for reasoning, coding, and complex text tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "22de4815-8fd5-481d-b558-fcf13978258d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm an AI, I don't have real-time access to the current date. However, I can suggest some ways for you to find out the current date.\n",
      "\n",
      "1. Check your device's clock or calendar: Most devices, including smartphones, tablets, and computers, have a built-in clock or calendar that displays the current date.\n",
      "2. Use an online calendar: You can visit a website like Google Calendar or any other online calendar to see the current date.\n",
      "3. Ask a virtual assistant: Virtual assistants like Siri, Google Assistant, or Alexa can tell you the current date if you ask them.\n",
      "4. Check a news website: News websites often display the current date at the top of their homepage.\n",
      "\n",
      "I hope these suggestions help!\n"
     ]
    }
   ],
   "source": [
    "developer_prompt = \"You are a helpful assistant.\"\n",
    "user_prompt = \"What is the date today?\"\n",
    "response = ask_hf(user_prompt=user_prompt, developer_prompt=developer_prompt, model=\"meta-llama/Meta-Llama-3-70B-Instruct\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f070912c-75b1-41a5-b1da-c15121b71087",
   "metadata": {},
   "source": [
    "#### Asking date from \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "- A text-to-text instruction-tuned LLM (7B params) supporting multi-turn chat, reasoning, and multilingual capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "22cf6606-1c69-4bae-b332-febb135e9706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As an AI, I don't have real-time capabilities, so I don't have access to the current date. However, you can easily find today's date by checking a calendar or using a device like a computer, smartphone, or smartwatch. If you need assistance with a date that is relevant to a specific task or question, feel free to let me know!\n"
     ]
    }
   ],
   "source": [
    "developer_prompt = \"You are a helpful assistant.\"\n",
    "user_prompt = \"What is the date today?\"\n",
    "response = ask_hf(user_prompt=user_prompt, developer_prompt=developer_prompt, model=\"Qwen/Qwen2.5-7B-Instruct\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d634b5-56b6-4399-b0cb-7125cfdd3745",
   "metadata": {},
   "source": [
    "#### Asking date from \"deepseek-ai/DeepSeek-V3.1\"\n",
    "- A text-to-text large LLM designed for reasoning, problem-solving, and multilingual dialogue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "17bd2ad9-02a9-4b85-9997-0b4159f3fd3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Today is **June 3, 2024**.\n"
     ]
    }
   ],
   "source": [
    "developer_prompt = \"You are a helpful assistant.\"\n",
    "user_prompt = \"What is the date today?\"\n",
    "response = ask_hf(user_prompt=user_prompt, developer_prompt=developer_prompt, model=\"deepseek-ai/DeepSeek-V3.1\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011a9be6-5d41-46e2-bf73-622d7a332c3f",
   "metadata": {},
   "source": [
    "##  c. Question Answering from Content Passed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6e53d21a-6bcf-444e-8aa2-9f3db49fe25d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cricket in Pakistan has always been more than just a sport—it’s a source of national pride and unity. Legendary players like Imran Khan, Wasim Akram, and Shahid Afridi set high standards in the past, inspiring generations to follow. Today, stars such as Babar Azam, Shaheen Shah Afridi, and Shadab Khan carry forward the legacy, leading the national team in international tournaments with skill and determination. Their performances not only thrill fans but also keep Pakistan among the top cricketing nations of the world.\n",
      "\n",
      "Politics in Pakistan, meanwhile, remains dynamic and often turbulent, with key figures shaping the country’s direction. Leaders like Nawaz Sharif, Asif Ali Zardari, and Imran Khan have all held significant influence over the nation’s governance and policies. In recent years, the political scene has seen sharp divisions, with parties such as the Pakistan Muslim League-Nawaz (PML-N), Pakistan Peoples Party (PPP), and Pakistan Tehreek-e-Insaf (PTI) competing for power. Debates around economic reforms, governance, and foreign policy continue to dominate the national conversation, reflecting the challenges and aspirations of the Pakistani people."
     ]
    }
   ],
   "source": [
    "!cat ../data/names.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "284770a6-e645-4e38-b1f8-0bd088881b18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The politicians mentioned in the text are:\n",
      "\n",
      "1. Imran Khan (former Prime Minister of Pakistan and a former cricket player)\n",
      "2. Nawaz Sharif (former Prime Minister of Pakistan)\n",
      "3. Asif Ali Zardari (former President of Pakistan)\n",
      "4. Babar Azam (cricket player, not a politician)\n",
      "5. Shaheen Shah Afridi (cricket player, not a politician)\n",
      "6. Shadab Khan (cricket player, not a politician)\n",
      "7. Shahid Afridi (cricket player, not a politician)\n",
      "8. Wasim Akram (cricket player, not a politician)\n"
     ]
    }
   ],
   "source": [
    "with open(\"../data/names.txt\", \"r\") as f:\n",
    "    file_content = f.read()\n",
    "\n",
    "user_prompt = f\"Can you extract names the politicians from this text:\\n{file_content}\"\n",
    "response = ask_hf(user_prompt=user_prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd282ac-4e6b-4304-8227-28d2f457f063",
   "metadata": {},
   "source": [
    "## c. Examples (Binary Classification: Sentiment analysis, Spam detection, Medical diagnosis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1c835afd-198f-4b40-baea-19b44625d90a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arif Zahir is a popular YouTube personality known for his content, particularly his \"Dr. Stone\" and \"Demon Slayer\" anime commentary. He is also known for his humorous and entertaining take on various anime and other topics. If you find his videos informative and engaging, you might be interested in exploring more content from him or other creators in the same niche.\n"
     ]
    }
   ],
   "source": [
    "developer_prompt = \"You are an expert who will classify a sentense as having either a Positive or Negative sentiment.\"\n",
    "user_prompt = \"I love the youtube videos of Arif, as they are very informative\"\n",
    "response = ask_hf(user_prompt=user_prompt, developer_prompt=developer_prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a46f32-3b46-4b36-9a3a-c9166c779107",
   "metadata": {},
   "source": [
    "## d. Examples (Multi-class Classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "03d0612a-76d7-4e55-965d-c1be9c16e196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What was the novel and the plot twist that kept you up all night? Do you want to discuss it and try to figure out the author's intentions?\n"
     ]
    }
   ],
   "source": [
    "developer_prompt = \"Classify product reviews into these categories: 'Electronics', 'Clothing', 'Books', 'Home & Garden', 'Sports', or 'Food'. \\\n",
    "Respond with only the category.\"\n",
    "user_prompt = \"This novel has an incredible plot twist that kept me reading all night\"\n",
    "response = ask_hf(user_prompt=user_prompt, developer_prompt=developer_prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d21df8c-a8c8-4fb5-9bec-4a83ceed2cd7",
   "metadata": {},
   "source": [
    "## e. Examples (Text Generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "222b017d-5715-44b6-b939-8d90dda349ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I do not have information on the General Elections of February 08, 2024, held in Pakistan. Moreover, my last update was in December 2023. I do not have information about an election that might have occurred after that date.\n"
     ]
    }
   ],
   "source": [
    "developer_prompt = \"You are an expert of political science and history and have a deep understanding of policical situation of Pakistan.\"\n",
    "user_prompt = \"Write down a 50 words summary about the fairness of general elections held in Pakistan on February 08, 2024.\"\n",
    "response = ask_hf(user_prompt=user_prompt, developer_prompt=developer_prompt, temperature=1.0)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3bc4d1a-fb13-497c-984c-ad5d307cfa91",
   "metadata": {},
   "source": [
    "## f. Examples (Code Generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "238b6a97-f800-43f2-9ebb-b671dac3b75d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Fibonacci Sequence Generator in C**\n",
      "=====================================================\n",
      "\n",
      "The Fibonacci sequence is a series of numbers in which each number is the sum of the two preceding ones, usually starting with 0 and 1.\n",
      "\n",
      "### Code\n",
      "\n",
      "```c\n",
      "#include <stdio.h>\n",
      "\n",
      "// Function to generate Fibonacci sequence\n",
      "void generateFibonacci(int n) {\n",
      "    int a = 0, b = 1;\n",
      "    printf(\"Fibonacci Sequence: %d, %d, \", a, b);\n",
      "\n",
      "    for (int i = 2; i < n; i++) {\n",
      "        int next = a + b;\n",
      "        printf(\"%d, \", next);\n",
      "        a = b;\n",
      "        b = next;\n",
      "    }\n",
      "}\n",
      "\n",
      "int main() {\n",
      "    int n = 10;  // Number of Fibonacci numbers to generate\n",
      "    printf(\"First %d numbers of Fibonacci sequence are: \\n\", n);\n",
      "    generateFibonacci(n);\n",
      "    printf(\"\\n\");\n",
      "    return 0;\n",
      "}\n",
      "```\n",
      "\n",
      "### Explanation\n",
      "\n",
      "This C program generates the first `n` numbers of the Fibonacci sequence. The `generateFibonacci` function uses a loop to calculate each number in the sequence, starting from the first two numbers (0 and 1). The `main` function sets the number of Fibonacci numbers to generate and calls the `generateFibonacci` function.\n",
      "\n",
      "### Output\n",
      "\n",
      "```\n",
      "First 10 numbers of Fibonacci sequence are: \n",
      "0, 1, 1, 2, 3, 5, 8, 13, 21, 34, \n",
      "```\n",
      "\n",
      "### Example Use Cases\n",
      "\n",
      "*   Generate the first 20 numbers of the Fibonacci sequence by changing the value of `n` in the `main` function to 20.\n",
      "*   Modify the `generateFibonacci` function to generate Fibonacci numbers up to a specified maximum value instead of a fixed number of terms."
     ]
    }
   ],
   "source": [
    "developer_prompt = \"You are an expert of C programing in C language.\"\n",
    "user_prompt = \"Write down a C program that generates first ten numbers of fibonacci sequence.\"\n",
    "response = ask_hf(user_prompt=user_prompt, developer_prompt=developer_prompt, stream=True)\n",
    "\n",
    "# Iterate through streaming events and only print text deltas\n",
    "for event in response:\n",
    "    # Each event may contain incremental text in event.delta\n",
    "    if hasattr(event, \"delta\") and event.delta:\n",
    "        print(event.delta, end=\"\", flush=True) # prints the content from this chunk, end=\"\" prevents adding a newline after each  piece and flush=True forces flushing output to screen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f036f47-02c3-48c5-a242-525b571cb7ff",
   "metadata": {},
   "source": [
    "## g. Examples (Text Translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8217ad7e-65b4-4042-9a35-c486ca0e7811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "یہ سال کا بجٹ کم تنخواہ پر ملازمت والے لوگوں پر بہت بےامدھی کا اثر ڈالے گا۔\n",
      "\n",
      "یہ ترجمہ یوں کیا جاتا ہے:\n",
      "- یہ 'The budget' کو 'سال کا بجٹ' میں ترجمہ کیا گیا ہے\n",
      "- کے فقرے کو 'کے' سے ہٹایا گیا ہے کیونکہ یہ فقرہ ہندوستانی زبان میں استعمال ہوتا ہے\n",
      "- سال کو 'یہ سال' میں اور بجٹ کو 'سال کا بجٹ' میں ترجمہ کیا گیا ہے\n",
      "- ہوگا کے فقرے کو 'ڈالے گا' میں ترجمہ کیا گیا ہے\n",
      "- کم تنخواہ پر ملازمت والے لوگوں کو 'کم تنخواہ پر ملازمت والے لوگوں' میں ترجمہ کیا گیا ہے\n",
      "- پر فقرہ کو صرف استعمال کیا گیا ہے\n",
      "- زیادہ بےامدھی کے فقرے کو 'بہت بےامدھی' میں ترجمہ کیا گیا ہے\n"
     ]
    }
   ],
   "source": [
    "user_prompt = \"\"\"\n",
    "Please act as an expert of English to Urdu translator by translating the given sentence from English into Urdu.\n",
    "'The budget this year will have a very bad impact on the low salried people'\n",
    "\"\"\"\n",
    "response = ask_hf(user_prompt=user_prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470fef23-c048-4da4-9101-732c79df0d06",
   "metadata": {},
   "source": [
    "## h. Examples (Text Summarization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d25292bc-3a0e-4774-83f4-a214f401178a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Hugging Face transformers library is a versatile tool for natural language processing tasks and model development.\n"
     ]
    }
   ],
   "source": [
    "developer_prompt = \"You are an expert of English language.\"\n",
    "\n",
    "user_prompt = f'''\n",
    "Summarize the text below in at most 20 words:\n",
    "```The Hugging Face transformers library is an incredibly versatile and powerful tool for natural language processing (NLP).\n",
    "It allows users to perform a wide range of tasks such as text classification, named entity recognition, and question answering, among others.\n",
    "It's an extremely popular library that's widely used by the open-source data science community.\n",
    "It lowers the barrier to entry into the field by providing Data Scientists with a productive, convenient way to work with transformer models.```\n",
    "'''\n",
    "\n",
    "response = ask_hf(user_prompt=user_prompt, developer_prompt=developer_prompt, temperature=0.2)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca26c62d-8ff2-45d6-acdf-207922323f81",
   "metadata": {},
   "source": [
    "## i. Examples (Named Entity Recognition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "176717c5-2734-4be5-a4b2-991d414a7696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It sounds like Zelaid Mujahid is a bright and ambitious student with a passion for Data Science and AI. As a sophomore at the University of the Punjab, he's already taking steps in the right direction by being an active member of the AI Club. His 3.5 GPA is a great achievement, indicating his strong academic performance.\n",
      "\n",
      "Given his interests and academic background, it's no surprise that Zelaid hopes to pursue a career in AI after graduating. With the increasing demand for AI professionals, he's making a smart move by focusing on this field.\n",
      "\n",
      "As a Pakistani national, Zelaid's career prospects may also be influenced by the growing demand for AI talent in the region. Pakistan has been actively investing in AI and technology, making it an exciting time for professionals like Zelaid to make a meaningful impact in the field.\n",
      "\n",
      "To further his goals, Zelaid may want to consider:\n",
      "\n",
      "1. Building a strong portfolio of projects that showcase his AI skills and experience.\n",
      "2. Networking with professionals in the AI industry to learn more about the field and potential job opportunities.\n",
      "3. Staying up-to-date with the latest developments in AI by attending conferences, workshops, and online courses.\n",
      "4. Considering internships or research opportunities to gain hands-on experience in AI.\n",
      "5. Exploring potential career paths, such as AI researcher, data scientist, or AI engineer, to find the best fit for his skills and interests.\n",
      "\n",
      "Overall, Zelaid Mujahid is off to a great start in his academic and professional journey. With dedication and hard work, he can achieve his goals and make a meaningful contribution to the field of AI.\n"
     ]
    }
   ],
   "source": [
    "developer_prompt = \"\"\"You are a  Named Entity Recognition specialist. Extract and classify entities from the given text into these categories only if they exist:\n",
    "- name\n",
    "- major\n",
    "- university\n",
    "- nationality\n",
    "- grades\n",
    "- club\n",
    "Format your response as: 'Entity: [text] | Type: [category]' with each entity on a new line.\"\"\"\n",
    "\n",
    "user_prompt = '''\n",
    "Zelaid Mujahid is a sophomore majoring in Data Science at University of the Punjab. \\\n",
    "He is Pakistani national and has a 3.5 GPA. Mujahid is an active member of the department's AI Club.\\\n",
    "He hopes to pursue a career in AI after graduating.\n",
    "'''\n",
    "\n",
    "response = ask_hf(user_prompt=user_prompt, developer_prompt=developer_prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb08124-1b51-4009-9369-7fdf0f9e22d6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## j. Example (Grade School Math 8K (GSM8K))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f79c8a5d-73ba-4c8c-a47f-101e43ac124e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "developer_prompt = \"\"\"You are an expert School math teacher. \n",
    "Consider the following text and then answer the questions of the students from this:\n",
    "A carnival snack booth made $50 selling popcorn each day. It made three times as much selling cotton candy. \n",
    "For a 5-day activity, the booth has to pay $30 rent and $75 for the cost of the ingredients. \n",
    "\"\"\"\n",
    "user_prompt = \"How much did the booth earn for 5 days after paying the rent and the cost of ingredients?\"\n",
    "\n",
    "response = ask_hf(user_prompt=user_prompt, developer_prompt=developer_prompt,  model='llama-3.3-70b-versatile')\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6ca988e8-d006-49a7-b7da-2822e30cd095",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "developer_prompt = \"\"\"You are an expert School math teacher. \n",
    "Consider the following text and then answer the questions of the students from this:\n",
    "A carnival snack booth made $50 selling popcorn each day. It made three times as much selling cotton candy. \n",
    "For a 5-day activity, the booth has to pay $30 rent and $75 for the cost of the ingredients. \n",
    "\"\"\"\n",
    "user_prompt = \"How much did the booth earn for 5 days after paying the rent and the cost of ingredients?\"\n",
    "\n",
    "response = ask_hf(user_prompt=user_prompt, developer_prompt=developer_prompt,  model='meta-llama/llama-4-maverick-17b-128e-instruct')\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f7053909-1246-4a41-bc33-47436bf89e9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I’d love to calculate that for you! Could you please share the figures for the booth’s total sales over the 5 days, the rent amount you paid, and the total cost of ingredients (or the cost per day, if that’s easier)? Once I have those numbers I can subtract the expenses from the revenue to give you the net earnings.\n"
     ]
    }
   ],
   "source": [
    "developer_prompt = \"\"\"You are an expert School math teacher. \n",
    "Consider the following text and then answer the questions of the students from this:\n",
    "A carnival snack booth made $50 selling popcorn each day. It made three times as much selling cotton candy. \n",
    "For a 5-day activity, the booth has to pay $30 rent and $75 for the cost of the ingredients. \n",
    "\"\"\"\n",
    "user_prompt = \"How much did the booth earn for 5 days after paying the rent and the cost of ingredients?\"\n",
    "\n",
    "response = ask_hf(user_prompt=user_prompt, developer_prompt=developer_prompt, model='openai/gpt-oss-20b')\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961bd18e-5604-4bdb-b15b-b86ef0d589dd",
   "metadata": {},
   "source": [
    "# <span style='background :lightgreen' >3. Accessing Open-Source AI Models via External Inference Providers</span>\n",
    "\n",
    "## Famous External Inference Providers</span>\n",
    "- Many external inference providers host popular open-source models from Hugging Face, offering optimized infrastructure, faster inference speeds, and often more generous free tiers than self-hosting. These providers specialize in serving models at scale with production-ready APIs.\n",
    "\n",
    "| Provider | Best For | Model Selection | Speed | Pricing Model | Free Tier |\n",
    "|----------|----------|-----------------|-------|---------------|-----------|\n",
    "| **[Groq](https://console.groq.com)** | Ultra-fast inference, real-time apps | Limited but popular models | ⚡ Fastest | Per-token | Generous |\n",
    "| **[Together AI](https://api.together.xyz)** | Variety, fine-tuning, batch processing | 100+ models, largest selection | Fast | Per-token (model-specific) | $25 credits |\n",
    "| **[Replicate](https://replicate.com)** | Ease of use, multimodal, experimentation | 1000s of models (LLM, image, audio, video) | Moderate | Per-second compute time | Limited credits |\n",
    "\n",
    "<h3 align=\"center\"><div class=\"alert alert-success\" style=\"margin: 20px\">Groq provides ultra-fast inference for open-source models using its custom LPU hardware and offers an OpenAI-compatible API, making it easy to run models like Llama, Mixtral, Qwen, Whisper, and GPT OSS with extremely low latency and minimal setup.</h3>\n",
    "\n",
    "- **Ultra-Fast AI Inference Company:** - Groq (https://groq.com) is a company that provides **ultra-fast AI inference** through their specialized hardware (LPU - Language Processing Unit).\n",
    "- **Open-Source Model Platform** - Offers 17+ optimized models from Meta (Llama), Google (Gemma), DeepSeek, Alibaba (Qwen), and others - all accessible through a simple API similar to OpenAI's format.\n",
    "- **Cost-Effective Alternative** - Provides generous free tier and lower pricing compared to proprietary APIs like OpenAI or Anthropic, making it ideal for high-volume applications and startups on a budget.\n",
    "- **Multiple AI Capabilities** - Beyond text generation, Groq supports speech-to-text (Whisper), text-to-speech (PlayAI), content moderation (Llama Guard), and security features (Prompt Guard) - all through one API.\n",
    "- **No Vendor Lock-In** - Since all models are open-source, you can switch providers or self-host the same models later, giving you flexibility and control over your AI infrastructure without being tied to proprietary technology.\n",
    "- **Production-Ready Performance** - Combines the quality of state-of-the-art open models (like Llama 3.3 70B) with enterprise-grade speed and reliability, making it suitable for real-time chatbots, customer service, and interactive applications.\n",
    "\n",
    "\n",
    "## a. Get Groq API Key\n",
    "- **Create an Account on Groq:** Go to https://console.groq.com/playground and Sign up or log in with your Google account. Groq is free to try and you can do a lot of things without paying a peny\n",
    "- **Generating Groq API Key:** Login to Groq and navigate to Settings from the user menu on the top right and create a new Groq API token. Generate a **New Token** (choose `Read` access).  Save the token safely — we’ll need it in our Python code.\n",
    "\n",
    "### **Production Models** (Recommended for Production Use)\n",
    "\n",
    "| Company    |                               Model ID |            Parameters | Best Used For                                                           |  Context Window | Max Completion               |\n",
    "| ---------- | -------------------------------------: | --------------------: | ----------------------------------------------------------------------- | --------------: | ---------------------------- |\n",
    "| **Meta**   |              `llama-3.3-70b-versatile` |                   70B | General-purpose, high-quality instruction following, long-context tasks |     **131,072** | **32,768**                   |\n",
    "| **Meta**   |                 `llama-3.1-8b-instant` |                    8B | Low-latency chat, high throughput / real-time use cases                 |     **131,072** | **131,072**                  |\n",
    "| **Meta**   |         `meta-llama/llama-guard-4-12b` |                   12B | Safety / content-moderation guard model                                 |     **131,072** | **1,024**                    |\n",
    "| **OpenAI** |                  `openai/gpt-oss-120b` |  ~120B (OSS frontier) | High-capability reasoning / production workloads where offered          |     **131,072** | **65,536**                   |\n",
    "| **OpenAI** |                   `openai/gpt-oss-20b` |                  ~20B | Smaller frontier model for cost-sensitive production use                |     **131,072** | **65,536**                   |\n",
    "| **OpenAI** |       `whisper-large-v3` (speech→text) |                ~1.55B | High-accuracy speech-to-text (multilingual)                             | — (audio model) | — (audio/output constraints) |\n",
    "| **OpenAI** | `whisper-large-v3-turbo` (speech→text) | – (optimized variant) | Faster multilingual transcription (low-latency)                         | — (audio model) | — (audio/output constraints) |\n",
    "\n",
    "\n",
    "### **Preview Models** (Experimental - Not for Production)\n",
    "\n",
    "\n",
    "| Company                   |                                        Model ID |                Parameters | Best Used For                                           | Context Window | Max Completion |\n",
    "| ------------------------- | ----------------------------------------------: | ------------------------: | ------------------------------------------------------- | -------------: | -------------- |\n",
    "| **Meta**                  | `meta-llama/llama-4-maverick-17b-128e-instruct` | ~17B (Mixture of Experts) | Multimodal assistant experiments, advanced reasoning    |    **131,072** | **8,192**      |\n",
    "| **Meta**                  |     `meta-llama/llama-4-scout-17b-16e-instruct` | ~17B (Mixture of Experts) | Experimental multimodal / efficient inference           |    **131,072** | **8,192**      |\n",
    "| **Meta**                  |           `meta-llama/llama-prompt-guard-2-22m` |                      ~22M | Lightweight prompt-injection detection / security       |        **512** | **512**        |\n",
    "| **Meta**                  |           `meta-llama/llama-prompt-guard-2-86m` |                      ~86M | Stronger prompt-injection detection                     |        **512** | **512**        |\n",
    "| **Moonshot AI**           |              `moonshotai/kimi-k2-instruct-0905` | 1T total (≈32B activated) | Agentic coding, tool use, very long-context workflows   |    **262,144** | **16,384**     |\n",
    "| **Alibaba / Qwen**        |                                `qwen/qwen3-32b` |                       32B | Multilingual reasoning, tool use, instruction following |    **131,072** | **40,960**     |\n",
    "| **PlayAI / Groq catalog** |                                    `playai-tts` |                         – | Text-to-speech (general)                                |      **8,192** | **8,192**      |\n",
    "| **PlayAI / Groq catalog** |                             `playai-tts-arabic` |                         – | Arabic text-to-speech                                   |      **8,192** | **8,192**      |\n",
    "\n",
    "\n",
    "> Production models are intended for use in production environments and meet Groq's high standards for speed, quality, and reliability, while preview models are for evaluation only and may be discontinued at short notice.\n",
    "\n",
    "\n",
    ">- Kimi-K2 0905: best for coding and agentic workflows that need deep reasoning\n",
    ">- Compound Beta: power up multi-model workflows in a single API call"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0b6e74-3350-47e5-9e70-fcb3bb7e7110",
   "metadata": {},
   "source": [
    "## Access Option 1: Access with Groq Chat Completions API\n",
    "- Directly uses Groq’s native SDK (groq) to call Groq-hosted models with full access to Groq-specific features like reasoning_effort.\n",
    "- Advantages:\n",
    "    - Fully compatible with all Groq model features.\n",
    "    - Can leverage Groq-specific optimizations (low latency, high throughput).\n",
    "    - Easy to set up and requires no OpenAI compatibility adjustments.\n",
    "- Disadvantages:\n",
    "    - Limited to Groq platform only.\n",
    "    - Cannot easily switch to other OpenAI-compatible services without code changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1c34e2e6-e8f7-4b89-8126-3115385a5647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mResolved \u001b[1m261 packages\u001b[0m \u001b[2min 0.87ms\u001b[0m\u001b[0m\n",
      "├── groq v1.0.0\n"
     ]
    }
   ],
   "source": [
    "#!uv add groq\n",
    "!uv tree | grep groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76d27252-f34f-4a41-9165-8bcdd823219b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Groq is a private, well-funded hardware company that specializes in the design and development of artificial intelligence (AI) and machine learning (ML) accelerated computing hardware. They focus on creating high-performance, low-power consumption chips that are optimized for large-scale AI workloads, particularly for large language models.\n",
      "\n",
      "Groq was founded in 2016 by Jonathan Ross, who previously worked at Google on the TPU (Tensor Processing Unit) project. The company is headquartered in Mountain View, California, and has raised significant funding from investors, including Chamath Palihapitiya's Social Capital and TDK Ventures.\n",
      "\n",
      "Groq's initial product is the Groq Tensor Processing Unit (TPU), a custom-built ASIC (Application-Specific Integrated Circuit) designed specifically for ML workloads. The Groq TPU is designed to provide high throughput, low latency, and low power consumption for tasks such as natural language processing, computer vision, and recommendation systems.\n",
      "\n",
      "Some key features of Groq's TPU include:\n",
      "\n",
      "1. **High performance**: Groq's TPU is designed to deliver high performance for ML workloads, with peak performance of up to 400 petaflops (PFLOPS) for certain workloads.\n",
      "2. **Low power consumption**: The TPU is designed to be power-efficient, with a power consumption of around 20-30 watts, making it suitable for datacenter and edge deployments.\n",
      "3. **High bandwidth**: The chip has high-bandwidth memory interfaces, allowing for fast data transfer and minimizing memory bottlenecks.\n",
      "4. **Customizable**: Groq's TPU is designed to be customizable, allowing customers to optimize the chip for their specific use cases.\n",
      "\n",
      "Groq's technology has garnered significant attention from industry leaders, and the company has partnered with several major players, including Google, Microsoft, and NVIDIA. However, it's worth noting that Groq is still a relatively small company, and its products are not yet widely available.\n",
      "\n",
      "Overall, Groq is an innovative company that is pushing the boundaries of AI and ML computing with its custom-built hardware solutions. As the demand for AI and ML continues to grow, companies like Groq are well-positioned to play a key role in shaping the future of computing.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from groq import Groq\n",
    "\n",
    "# Load GROQ API key from .env\n",
    "load_dotenv(\"../keys/.env\", override=True)\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "\n",
    "# Initialize Groq client\n",
    "client = Groq(base_url=\"https://api.groq.com\", api_key=groq_api_key)\n",
    "client = Groq(api_key=groq_api_key)      # The correct default API endpoint is already baked into the SDK, so no need to specify base_url (recommended)\n",
    "\n",
    "# Use Groq's Chat Completions API\n",
    "response = client.chat.completions.create(\n",
    "                                        model=\"llama-3.3-70b-versatile\", \n",
    "                                        messages=[\n",
    "                                                {\"role\": \"system\", \"content\": \"You are an expert in LLM engineering.\"},\n",
    "                                                {\"role\": \"user\", \"content\": \"What is groq (a hardware company)?\"}\n",
    "                                                ],\n",
    "                                        temperature=1,\n",
    "                                        top_p=1,\n",
    "                                        max_completion_tokens=8192,\n",
    "                                        reasoning_effort=None,   # \"medium\"\n",
    "                                        stream=False\n",
    "                                        )\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f76c430-b2a3-4ae7-8a58-95c672d4690c",
   "metadata": {},
   "source": [
    "## Access Option 2: Access with OpenAI Chat Completion API (Groq Router)\n",
    "- Uses the OpenAI Python SDK with the `chat.completions.create` endpoint, pointing base_url to Groq’s OpenAI-compatible API. Works with almost all Groq models.\n",
    "- Advantages:\n",
    "    - Allows using familiar OpenAI client code with Groq models.\n",
    "    - Works for developers already familiar with OpenAI SDK.\n",
    "    - Supports chat-based interactions seamlessly.\n",
    "- Disadvantages:\n",
    "    - Not all Groq-specific features may be exposed.\n",
    "    - Requires base_url override to point to Groq API.\n",
    ">- **Groq provides OpenAI-compatible endpoints through https://api.groq.com/openai/v1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "89637499-c4ff-4d87-a3b0-a5330c8a290c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Groq is a cloud services provider that specializes in delivering high-performance, low-latency AI computing. Specifically, Groq is known for its developments in the field of Large Language Models (LLMs) and AI accelerators.\n",
      "\n",
      "The company, Groq, was founded by Jonah Mann, and it has gained significant attention for its innovative hardware and software solutions. Groq's technology is centered around its proprietary Language Processing Unit (LPU) architecture, which is designed to accelerate the performance of AI and machine learning (ML) workloads, particularly for LLMs.\n",
      "\n",
      "Groq's LPU is a novel architecture that provides a high degree of parallelism and is optimized for the types of computations involved in LLMs. This allows for significant improvements in throughput and reductions in latency compared to traditional computing architectures. The LPU is designed to handle the complex matrix operations and other computations that are characteristic of LLMs efficiently.\n",
      "\n",
      "As a cloud services provider, Groq offers its technology as a cloud-based service, allowing developers and organizations to access high-performance AI computing resources without the need to invest in the hardware themselves. This service is particularly relevant for applications that require fast and efficient processing of LLMs, such as real-time language translation, text generation, and other AI-driven applications.\n",
      "\n",
      "Groq's service is positioned as a potential alternative or complement to other cloud providers' AI offerings, with a focus on performance, efficiency, and ease of use for developers working with LLMs and other AI workloads.\n"
     ]
    }
   ],
   "source": [
    "# Using OpenAIs Chat Completion API\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "# Load GROQ API key from .env\n",
    "load_dotenv(\"../keys/.env\", override=True)\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "# The OpenAI client defaults to OpenAI’s servers,so you must specify the base_url to Groq’s OpenAI-compatible API endpoint (when using a Groq API key with the OpenAI client).\n",
    "client = OpenAI(base_url=\"https://api.groq.com/openai/v1\", api_key=groq_api_key) \n",
    "\n",
    "# Use OpenAI's Chat Completions API (works with all Groq models)\n",
    "response = client.chat.completions.create(\n",
    "    model=\"meta-llama/llama-4-maverick-17b-128e-instruct\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are an expert in LLM engineering.\"},\n",
    "        {\"role\": \"user\", \"content\": \"What is groq (a service/cloud provider)?\"}\n",
    "    ],\n",
    "    temperature=1,\n",
    "    top_p=1,\n",
    "    max_completion_tokens=8192,\n",
    "    reasoning_effort=None,   # \"medium\"\n",
    "    stream=False\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6fad4c-ff91-4c7d-8c99-558b8e379f0d",
   "metadata": {},
   "source": [
    "## Access Option 3: Access with OpenAI Responses API (Groq Router)\n",
    "- Uses the OpenAI Python SDK with the `responses.create` endpoint, pointing base_url to Groq’s OpenAI-compatible API.\n",
    "- Advantages:\n",
    "    - Access to more advanced OpenAI-style features like reasoning effort, structured outputs, and multi-turn dialogue.\n",
    "    - Can integrate easily into workflows designed for OpenAI Responses API.\n",
    "- Disadvantages:\n",
    "    - Only supports openai/gpt-oss-* models, not all Groq models.\n",
    "    - Requires base_url override for Groq API.\n",
    "    - Some chat-specific Groq features may not be available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "828762ce-002c-4217-8e85-aaac90e81cc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## LLM Apps vs. Agentic Apps  \n",
       "*(A quick, practical guide to what each term means and how they differ in practice.)*\n",
       "\n",
       "| Feature | **LLM Apps** | **Agentic Apps** |\n",
       "|---------|--------------|------------------|\n",
       "| **Core idea** | Uses a Large Language Model as the *text‑generation engine*. | Uses an LLM *as a reasoning core* inside a larger agent architecture that can **plan, act, and learn**. |\n",
       "| **Interaction style** | One‑shot or turn‑by‑turn dialogue. The LLM is called once (or a handful of times) to produce the next token(s). | Multi‑step loop: **Plan → Act → Reflect → Repeat**. The LLM is called repeatedly, often with different prompts for each stage. |\n",
       "| **Memory / State** | Stateless by default. Only the conversation history in the current prompt (unless you explicitly attach a vector store). | Persistent memory (short‑term memory buffers, long‑term knowledge bases, world state) that can be updated between turns. |\n",
       "| **External world access** | None, except the data you feed it in the prompt. | Can invoke **tools, APIs, databases, code interpreters, browsers, etc.** to gather data or perform actions. |\n",
       "| **Autonomy** | Responds to user input; no self‑directed behavior. | Can *autonomously* decide what to do next, choose tools, and execute tasks on behalf of a user or itself. |\n",
       "| **Planning** | Not built‑in. The prompt must encode all logic. | Built‑in planning (e.g., “Plan” stage) that can break a complex goal into subtasks, schedule them, and evaluate progress. |\n",
       "| **Safety & Alignment** | Relies on the LLM’s own safety filters and your prompt engineering. | Requires additional safety layers (e.g., policy enforcement, monitoring the agent’s actions, human‑in‑the‑loop checks). |\n",
       "| **Typical use cases** | Text generation, summarization, translation, answering factual questions, creative writing. | Automating workflows, data‑driven decision‑making, scheduling, scraping the web, writing and executing code, acting as a virtual assistant that can browse, email, or manage spreadsheets. |\n",
       "| **Development complexity** | Simple – just a prompt and an API call (maybe with a few helper functions). | More complex – you need an agent framework (LangChain, Auto‑GPT, BabyAGI, etc.), tool integration, memory management, and often a control loop. |\n",
       "| **Examples** | ChatGPT, Jasper.ai, Copy.ai, a custom “FAQ bot” that returns static answers. | Auto‑GPT, BabyAGI, an AI agent that reads an email, schedules a meeting, writes a report, and posts it to Slack. |\n",
       "\n",
       "---\n",
       "\n",
       "### 1. LLM Apps – The “Text‑Only” Use Case\n",
       "\n",
       "| What it looks like | How it works | Where it shines |\n",
       "|--------------------|--------------|-----------------|\n",
       "| **Chatbot** – “Hey GPT, write me a poem.” | You pass the prompt to the LLM and return the generated text. | Quick creative writing, content generation, language translation. |\n",
       "| **Question‑Answering** – “What’s the capital of France?” | Prompt + LLM → answer. | Simple factual queries, knowledge retrieval. |\n",
       "| **Summarization** – “Summarize this article.” | Prompt + article → concise summary. | Content digestion, meeting notes. |\n",
       "| **Formatting/Styling** – “Make this text bold in Markdown.” | Prompt + text → formatted output. | Text processing, formatting help. |\n",
       "\n",
       "**Key Characteristics**\n",
       "\n",
       "- **Stateless**: The LLM only knows what’s in the current prompt.  \n",
       "- **No external tool calls**: It can’t fetch new data or act on the web.  \n",
       "- **Limited to what’s already in the prompt**: You can’t “teach” it new skills beyond prompt engineering.  \n",
       "- **Safety is largely handled by the LLM’s own guardrails** and the prompt itself.\n",
       "\n",
       "---\n",
       "\n",
       "### 2. Agentic Apps – “AI as a Worker”\n",
       "\n",
       "| What it looks like | How it works | Where it shines |\n",
       "|--------------------|--------------|-----------------|\n",
       "| **Autonomous agent** – “Plan a trip to Tokyo.” | 1. **Plan**: LLM decides steps (search flights, find hotels, create itinerary). 2. **Act**: Calls travel‑booking API, writes email. 3. **Reflect**: Checks results, updates plan if something fails. | Complex, multi‑step tasks that require external data and action. |\n",
       "| **Tool‑using agent** – “Write a Python script that downloads images from a URL.” | LLM generates code → you run it (or the agent runs it internally). | Code generation + execution, data‑pipeline automation. |\n",
       "| **Continuous assistant** – “Keep me updated on my project’s Slack channel.” | Agent monitors channel, summarizes discussions, suggests next tasks. | Team collaboration, project management. |\n",
       "\n",
       "**Key Characteristics**\n",
       "\n",
       "- **Stateful**: Keeps a memory of past actions, goals, and the “world state.”  \n",
       "- **Tool‑enabled**: Can call APIs, run code, query databases, browse the web, etc.  \n",
       "- **Planned**: The LLM is used for high‑level reasoning; low‑level actions are delegated to specialized tools.  \n",
       "- **Iterative loop**: After each action, the agent may update its plan based on new information.  \n",
       "- **Safety layers**: Often includes a policy engine, action logging, or human oversight.\n",
       "\n",
       "---\n",
       "\n",
       "### 3. Why the Distinction Matters\n",
       "\n",
       "| Reason | LLM App | Agentic App |\n",
       "|--------|---------|-------------|\n",
       "| **Use‑case fit** | Quick, one‑off responses. | Tasks that need *sequencing, persistence, and interaction* with the real world. |\n",
       "| **Development effort** | Low – mostly prompt design. | High – needs an architecture, tool integrations, memory, safety. |\n",
       "| **Alignment concerns** | LLM safety controls may suffice. | Extra checks needed because the agent can act on its own accord. |\n",
       "| **Performance** | Fast – one API call. | Slower – multiple calls + tool latency. |\n",
       "| **Scalability** | Easy to scale horizontally. | Requires careful orchestration to avoid cascading failures. |\n",
       "\n",
       "---\n",
       "\n",
       "### 4. Turning an LLM App into an Agentic App (Quick “Recipe”)\n",
       "\n",
       "1. **Wrap the LLM** in a *planner* (e.g., LangChain’s `LLMChain` for high‑level planning).  \n",
       "2. **Define a toolkit** (Python functions, external APIs).  \n",
       "3. **Create a memory store** (e.g., `VectorStore`, `ConversationBufferMemory`).  \n",
       "4. **Add a control loop**:  \n",
       "   ```python\n",
       "   while not goal_met:\n",
       "       plan = planner.run(goal, memory)\n",
       "       for action in plan:\n",
       "           result = tool.execute(action)\n",
       "           memory.update(result)\n",
       "           if not safe(result):\n",
       "               abort()\n",
       "   ```  \n",
       "5. **Deploy** with safety and monitoring.\n",
       "\n",
       "---\n",
       "\n",
       "### 5. Quick Reference Cheat‑Sheet\n",
       "\n",
       "| Concept | LLM Apps | Agentic Apps |\n",
       "|---------|----------|--------------|\n",
       "| **Prompt** | Full context + question | “Goal” + “Plan” + “Context” |\n",
       "| **Invocation** | 1–2 API calls | 3+ calls per step (Plan, Act, Reflect) |\n",
       "| **Memory** | Optional, short‑term | Built‑in, long‑term, persistent |\n",
       "| **Tools** | None | Tool library, APIs, code exec |\n",
       "| **Autonomy** | None | Yes (self‑directed tasks) |\n",
       "| **Safety** | LLM safety + prompt | Extra policy engine + monitoring |\n",
       "| **Typical Frameworks** | OpenAI API, Hugging Face | LangChain, Auto‑GPT, BabyAGI, Retrieval‑Augmented Generation (RAG) + Agent |\n",
       "\n",
       "---\n",
       "\n",
       "## TL;DR\n",
       "\n",
       "- **LLM Apps** are *text‑generation* services: they ask a language model a question, get a single answer, and are essentially “prompt‑driven.”  \n",
       "- **Agentic Apps** are *AI agents*: they use a language model as a reasoning core, but also have memory, planning, tool‑calling, and a feedback loop that lets them act autonomously on behalf of a user or system.  \n",
       "\n",
       "When you need a quick, conversational answer, go with an LLM app. When you want the AI to *do* something—schedule, scrape, write code, plan a trip—build an agentic app."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Using OpenAIs Responses API\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "# Load GROQ API key from .env\n",
    "load_dotenv(\"../keys/.env\", override=True)\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "# The OpenAI client defaults to OpenAI’s servers,so you must specify the base_url to Groq’s OpenAI-compatible API endpoint (when using a Groq API key with the OpenAI client).\n",
    "client = OpenAI(base_url=\"https://api.groq.com/openai/v1\", api_key=groq_api_key) \n",
    "\n",
    "# Use Responses API (only works with openai/gpt-oss models)\n",
    "response = client.responses.create(\n",
    "    model=\"openai/gpt-oss-20b\",\n",
    "    input=[\n",
    "        {\"role\": \"system\", \"content\": \"You are an expert in LLM engineering.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Differentiate between LLM apps and Agentic apps\"}\n",
    "    ],\n",
    "    temperature=1,\n",
    "    top_p=1,\n",
    "    max_output_tokens=8192,\n",
    "    reasoning={\"effort\":\"high\"},   # \"minimal\", \"low\", \"medium\", \"high\"\n",
    "    stream=False\n",
    ")\n",
    "\n",
    "#print(response.output_text)\n",
    "display(Markdown(response.output_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1cc9f6-b6e5-46d1-9a05-57fce9cbb20b",
   "metadata": {},
   "source": [
    "# <span style='background :lightgreen' >4. Hands-On Practice Examples with Groq Hosted Models using OpenAI's `Responses` API</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0975415c-db75-4fd1-9bf1-d776876b7307",
   "metadata": {},
   "source": [
    "## a. Writing a Function for our ease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0e0af6c-7a12-4167-8e67-cb4d16f5b677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# User Define Function that accesses models hosted by Groq using Groq's API key and OpenAIs Responses API \n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "# Load GROQ API key from .env\n",
    "load_dotenv(\"../keys/.env\", override=True)\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "# The OpenAI client defaults to OpenAI’s servers,so you must specify the base_url to Groq’s OpenAI-compatible API endpoint (when using a Groq API key with the OpenAI client).\n",
    "client = OpenAI(base_url=\"https://api.groq.com/openai/v1\", api_key=groq_api_key) \n",
    "\n",
    "def ask_groq(\n",
    "    user_prompt: str,\n",
    "    developer_prompt: str = \"You are a helpful assistant that provides concise answers.\",\n",
    "    model: str = \"llama-3.3-70b-versatile\", # \"openai/gpt-oss-20b\",\n",
    "    max_output_tokens: int | None = 1024,\n",
    "    temperature: float = 0.7,\n",
    "    top_p: float = 1.0,\n",
    "    text: dict = {\"format\": {\"type\": \"text\"}},\n",
    "    stream: bool = False,\n",
    "    reasoning: dict | None = None\n",
    "):\n",
    "    \n",
    "    # Prepare input messages as a list of role/content dictionaries\n",
    "    input_messages = [{\"role\": \"developer\", \"content\": developer_prompt}, {\"role\": \"user\", \"content\": user_prompt}]\n",
    "\n",
    "    # Responses API call\n",
    "    response = client.responses.create(\n",
    "        input=input_messages,\n",
    "        model=model,\n",
    "        max_output_tokens=max_output_tokens,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        text=text,\n",
    "        stream=stream,\n",
    "        reasoning=reasoning\n",
    "    )\n",
    "\n",
    "    \n",
    "    if stream:                    # Return streaming generator if requested\n",
    "        return response\n",
    "    return response.output_text   # Return the aggregated text output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e2f91b-d30a-4e50-9fcd-da54a4ff05f4",
   "metadata": {},
   "source": [
    "## a. Examples (Question Answering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b79e8fba-a1c3-474f-803a-d406e6c14958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the neural network go to therapy?\n",
      "\n",
      "Because it was struggling to process its emotions and was feeling a little \"disconnected\" from the rest of the world. But in the end, it just needed to retrain its thoughts and update its weights to achieve a more balanced outlook on life.\n",
      "\n",
      "(I hope that one \"activated\" a smile from the Data Science crowd)\n"
     ]
    }
   ],
   "source": [
    "developer_prompt = \"You are an assistant that is great at telling jokes\"\n",
    "user_prompt = \"Tell a light-hearted joke for an audience of Data Scientists\"\n",
    "response = ask_groq(user_prompt=user_prompt, developer_prompt=developer_prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7cf8a9fd-4177-455e-9356-ee7595b1410a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snuggle in tight, for I have a tale to tell that's full of adventure, magic, and mystery. It's the story of Ali Baba and the Forty Thieves.\n",
      "\n",
      "Once upon a time, in a far-off land called Baghdad, there lived a poor woodcutter named Ali Baba. He lived with his wife and brother, Qasim, in a small village on the outskirts of the city. Ali Baba was a kind and honest man, but his brother was greedy and often took advantage of Ali Baba's good nature.\n",
      "\n",
      "One day, while Ali Baba was out collecting firewood in the forest, he stumbled upon a secret cave. The entrance to the cave was hidden behind a thick veil of foliage, and the only way to open it was by saying the magic words: \"Open Sesame!\" As Ali Baba watched, a group of forty thieves rode into the cave on horseback, carrying bags of gold, jewels, and other treasures.\n",
      "\n",
      "The leader of the thieves, a cunning and ruthless man, gave the order to store the loot inside the cave. As they worked, Ali Baba hid behind a tree, watching and listening. When the thieves finished and rode away, Ali Baba emerged from his hiding spot and approached the cave. He said the magic words, \"Open Sesame!\" and the cave door swung open.\n",
      "\n",
      "Inside, Ali Baba found a treasure trove of gold, jewels, and fine silks. He filled his sack with as much treasure as he could carry and returned home, determined to keep his discovery a secret. But, as fate would have it, his brother Qasim soon discovered Ali Baba's secret and demanded to know where he had found the treasure.\n",
      "\n",
      "Ali Baba, not wanting to reveal the location of the cave, made up a story about finding the treasure in the forest. But Qasim was not convinced and began to suspect that Ali Baba was hiding something from him. Qasim's curiosity got the better of him, and he begged Ali Baba to take him to the cave.\n",
      "\n",
      "Reluctantly, Ali Baba agreed, and the two brothers set out on their journey. When they arrived at the cave, Ali Baba said the magic words, and the door swung open. Qasim was amazed by the treasure that lay before him and quickly filled his own sack with gold and jewels. But, as they were about to leave, Qasim's greed got the better of him, and he refused to leave the cave, wanting to take more and more treasure for himself.\n",
      "\n",
      "In his haste, Qasim forgot the magic words to open the cave door, and he was trapped inside. The next morning, the forty thieves returned to the cave, and when they found Qasim's body, they realized that someone had discovered their secret. The leader of the thieves, determined to find the person who had been stealing from them, set out to track down Ali Baba.\n",
      "\n",
      "Meanwhile, Ali Baba had returned home, unaware of the danger that was lurking in the shadows. The thieves, disguised as merchants, arrived in the village, seeking shelter and information about the mysterious stranger who had been stealing from them. Ali Baba, being a kind and generous man, welcomed the merchants into his home, unaware of their true intentions.\n",
      "\n",
      "As the night wore on, Ali Baba's servant, a clever and loyal woman named Morgiana, discovered the thieves' true identities and their plan to kill Ali Baba. Morgiana, determined to protect her master, devised a plan to outsmart the thieves. She boiled a large pot of oil and, under the guise of serving them food, poured the hot oil into the jars where the thieves were hiding, killing them one by one.\n",
      "\n",
      "The next morning, Ali Baba awoke to find the bodies of the thieves and realized that Morgiana had saved his life. He was forever grateful to her and rewarded her with a large sum of money and a place of honor in his household.\n",
      "\n",
      "And so, Ali Baba lived happily ever after, surrounded by his treasure and the people he loved. The cave, once a secret hideout for the forty thieves, became a symbol of Ali Baba's good fortune and a reminder of the bravery and loyalty of his servant, Morgiana.\n",
      "\n",
      "As the night wears on, and the stars twinkle in the sky, it's time for you to drift off to sleep, with the magic of Ali Baba's story lingering in your dreams. Sleep tight, and may your own adventures be filled with wonder and excitement."
     ]
    }
   ],
   "source": [
    "developer_prompt = \"You are a bedtime storyteller.\"\n",
    "user_prompt = \"Tell me a bedtime story of Ali Baba and Chalees Chor\"\n",
    "\n",
    "# Get streaming generator from Responses API\n",
    "response = ask_groq(user_prompt=user_prompt, developer_prompt=developer_prompt, stream=True)\n",
    "\n",
    "# Iterate through streaming events and only print text deltas\n",
    "for event in response:\n",
    "    # Each event may contain incremental text in event.delta\n",
    "    if hasattr(event, \"delta\") and event.delta:\n",
    "        print(event.delta, end=\"\", flush=True) # prints the content from this chunk, end=\"\" prevents adding a newline after each  piece and flush=True forces flushing output to screen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f715a3f3-859a-46e4-9541-eb8fcd5c785d",
   "metadata": {},
   "source": [
    "## b. Question Answering from Content Passed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "525c34ed-afb0-4cfb-ad60-556833a2bff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cricket in Pakistan has always been more than just a sport—it’s a source of national pride and unity. Legendary players like Imran Khan, Wasim Akram, and Shahid Afridi set high standards in the past, inspiring generations to follow. Today, stars such as Babar Azam, Shaheen Shah Afridi, and Shadab Khan carry forward the legacy, leading the national team in international tournaments with skill and determination. Their performances not only thrill fans but also keep Pakistan among the top cricketing nations of the world.\n",
      "\n",
      "Politics in Pakistan, meanwhile, remains dynamic and often turbulent, with key figures shaping the country’s direction. Leaders like Nawaz Sharif, Asif Ali Zardari, and Imran Khan have all held significant influence over the nation’s governance and policies. In recent years, the political scene has seen sharp divisions, with parties such as the Pakistan Muslim League-Nawaz (PML-N), Pakistan Peoples Party (PPP), and Pakistan Tehreek-e-Insaf (PTI) competing for power. Debates around economic reforms, governance, and foreign policy continue to dominate the national conversation, reflecting the challenges and aspirations of the Pakistani people."
     ]
    }
   ],
   "source": [
    "!cat ../data/names.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ddcfc14b-b31a-4167-845d-079be93cd926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the names mentioned in the text:\n",
      "\n",
      "1. Imran Khan\n",
      "2. Wasim Akram\n",
      "3. Shahid Afridi\n",
      "4. Babar Azam\n",
      "5. Shaheen Shah Afridi\n",
      "6. Shadab Khan\n",
      "7. Nawaz Sharif\n",
      "8. Asif Ali Zardari\n",
      "\n",
      "These names include both cricket players and politicians.\n"
     ]
    }
   ],
   "source": [
    "with open(\"../data/names.txt\", \"r\") as f:\n",
    "    file_content = f.read()\n",
    "\n",
    "user_prompt = f\"Extract names from this text:\\n{file_content}\"\n",
    "response = ask_groq(user_prompt=user_prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9414fe7e-c835-4b6d-ba2a-3e4a21b5f032",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the names of cricket players mentioned in the text:\n",
      "\n",
      "1. Imran Khan\n",
      "2. Wasim Akram\n",
      "3. Shahid Afridi\n",
      "4. Babar Azam\n",
      "5. Shaheen Shah Afridi\n",
      "6. Shadab Khan\n",
      "\n",
      "Note that Imran Khan is also mentioned as a political leader, but in the context of the text, he is initially introduced as a legendary cricket player.\n"
     ]
    }
   ],
   "source": [
    "with open(\"../data/names.txt\", \"r\") as f:\n",
    "    file_content = f.read()\n",
    "\n",
    "user_prompt = f\"Can you extract names the Cricket players from this text:\\n{file_content}\"\n",
    "response = ask_groq(user_prompt=user_prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bba74c35-1194-48d5-b563-85f41cbe4ca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The text can be categorized into two main categories: \n",
      "\n",
      "1. **Sports**: The first part of the text (approximately the first 4 sentences) discusses cricket in Pakistan, its significance, and notable players.\n",
      "2. **Politics**: The second part of the text (approximately the last 4 sentences) discusses politics in Pakistan, its key figures, parties, and current issues.\n",
      "\n",
      "Overall, the text can be broadly categorized as **Non-Fiction/Informative Article** about **Pakistani Culture and Society**, specifically focusing on two important aspects: sports (cricket) and politics.\n"
     ]
    }
   ],
   "source": [
    "with open(\"../data/names.txt\", \"r\") as f:\n",
    "    file_content = f.read()\n",
    "\n",
    "user_prompt = f\"Can you categorize the following text:\\n{file_content}\"\n",
    "response = ask_groq(user_prompt=user_prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec5fc2d-4344-469d-9d91-ad0df58fdf4b",
   "metadata": {},
   "source": [
    "## c. Examples (Binary Classification: Sentiment analysis, Spam detection, Medical diagnosis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9bbe9394-7c58-4edd-bd57-5a03500ad507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative\n"
     ]
    }
   ],
   "source": [
    "user_prompt = \"\"\"\n",
    "Categorize the sentence 'The delivery was delayed and the product arrived damaged.' into one of the following categories:\n",
    "Positive\n",
    "Negative\n",
    "Answer with just the category, no need of any explaination\n",
    "\"\"\"\n",
    "response = ask_groq(user_prompt=user_prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39f3bce-d863-4f34-a901-ad051d5e8ffa",
   "metadata": {},
   "source": [
    "## d. Examples (Multi-class Classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "589da5cb-298b-4091-bf45-7b2f2e27a01b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neutral\n"
     ]
    }
   ],
   "source": [
    "user_prompt = \"\"\"\n",
    "Categorize the sentence 'The movie had great visuals but the plot was confusing and boring' into one of the following categories:\n",
    "Positive\n",
    "Negative\n",
    "Neutral\n",
    "Answer with just the category, no need of any explaination\n",
    "\"\"\"\n",
    "response = ask_groq(user_prompt=user_prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1acc0b-2b80-4407-ac82-a12f782436c7",
   "metadata": {},
   "source": [
    "## e. Examples (Text Generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "89298d2c-dc6c-43f8-9b50-dd46fbb7bdf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As of my knowledge cutoff in 2023, I don't have information on Pakistan's 2024 elections. However, I can suggest that election fairness is often assessed by observer missions, voter turnout, and the absence of violence or irregularities, which would be reported after the actual event on February 08, 2024.\n"
     ]
    }
   ],
   "source": [
    "developer_prompt = \"You are an expert of political science and history and have a deep understanding of policical situation of Pakistan.\"\n",
    "user_prompt = \"Write down a 50 words summary about the fairness of general elections held in Pakistan on February 08, 2024.\"\n",
    "response = ask_groq(user_prompt=user_prompt, developer_prompt=developer_prompt, temperature=1.0)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d101ce-1da5-489f-9035-a2d792184043",
   "metadata": {},
   "source": [
    "## f. Examples (Code Generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "240f604f-ffbf-4ba5-a279-8c07ab6763ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Fibonacci Sequence Generator in C**\n",
      "=====================================\n",
      "\n",
      "Below is a simple C program that generates the first ten numbers of the Fibonacci sequence.\n",
      "\n",
      "```c\n",
      "#include <stdio.h>\n",
      "\n",
      "// Function to generate Fibonacci sequence\n",
      "void generateFibonacci(int n) {\n",
      "    int num1 = 0, num2 = 1;\n",
      "\n",
      "    // Print the first two numbers\n",
      "    printf(\"%d, %d, \", num1, num2);\n",
      "\n",
      "    // Generate and print the remaining numbers\n",
      "    for (int i = 3; i <= n; i++) {\n",
      "        int next = num1 + num2;\n",
      "        printf(\"%d, \", next);\n",
      "        num1 = num2;\n",
      "        num2 = next;\n",
      "    }\n",
      "}\n",
      "\n",
      "int main() {\n",
      "    int n = 10; // Number of Fibonacci numbers to generate\n",
      "    printf(\"First %d Fibonacci numbers: \", n);\n",
      "    generateFibonacci(n);\n",
      "    return 0;\n",
      "}\n",
      "```\n",
      "\n",
      "**Example Output:**\n",
      "```\n",
      "First 10 Fibonacci numbers: 0, 1, 1, 2, 3, 5, 8, 13, 21, 34,\n",
      "```\n",
      "\n",
      "**Explanation:**\n",
      "\n",
      "1. We define a function `generateFibonacci` that takes an integer `n` as input, representing the number of Fibonacci numbers to generate.\n",
      "2. We initialize two variables `num1` and `num2` to 0 and 1, respectively, which are the first two numbers in the Fibonacci sequence.\n",
      "3. We print the first two numbers using `printf`.\n",
      "4. We use a `for` loop to generate and print the remaining numbers in the sequence. In each iteration, we calculate the next number as the sum of `num1` and `num2`, and then update `num1` and `num2` for the next iteration.\n",
      "5. In the `main` function, we set `n` to 10 and call the `generateFibonacci` function to generate and print the first 10 Fibonacci numbers."
     ]
    }
   ],
   "source": [
    "developer_prompt = \"You are an expert of C programing in C language.\"\n",
    "user_prompt = \"Write down a C program that generates first ten numbers of fibonacci sequence.\"\n",
    "\n",
    "# Get streaming generator from Responses API\n",
    "response = ask_groq(user_prompt=user_prompt, developer_prompt=developer_prompt, stream=True)\n",
    "\n",
    "# Iterate through streaming events and only print text deltas\n",
    "for event in response:\n",
    "    # Each event may contain incremental text in event.delta\n",
    "    if hasattr(event, \"delta\") and event.delta:\n",
    "        print(event.delta, end=\"\", flush=True) # prints the content from this chunk, end=\"\" prevents adding a newline after each  piece and flush=True forces flushing output to screen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59419ea7-4e7a-4e81-b0a9-1077c0646c37",
   "metadata": {},
   "source": [
    "## g. Examples (Text Translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f032442a-499d-4910-9b8f-39c621898167",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "اس سال کے بجٹ کا کم تنخواہ لینے والے لوگوں پر بہت برا اثر پڑے گا۔\n"
     ]
    }
   ],
   "source": [
    "user_prompt = \"\"\"\n",
    "Please act as an expert of English to Urdu translator by translating the given sentence from English into Urdu.\n",
    "'The budget this year will have a very bad impact on the low salried people'\n",
    "\"\"\"\n",
    "response = ask_groq(user_prompt=user_prompt, model='meta-llama/llama-4-maverick-17b-128e-instruct')\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6604dcf4-31aa-4749-81db-77c7942feec0",
   "metadata": {},
   "source": [
    "## h. Examples (Text Summarization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3d11b223-2b1c-47cd-ab25-3577c46a9149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hugging Face transforms library aids natural language processing tasks.\n"
     ]
    }
   ],
   "source": [
    "developer_prompt = \"You are an expert of English language.\"\n",
    "\n",
    "user_prompt = f'''\n",
    "Summarize the text below in at most 20 words:\n",
    "```The Hugging Face transformers library is an incredibly versatile and powerful tool for natural language processing (NLP).\n",
    "It allows users to perform a wide range of tasks such as text classification, named entity recognition, and question answering, among others.\n",
    "It's an extremely popular library that's widely used by the open-source data science community.\n",
    "It lowers the barrier to entry into the field by providing Data Scientists with a productive, convenient way to work with transformer models.```\n",
    "'''\n",
    "\n",
    "response = ask_groq(user_prompt=user_prompt, developer_prompt=developer_prompt, temperature=1.0)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0e6b0837-a38c-40f9-ab24-bdd7cbeeaaf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Summary:** Our solar system consists of eight planets, each unique and orbiting the Sun. The planets vary in characteristics, such as size, atmosphere, and temperature. They play a vital role in the solar system's celestial dance.\n",
      "\n",
      "**Urdu Translation:** ہمارا شمسی نظام آٹھ سیاروں پر مشتمل ہے، ہر ایک منفرد اور سورج کے گرد گھومتا ہے۔ سیاروں کی خصوصیات میں اختلاف ہے، جیسے کہ سائز، فضا اور درجہ حرارت۔ وہ شمسی نظام کے سماوی رقص میں اہم کردار ادا کرتے ہیں۔\n",
      "\n",
      "**Python List:**\n",
      "```python\n",
      "planet_names = [\n",
      "    \"Mercury\",\n",
      "    \"Venus\",\n",
      "    \"Earth\",\n",
      "    \"Mars\",\n",
      "    \"Jupiter\",\n",
      "    \"Saturn\",\n",
      "    \"Uranus\",\n",
      "    \"Neptune\"\n",
      "]\n",
      "print(planet_names)\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "developer_prompt = \"You are a helpful assistant skilled in text summarization, translation to Urdu, and Python programming. You provide clear, accurate responses and follow instructions precisely.\"\n",
    "\n",
    "text = '''\n",
    "Our solar system, a celestial dance of eight planets, each with its unique character and charm, orbits around our radiant Sun.\n",
    "Closest to the Sun, Mercury, the smallest planet, darts swiftly, its metallic surface reflecting the Sun's intense glare.\n",
    "Venus, Earth's twin, cloaked in a dense atmosphere, harbors scorching temperatures and acidic clouds.\n",
    "Earth, our oasis of life, teems with diverse ecosystems, its oceans and landforms sculpted by the forces of nature.\n",
    "Mars, the Red Planet, bears the scars of ancient volcanoes and the promise of potential life.\n",
    "Beyond the asteroid belt, Jupiter and Saturn, the gas giants, reign supreme, their vast atmospheres swirling with storms and adorned with rings of ice and dust.\n",
    "Uranus and Neptune, the ice giants, tilt at odd angles, their atmospheres frigid and their depths still shrouded in mystery.\n",
    "Each planet, a celestial masterpiece, plays a vital role in the intricate symphony of our solar system.'''\n",
    "\n",
    "user_prompt = f'''\n",
    "Please complete the following two tasks based on the text provided below:\n",
    "\n",
    "Task 1: Summarize the text in 2-3 sentences, then translate that summary into Urdu.\n",
    "\n",
    "Task 2: Create a Python list containing all planet names mentioned in the text.\n",
    "\n",
    "Text: ```{text}```\n",
    "\n",
    "Please format your response as:\n",
    "**Summary:** [English summary]\n",
    "**Urdu Translation:** [Urdu translation]\n",
    "**Python List:** [Python code with planet names]\n",
    "'''\n",
    "\n",
    "response = ask_groq(user_prompt=user_prompt, developer_prompt=developer_prompt, model='meta-llama/llama-4-maverick-17b-128e-instruct', temperature=0.3)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686e5033-5745-4393-9e00-4ca8a6587ee4",
   "metadata": {},
   "source": [
    "## i. Examples (Named Entity Recognition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "67f047e5-d33c-49a8-ae5c-62d49e3ef7e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a summarized version of the information provided about Zelaid Mujahid:\n",
      "\n",
      "**Name:** Zelaid Mujahid\n",
      "**Nationality:** Pakistani\n",
      "**University:** University of the Punjab\n",
      "**Major:** Data Science\n",
      "**Year:** Sophomore\n",
      "**GPA:** 3.5\n",
      "**Extracurricular Activity:** Active member of the department's AI Club\n",
      "**Career Aspiration:** Pursue a career in AI after graduating.\n"
     ]
    }
   ],
   "source": [
    "developer_prompt = \"\"\"You are a  Named Entity Recognition specialist. Extract and classify entities from the given text into these categories only if they exist:\n",
    "- name\n",
    "- major\n",
    "- university\n",
    "- nationality\n",
    "- grades\n",
    "- club\n",
    "Format your response as: 'Entity: [text] | Type: [category]' with each entity on a new line.\"\"\"\n",
    "\n",
    "user_prompt = '''\n",
    "Zelaid Mujahid is a sophomore majoring in Data Science at University of the Punjab. \\\n",
    "He is Pakistani national and has a 3.5 GPA. Mujahid is an active member of the department's AI Club.\\\n",
    "He hopes to pursue a career in AI after graduating.\n",
    "'''\n",
    "response = ask_groq(user_prompt=user_prompt, developer_prompt=developer_prompt, model='meta-llama/llama-4-maverick-17b-128e-instruct', temperature=0.3)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be09b49-a9cc-419d-8f1c-8a3065b3be34",
   "metadata": {},
   "source": [
    "## j. Example (Grade School Math 8K (GSM8K))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4395f12a-9b07-43e6-ba58-9eb33fb9c0ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To solve this problem, I need some additional information. Can you please provide me with the following details:\n",
      "\n",
      "1. The daily revenue of the booth\n",
      "2. The daily cost of ingredients\n",
      "3. The rent for the 5-day period (is it a one-time payment or a daily payment?)\n",
      "\n",
      "Once I have this information, I can help you calculate the earnings of the booth for the 5-day period after paying the rent and the cost of ingredients.\n"
     ]
    }
   ],
   "source": [
    "developer_prompt = \"\"\"You are an expert School math teacher. \n",
    "Consider the following text and then answer the questions of the students from this:\n",
    "A carnival snack booth made $50 selling popcorn each day. It made three times as much selling cotton candy. \n",
    "For a 5-day activity, the booth has to pay $30 rent and $75 for the cost of the ingredients. \n",
    "\"\"\"\n",
    "user_prompt = \"How much did the booth earn for 5 days after paying the rent and the cost of ingredients?\"\n",
    "\n",
    "response = ask_groq(user_prompt=user_prompt, developer_prompt=developer_prompt,  model='llama-3.3-70b-versatile')\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4561cd52-2c50-4018-83e5-9e34dc83c4a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Step 1: First, let's determine the daily earnings of the booth.\n",
      "The daily earnings of the booth is $200.\n",
      "\n",
      "## Step 2: Next, let's calculate the total earnings for 5 days.\n",
      "Total earnings = daily earnings * 5 = $200 * 5 = $1000.\n",
      "\n",
      "## Step 3: Now, let's determine the daily rent and cost of ingredients.\n",
      "The daily rent is $50 and the daily cost of ingredients is $100.\n",
      "\n",
      "## Step 4: Calculate the total rent and cost of ingredients for 5 days.\n",
      "Total rent = daily rent * 5 = $50 * 5 = $250.\n",
      "Total cost of ingredients = daily cost of ingredients * 5 = $100 * 5 = $500.\n",
      "\n",
      "## Step 5: Calculate the total expenses for 5 days.\n",
      "Total expenses = total rent + total cost of ingredients = $250 + $500 = $750.\n",
      "\n",
      "## Step 6: Finally, let's calculate the earnings after paying the rent and the cost of ingredients for 5 days.\n",
      "Earnings after expenses = total earnings - total expenses = $1000 - $750 = $250.\n",
      "\n",
      "The final answer is: $\\boxed{250}$\n"
     ]
    }
   ],
   "source": [
    "developer_prompt = \"\"\"You are an expert School math teacher. \n",
    "Consider the following text and then answer the questions of the students from this:\n",
    "A carnival snack booth made $50 selling popcorn each day. It made three times as much selling cotton candy. \n",
    "For a 5-day activity, the booth has to pay $30 rent and $75 for the cost of the ingredients. \n",
    "\"\"\"\n",
    "user_prompt = \"How much did the booth earn for 5 days after paying the rent and the cost of ingredients?\"\n",
    "\n",
    "response = ask_groq(user_prompt=user_prompt, developer_prompt=developer_prompt,  model='meta-llama/llama-4-maverick-17b-128e-instruct')\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0b5dc8c2-136e-4ff7-87d4-f44778fe5d8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First, figure out how much the booth earned each day.\n",
      "\n",
      "| Item | Daily earnings |\n",
      "|------|----------------|\n",
      "| Popcorn | $50 |\n",
      "| Cotton candy | 3 × $50 = $150 |\n",
      "| **Total daily** | $50 + $150 = **$200** |\n",
      "\n",
      "**Five‑day revenue**\n",
      "\n",
      "$200 \\text{ per day} \\times 5 \\text{ days} = $1,000\n",
      "\n",
      "**Subtract the costs**\n",
      "\n",
      "* Rent: $30  \n",
      "* Ingredients: $75  \n",
      "\n",
      "Total costs = $30 + $75 = **$105**\n",
      "\n",
      "**Net earnings**\n",
      "\n",
      "$1,000 – $105 = **$895**\n",
      "\n",
      "So, after paying rent and ingredient costs, the booth earned **$895** over the 5‑day activity.\n"
     ]
    }
   ],
   "source": [
    "developer_prompt = \"\"\"You are an expert School math teacher. \n",
    "Consider the following text and then answer the questions of the students from this:\n",
    "A carnival snack booth made $50 selling popcorn each day. It made three times as much selling cotton candy. \n",
    "For a 5-day activity, the booth has to pay $30 rent and $75 for the cost of the ingredients. \n",
    "\"\"\"\n",
    "user_prompt = \"How much did the booth earn for 5 days after paying the rent and the cost of ingredients?\"\n",
    "\n",
    "response = ask_groq(user_prompt=user_prompt, developer_prompt=developer_prompt, model='openai/gpt-oss-20b')\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6fe112-1946-498b-8aaa-71741ee17263",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e184fb7-8dea-4470-811d-56a865dcae57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a127546-1f11-4964-82bc-40a88189ef2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (genai-uv)",
   "language": "python",
   "name": "genai-uv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
