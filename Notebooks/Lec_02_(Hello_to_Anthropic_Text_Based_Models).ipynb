{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a41c76e0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "---   \n",
    " <img align=\"left\" width=\"75\" height=\"75\"  src=\"https://upload.wikimedia.org/wikipedia/en/c/c8/University_of_the_Punjab_logo.png\"> \n",
    "\n",
    "<h1 align=\"center\">Department of Data Science</h1>\n",
    "\n",
    "---\n",
    "<h3><div align=\"right\">Instructor: Muhammad Arif Butt, Ph.D.</div></h3>    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6fe310",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "<h1 align=\"center\">Lec-02: A Hello to Anthropic Family of AI Models</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef15071",
   "metadata": {},
   "source": [
    "# Learning agenda of this notebook\n",
    "\n",
    "1. Installing Anthropic SDK and Accessing Anthropic Claude Family of AI Models via API keys\n",
    "    - Anthropic Claude Family of Models\n",
    "    - Installing Anthropic SDK\n",
    "    - Generating API Keys for Anthropic\n",
    "    - Saving the API Keys\n",
    "    - Accessing and Testing the API Keys\n",
    "    - Initialize the Anthropic Client uisng `anthropic.Anthropic()` Method\n",
    "    - Access API Endpoint: `client.models.list()`\n",
    "    - Access API Endpoint: `client.messages.create()`\n",
    "2. Hello World Examples using Claude family of Models for Text Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa00096-4d61-4b45-ab3f-bd56857e41d4",
   "metadata": {},
   "source": [
    "# <span style='background :lightgreen' >1. Installing Anthropic SDK and Accessing Anthropic AI Models via API keys</span>\n",
    "\n",
    "<h2 align=\"center\"><div class=\"alert alert-success\" style=\"margin: 20px\">Anthropic is an AI safety and research company founded in 2021 by Dario and Daniela and other researchers who left OpenAI to focus on AI alignment and safety.</h2>\n",
    "\n",
    "\n",
    "## a. Anthropic Claude Family of Models\n",
    "- **Overview:** Claude 3.x and onwards represents  the third generation of Anthropic’s family of frontier large language models.\n",
    "- Claude family of models are , engineered with a strong emphasis on **alignment**, **reliability**, and **safe reasoning**.\n",
    "- Named after Claude Shannon—the founder of information theory—the Claude models are built to be **helpful**, **harmless**, and **honest**.\n",
    "- They compete directly with OpenAI’s GPT models and Google’s Gemini family, and they are widely used across enterprise, research, and developer ecosystems.\n",
    "- **Access Points:**\n",
    "    - **Claude Chat Interface:** https://claude.ai/  \n",
    "    - **Anthropic Developer Console:** https://console.anthropic.com/\n",
    "- **Claude Model Tiers:**\n",
    "    - Anthropic’s models have historically been **text-first**, but the newer Claude 3 and Claude 4 series introduce expanding **multimodal capabilities**, allowing them to process images, charts, documents, and structured data.  \n",
    "    - The Claude family is organized into capability tiers—**Opus**, **Sonnet**, and **Haiku**—each optimized for different computational budgets and tasks.\n",
    "        - **Claude Opus 4:**  The highest-capability model, optimized for advanced reasoning, long-context workflows, complex tool use, and stateful agentic tasks. Particularly strong in scientific analysis, coding, and multi-step problem solving.\n",
    "        - **Claude Sonnet 4:**  Balances high reasoning power with efficiency. Ideal for day-to-day enterprise workflows, copilots, customer-facing systems, RAG and general-purpose agents.\n",
    "        - **Claude Haiku 3.5 / Haiku 3:**  The fastest and most cost-efficient tier. Designed for real-time applications where speed, scale, and low latency matter more than deep reasoning.\n",
    "\n",
    "## Claude Models: Complete Pricing & Specifications Table (December 2025)\n",
    "\n",
    "\n",
    "| **Model (Name + ID)**                                 | **Cost (Input / Output per MTok)** | **Context Window**                 | **Max Output Tokens** | **Notes / Performance**                                                                                                  |\n",
    "| ----------------------------------------------------- | ---------------------------------- | ---------------------------------- | --------------------- | ------------------------------------------------------------------------------------------------------------------------ |\n",
    "| **Claude Opus 4.5**<br>`claude-opus-4-5-20251101`     | $5.00 / $25.00                     | 200K tokens                        | ~32K†                 | Latest top-tier model with strong reasoning & coding performance, more efficient than classic Opus. |\n",
    "| **Claude Opus 4.1**<br>`claude-opus-4-1-20250805`     | $15 / $75                          | 200K tokens                        | ~32K                  | High-performance premium model.                            |\n",
    "| **Claude Opus 4**<br>`claude-opus-4-20250514`         | $15 / $75                          | 200K tokens                        | ~32K                  | Classic flagship Opus model.                            |\n",
    "| **Claude Sonnet 4.5**<br>`claude-sonnet-4-5-20250929` | $3.00 / $15.00                     | 200K tokens (1M beta available)    | ~64K                  | Balanced model with extended reasoning and larger output; beta 1M context supported.     |\n",
    "| **Claude Sonnet 4**<br>`claude-sonnet-4-20250514`     | $3.00 / $15.00                     | 200K tokens (1M beta available)    | ~64K                  | Strong reasoning with large context support.                                     |\n",
    "| **Claude Sonnet 3.7**<br>`claude-3-7-sonnet-20250219` | $3.00 / $15.00                     | 200K tokens (128K beta for output) | ~64K / 128K† (beta)   | Legacy mid-tier model; extended output via beta header.                            |\n",
    "| **Claude Sonnet 3.5**<br>`claude-3-5-sonnet-20241022` | $3.00 / $15.00                     | 200K tokens                        | ~8K                   | Mid-tier model good for general tasks.            |\n",
    "| **Claude Haiku 4.5**<br>`claude-haiku-4-5-20251001`   | $1.00 / $5.00                      | 200K tokens                        | ~8K                   | Very cost-efficient with competitive performance.                            |\n",
    "| **Claude Haiku 3.5**<br>`claude-3-5-haiku-20241022`   | $0.80 / $4.00                      | 200K tokens                        | ~8K                   | Legacy Haiku model — fast & cheap.                            |\n",
    "| **Claude Haiku 3**<br>`claude-3-haiku-20240307`       | $0.25 / $1.25                      | 200K tokens                        | ~4K                   | Most cost-effective legacy model.                             |\n",
    "\n",
    "## Notes\n",
    "\n",
    "- **MTok** = Input and output cost is mentioned per one Million Tokens (1,000,000 tokens)\n",
    "- **Context Window**: Maximum tokens that the model can process in one go\n",
    "- **Max Output**: Maximum tokens the model can generate in a single response\n",
    "- **Latency**: Approximate response time for typical queries (varies with load and output length)\n",
    "- **Deprecated**: Claude Sonnet 3.5, and Claude Sonnet 3.7 models are scheduled for retirement in 2026"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a4efa9-f287-4d9e-95e8-101291121a3e",
   "metadata": {},
   "source": [
    "## b. Installing Anthropic SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03a6d26d-2abd-48dd-8e8a-4b432b43cdcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install anthropic SDK\n",
    "#!uv add anthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e43ed50-0f8c-467b-a685-d81c5c6dc309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mResolved \u001b[1m240 packages\u001b[0m \u001b[2min 0.85ms\u001b[0m\u001b[0m\n",
      "├── anthropic v0.75.0\n"
     ]
    }
   ],
   "source": [
    "# check out the version of your anthropic installed package\n",
    "!uv tree | grep anthropic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07e7f75-c8bb-4b2e-abb8-4726f8bc09ee",
   "metadata": {},
   "source": [
    "## c. Generating API Keys for Anthropic \n",
    "<h3 align=\"center\"><div class=\"alert alert-success\" style=\"margin: 20px\">Anthropic lets you create an API key, but usage is billed per request (no permanent free tier)</h3>\n",
    "\n",
    "- Today we will be accessing Anthropic Claude LLMs via API — meaning you call the model running on Anthropic’s cloud, and it will respond with answers to your questions.\n",
    "- Follow the steps below to generate your Anthropic API key:\n",
    "    - Visit Anthropic Console: Go to https://console.anthropic.com/ and sign in with your Google account, GitHub account, or create a new account.\n",
    "    - Enable API access: Once logged in, navigate to the API Keys section in the left-hand menu.\n",
    "    - Create API key: Generate a new API key for use with Claude models. This key will authenticate your requests to Anthropic’s API.\n",
    "    - Save your key: Copy the API key and store it securely (e.g., in environment variables). Do not hardcode it in public code or share it.\n",
    "    - Set up billing: Anthropic requires you to add a payment method for API usage. Costs are based on the number of input and output tokens.\n",
    "    - Pricing reference: For the most up-to-date information on Claude model costs (e.g., Claude 3 Opus, Sonnet, Haiku, Claude 3.5 Sonnet), visit: https://docs.anthropic.com/claude/docs/pricing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302ffe92-7905-4e20-8d84-be7ae22405d8",
   "metadata": {},
   "source": [
    "## d. Saving the API Keys\n",
    "- An **environment variable** is a key–value pair stored by your operating system that programs can access at runtime. They’re used to configure programs without hardcoding settings in code. Think of them as invisible notes your system uses to tell programs what to do. For example: When you run a Python program, it might check an environment variable called DEBUG to decide whether to show detailed error logs. APIs like OpenAI or Stripe often require you to store your secret API keys in an environment variable like ANTHROPIC_API_KEY.\n",
    "\n",
    "#### Option 1: (Save API Keys in an environment variable on Mac/Linux/Windows)\n",
    "- For Linux/MAC machines, create an environment variable named ANTHROPIC_API_KEY inside .zshrc and save your key there by running this command on your terminal:`echo \"export ANTHROPIC_API_KEY='yourkey'\" >> ~/.zshrc`\n",
    "- Update the shell with the new variable by running the command: `source ~/.zshrc`\n",
    "- Confirm that you have set your environment variable using the command `echo $ANTHROPIC_API_KEY`\n",
    "- Now inside your Python code, you can access your key using this LOC `openai.api_key = os.environ[\"ANTHROPIC_API_KEY\"]`\n",
    "\n",
    "#### Option 2: (Save API Keys inside `.env` file):\n",
    "- The best way is to create a `.env` or just `env` file within your Python project directory and save all your API keys inside it, and it doesnot go into your git repositories. Your .env file must not be shared publicly as it will contain secrets like API keys or passwords. To ensure this you should add `.env` to your `.gitignore` file.\n",
    "- In a real project, we may need to use more than one API keys, one for OpenAI, another for Hugging Face, another for Google's Gemini, another for Pinecone, and so on. Finally, your `.env` file might contain many keys like:\n",
    "```\n",
    "OPENAI_API_KEY=sk-proj-xxxx\n",
    "ANTHROPIC_API_KEY=sk-proj-xxxx\n",
    "GOOGLE_API_KEY=sk-proj-xxxx\n",
    "DEEPSEEK_API_KEY=sk-proj-xxxx\n",
    "```\n",
    "\n",
    "#### Points to Ponder:\n",
    "- **Rotate Keys Regularly:** Must rotate your keys regularly. A quarterly rotation schedule is a widely accepted best practice. It minimizes the window of vulnerability in case a key ever gets compromised.\n",
    "- **Environment Separation:** Use distinct keys for development, testing, and production environments. This isolation ensures that even if a test key is leaked, your production systems and costs remain protected.\n",
    "- **Least Privilege Access:** Configure your API keys with only the minimal permissions needed for each task. For example, if one service only needs read access, don't give it full control. Limiting scope prevents small issues from turning into major security breaches.\n",
    "- **Monitor usage Pattern:** Most API providers offer dashboards or logs that show request frequency and token consumption. Keep an eye out for sudden spikes or unusual activity. These often indicate leaked credentials or automated misuse."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19dbabd3-7e51-45d7-a993-e922c05c74b1",
   "metadata": {},
   "source": [
    "## e. Accessing and Testing the API Keys\n",
    "- The `load_dotenv()` method looks for a file named `.env` in the current directory or in the specified path as mentioned in its first argument. The second argument `override=True` is optional and it means  any existing environment variables will be overwritten by values from the .env file. This method reads each line in the `.env` file and sets the environment variables accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95224568-482c-4c0e-bf9d-b649b6aa534b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anthropic API Key exists and begins sk-ant-api03-ZL\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv('../keys/.env', override=True) \n",
    "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "\n",
    "if anthropic_api_key:\n",
    "    print(f\"Anthropic API Key exists and begins {anthropic_api_key[:15]}\")\n",
    "else:\n",
    "    print(\"Anthropic API Key not set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a744d83f-658b-47e8-a590-766f1ddff3a0",
   "metadata": {},
   "source": [
    "## f. Initialize the Anthropic Client using `anthropic.Anthropic()` Method\n",
    "- The anthropic Python library is an abstraction layer that wraps Anthropic’s REST API, making it easy to call Claude (or other Claude-family) models from your Python code. It handles HTTP requests and response parsing, converting JSON responses into Python objects. \n",
    "- You create a client instance using the `anthropic.Anthropic()` (synchronous) or `anthropic.AsyncAnthropic()` (asynchronous) constructor. This is where you configure your API key and other options like retry behavior, timeouts, base URL, and HTTP client\n",
    "```python\n",
    "import os\n",
    "from anthropic import Anthropic\n",
    "\n",
    "client = Anthropic(\n",
    "    api_key=os.getenv(\"ANTHROPIC_API_KEY\"),  # REQUIRED (or pass it explicitly)\n",
    "    base_url=None,                           # OPTIONAL (defaults to https://api.anthropic.com) used to override the default API URL (e.g. for Anthropic-compatible proxies) \n",
    "    timeout=None,                            # OPTIONAL — A float value (in seconds) that controls how long to wait for API requests before giving up.\n",
    "    max_retries=2,                           # OPTIONAL — An integer value, used to set max automatic retries on network failures (default is 2)\n",
    "    default_headers=None,                    # OPTIONAL — A dictionary used to set additional headers to send with every request\n",
    "    http_client=None,                        # OPTIONAL — custom httpx.Client\n",
    ")\n",
    "```\n",
    "- The return value is a synchronous Anthropic client object, which you use to make API calls, for example: `client.models.list()`, `client.messages.create()` and others."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c3e53e-d902-4a8f-9a75-40c83ffcfd20",
   "metadata": {},
   "source": [
    "## g. Access API Endpoint: `client.models.list()`\n",
    "- Think of an **API Endpoint** like different departments in a building - each department (endpoint) handles different tasks.\n",
    "- All API requests start with a base URL, and then you add endpoint paths to access specific functions. (base url is like the main entrance of a building, while endpoints are the different rooms inside.\n",
    "- Think of it this way:\n",
    "    - An API (Application Programming Interface) is like a menu in a restaurant.\n",
    "    - An endpoint is a specific item on the menu that you can order, with its own format and expected inputs.\n",
    "    - The LLM model is the “kitchen” that prepares the output.\n",
    "- The Base URL specifies where to send requests. For OpenAI the base url is `https://api.anthropic.com/v1`\n",
    "- The Full URL endpoint is like a door into a different capability of the model.\n",
    "\n",
    "\n",
    "| Client Method                                             | Full URL Endpoint                                          | Typical Models Used                                                                    | API Action & Description                                                                                                           |\n",
    "| --------------------------------------------------------- | ---------------------------------------------------------- | -------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| **`client.models.list()`**                                | `https://api.anthropic.com/v1/models`                      | All Claude models: **Opus 3/4/4.1/4.5**, **Sonnet 3/3.5/4/4.5**, **Haiku 3/3.5/4/4.5** | Returns a list of all models available to your API key (IDs, capabilities, input types).                                           |\n",
    "| **`client.messages.create()`**                            | `https://api.anthropic.com/v1/messages`                    | All text/vision Claude models: **Opus**, **Sonnet**, **Haiku**                         | Primary chat/completions endpoint — generate text, answer questions, follow instructions, use vision, or interact with tool calls. |\n",
    "| **`client.messages.stream(...)`** *(SDK usage pattern)*   | `wss://api.anthropic.com/v1/messages?stream=true`          | Same as above                                                                          | Streams assistant responses chunk-by-chunk (incremental generation).                                                               |\n",
    "| **Token Counting (`client.tokens.count()`)**              | `https://api.anthropic.com/v1/messages/count_tokens`       | All Claude chat models                                                                 | Estimates tokens before a request — useful for price control and max_token planning.                                               |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31597733-63dc-4813-8dec-7b84f5d8a87f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - claude-opus-4-5-20251101\n",
      "  - claude-haiku-4-5-20251001\n",
      "  - claude-sonnet-4-5-20250929\n",
      "  - claude-opus-4-1-20250805\n",
      "  - claude-opus-4-20250514\n",
      "  - claude-sonnet-4-20250514\n",
      "  - claude-3-7-sonnet-20250219\n",
      "  - claude-3-5-haiku-20241022\n",
      "  - claude-3-haiku-20240307\n"
     ]
    }
   ],
   "source": [
    "# List the available models\n",
    "from anthropic import Anthropic\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv('../keys/.env', override=True) \n",
    "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "\n",
    "# Initialize Anthropic client\n",
    "client = Anthropic(base_url=\"https://api.anthropic.com\", api_key=anthropic_api_key) #The correct default API endpoint is already baked into the SDK, so it is optional to provide\n",
    "\n",
    "models = client.models.list() # calls the /models endpoint and returns a list of model objects.\n",
    "\n",
    "# Each object in models_response.data contains information like id, object, created, and permission. We loop through and print model.id to see the model names you can use with endpoints like chat, embeddings, or images.\n",
    "for m in models.data:\n",
    "        print(f\"  - {m.id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25165941-31b5-4494-9a43-0465c7f6f053",
   "metadata": {},
   "source": [
    "## h. Access API Endpoint: `client.messages.create()`\n",
    "<h3 align=\"center\"><div class=\"alert alert-success\" style=\"margin: 20px\">The Anthropic `messages.create()` endpoint is a simple, stateless API for generating AI responses where you manually manage conversation history and send it with each request.</h3>\n",
    "\n",
    "- This endpoint is the current standard for Anthropic's Claude API and supports all modern Claude models including Claude Opus 4.1, Claude Sonnet 4.5, Claude Haiku 4.5, and their variants.\n",
    "- Unlike Google, there is no OpenAI-compatible endpoint—so you must use Anthropic’s own SDK or REST API.\n",
    "- In the following code, `client` represents the connection, `messages` represents the endpoint, and `create` is the method with two required parameters:\n",
    "```python\n",
    "response = client.messages.create(\n",
    "    model=\"claude-3-haiku-20240307\",                         # REQUIRED — model ID for Claude\n",
    "    max_tokens=1024,                                         # REQUIRED — maximum number of tokens to generate\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Hello, Claude\"}], # REQUIRED — list of message dicts with \"role\" and \"content\"\n",
    "    system=None,                                             # OPTIONAL — system instruction/prompt to set behavior\n",
    "    temperature=1.0,                                         # OPTIONAL — sampling temperature (0.0 to 1.0)\n",
    "    top_p=None,                                              # OPTIONAL — nucleus sampling parameter\n",
    "    top_k=None,                                              # OPTIONAL — top-k sampling parameter\n",
    "    stop_sequences=None,                                     # OPTIONAL — list of strings where generation stops\n",
    "    stream=False,                                            # OPTIONAL — whether to stream response\n",
    "    thinking=None,                                           # OPTIONAL — enable extended thinking (beta)\n",
    "    tool_choice=None,                                        # OPTIONAL — specify tool selection strategy\n",
    "    tools=None,                                              # OPTIONAL — list of tools/functions Claude can call\n",
    "    metadata=None,                                           # OPTIONAL — arbitrary metadata dict\n",
    "    timeout=None,                                            # OPTIONAL — request timeout override\n",
    ")\n",
    "```\n",
    "\n",
    "###  The `model` Parameter (Anthropic)\n",
    "- The model parameter identifies the Claude model to which your request will be sent.\n",
    "- Anthropic recommends using specific dated versions in production for consistency, as aliases automatically update to newer snapshots.\n",
    "- Anthropic recommend to use dated model snapshots in production, ensuring consistent behavior across deployments.\n",
    "    - `claude-opus-4-1-20250514` — Most capable model for complex reasoning\n",
    "    - `claude-sonnet-4-5-20250929` — Balanced performance and efficiency (latest Sonnet)\n",
    "    - `claude-sonnet-4-20250514` — Previous Sonnet 4 version\n",
    "    - `claude-haiku-4-5-20251001` — Fastest, most cost-effective (latest Haiku)\n",
    "    - `claude-3-opus-20240229` — Deprecated June 2025, retiring January 2026\n",
    "    - `claude-3-5-sonnet-20241022` — Previous generation\n",
    "    - `claude-3-5-sonnet-20240620` — Earlier version\n",
    "    - `claude-3-5-haiku-20241022` — Previous Haiku\n",
    "    - `claude-3-haiku-20240307` — Older Haiku\n",
    "\n",
    "### The `messages` Parameter (Anthropic API)\n",
    "- Anthropic does not support system or developer roles directly inside the prompt, like OpenAI.\n",
    "- The `messages` parameter of Anthropic API is a list of dictionaries each having two properties: role and content. The role can take one of two values:\n",
    "    - **user:** Input from the human user — questions, instructions, statements\n",
    "    - **assistant:** Previous model responses (optional; helps maintain conversation context)\n",
    "- To specify developer instructions or prompt to set behavior of the model in Anthropic, you can use its separate optional parameter `system`\n",
    "\n",
    "```\n",
    "system = \"You are a helpful tutor. Explain with clarity and simple examples.\"\n",
    "messages = [{\"role\": \"user\", \"content\": \"Teach me about overfitting.\"}]\n",
    "\n",
    "system = \"You are a data analyst. Provide concise, actionable insights.\"\n",
    "messages = [{\"role\": \"user\", \"content\": \"Summarize the key insights from this report in 4 bullet points.\"}]\n",
    "\n",
    "system = \"Extract key entities and return JSON with fields: person, organization, date.\"\n",
    "messages = [{\"role\": \"user\", \"content\": \"Apple Inc. announced on January 15, 2024 that CEO Tim Cook will present at the conference.\"}]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5759c82d-2336-4724-8fff-cf3a1d29205c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of Pakistan is Islamabad.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Message</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">id</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'msg_0149Zf3Lrw3ruBr2ztZqDAho'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"font-weight: bold\">[</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">TextBlock</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">citations</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>, <span style=\"color: #808000; text-decoration-color: #808000\">text</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'The capital of Pakistan is Islamabad.'</span>, <span style=\"color: #808000; text-decoration-color: #808000\">type</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'text'</span><span style=\"font-weight: bold\">)]</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">model</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'claude-3-haiku-20240307'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">role</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'assistant'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">stop_reason</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'end_turn'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">stop_sequence</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">type</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'message'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">usage</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Usage</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">cache_creation</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">CacheCreation</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">ephemeral_1h_input_tokens</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #808000; text-decoration-color: #808000\">ephemeral_5m_input_tokens</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">cache_creation_input_tokens</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">cache_read_input_tokens</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">input_tokens</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">20</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">output_tokens</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">server_tool_use</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">service_tier</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'standard'</span>\n",
       "    <span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mMessage\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mid\u001b[0m=\u001b[32m'msg_0149Zf3Lrw3ruBr2ztZqDAho'\u001b[0m,\n",
       "    \u001b[33mcontent\u001b[0m=\u001b[1m[\u001b[0m\u001b[1;35mTextBlock\u001b[0m\u001b[1m(\u001b[0m\u001b[33mcitations\u001b[0m=\u001b[3;35mNone\u001b[0m, \u001b[33mtext\u001b[0m=\u001b[32m'The capital of Pakistan is Islamabad.'\u001b[0m, \u001b[33mtype\u001b[0m=\u001b[32m'text'\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m,\n",
       "    \u001b[33mmodel\u001b[0m=\u001b[32m'claude-3-haiku-20240307'\u001b[0m,\n",
       "    \u001b[33mrole\u001b[0m=\u001b[32m'assistant'\u001b[0m,\n",
       "    \u001b[33mstop_reason\u001b[0m=\u001b[32m'end_turn'\u001b[0m,\n",
       "    \u001b[33mstop_sequence\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "    \u001b[33mtype\u001b[0m=\u001b[32m'message'\u001b[0m,\n",
       "    \u001b[33musage\u001b[0m=\u001b[1;35mUsage\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mcache_creation\u001b[0m=\u001b[1;35mCacheCreation\u001b[0m\u001b[1m(\u001b[0m\u001b[33mephemeral_1h_input_tokens\u001b[0m=\u001b[1;36m0\u001b[0m, \u001b[33mephemeral_5m_input_tokens\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1m)\u001b[0m,\n",
       "        \u001b[33mcache_creation_input_tokens\u001b[0m=\u001b[1;36m0\u001b[0m,\n",
       "        \u001b[33mcache_read_input_tokens\u001b[0m=\u001b[1;36m0\u001b[0m,\n",
       "        \u001b[33minput_tokens\u001b[0m=\u001b[1;36m20\u001b[0m,\n",
       "        \u001b[33moutput_tokens\u001b[0m=\u001b[1;36m12\u001b[0m,\n",
       "        \u001b[33mserver_tool_use\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "        \u001b[33mservice_tier\u001b[0m=\u001b[32m'standard'\u001b[0m\n",
       "    \u001b[1m)\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from anthropic import Anthropic\n",
    "import rich\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv('../keys/.env', override=True) \n",
    "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "\n",
    "# Initialize Anthropic client\n",
    "client = Anthropic(base_url=\"https://api.anthropic.com\", api_key=anthropic_api_key) #The correct default API endpoint is already baked into the SDK, so it is optional to provide\n",
    "\n",
    "\n",
    "response = client.messages.create(\n",
    "                        model=\"claude-3-haiku-20240307\",                                            # required: an explicit model version strings (e.g., \"claude-sonnet-4-5-20250929\", not just \"claude-4\")\n",
    "                        messages=[{\"role\": \"user\", \"content\": \"What is the capital of Pakistan?\"}], # required: list of message objects with \"role\": Either \"user\" or \"assistant\" and \"content\": (text, images, documents)\n",
    "                        max_tokens=1024,                                       # required: an integer for maximum tokens to generate (must be > 1) A deliberate design choice prioritizing cost predictability and explicit API contracts over convenience.\n",
    "                        system=\"You are a helpful assistant.\",                 # optional: System prompts are specified in a separate system field, not in the messages array (as in openai api)  \n",
    ")\n",
    "\n",
    "print(response.content[0].text) # do understand the structure of the response object returned by Anthropic messages.create() method\n",
    "rich.print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee68928-4968-473e-9841-2408ab928d15",
   "metadata": {},
   "source": [
    "# <span style='background :lightgreen' >2. Hello World Examples using Claude family of Models for Text Processing</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19706209-bd89-4c50-a74a-85924380ef08",
   "metadata": {},
   "source": [
    "## Writing a Function for our ease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2d078ae8-9c33-420d-8bec-0edd3306b533",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from anthropic import Anthropic\n",
    "\n",
    "# Load API key from .env\n",
    "load_dotenv(\"../keys/.env\", override=True)\n",
    "anthropic_api_key = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "\n",
    "# Initialize Anthropic client\n",
    "client = Anthropic(api_key=anthropic_api_key)\n",
    "\n",
    "def ask_claude(\n",
    "                messages: str,\n",
    "                system: str = \"You are a helpful assistant that provides concise answers.\", \n",
    "                model: str = \"claude-3-haiku-20240307\",  # Cheapest Claude model\n",
    "                max_tokens: int = 1024,\n",
    "                temperature: float = 0.7,\n",
    "                top_p: float = 1.0,\n",
    "                stream: bool = False\n",
    "            ):\n",
    "\n",
    "    response = client.messages.create(\n",
    "                                    model=model,\n",
    "                                    messages=[{\"role\": \"user\", \"content\": user_prompt}],\n",
    "                                    max_tokens=max_tokens,\n",
    "                                    system=system_message,\n",
    "                                    temperature=temperature,\n",
    "                                    top_p=top_p,\n",
    "                                    stream=stream\n",
    "                                )    \n",
    "    if stream:\n",
    "        return response                 # Return streaming generator if requested\n",
    "    return response.content[0].text     # Extract text from the response content blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14df27e6-a811-4444-acf1-f58934efb1dc",
   "metadata": {},
   "source": [
    "## a. Examples (Question Answering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fa0e3364-4be4-4cb4-87e3-63cc5bdbb81d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of Pakistan is Islamabad.\n"
     ]
    }
   ],
   "source": [
    "system_message = \"You are a geography expert. Provide brief, accurate answers.\"\n",
    "user_prompt = \"What is the capital of Pakistan?\"\n",
    "\n",
    "response = ask_claude(messages=user_prompt, system=system_message)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6a6ddefd-afbb-4f06-8687-cec199920582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a story about Ali Baba and the Forty Thieves:\n",
      "\n",
      "Once upon a time, there lived a poor woodcutter named Ali Baba. One day, while he was out gathering firewood in the forest, he came across a group of forty thieves guarding the entrance to a secret cave. \n",
      "\n",
      "He heard the leader of the thieves say the magic words \"Open Sesame!\" and watched as the cave's entrance opened up, revealing a vast treasure hoard inside. After the thieves had gone, Ali Baba approached the cave and uttered the magic words himself. The entrance opened, and Ali Baba went inside, amazed at the piles of gold and jewels. He filled his bags with as much treasure as he could carry and hurried home.\n",
      "\n",
      "Ali Baba shared his newfound wealth with his poor brother Cassim, who became greedy and went to the cave himself. But he forgot the magic words to close the entrance, and the thieves caught him inside. They cut him into pieces as punishment.\n",
      "\n",
      "When Ali Baba went to the cave later, he found his brother's remains and gave him a proper burial. He then devised a plan to get rid of the thieves. With the help of a clever servant girl named Morgiana, Ali Baba was able to outwit the thieves one by one and eventually kill their leader.\n",
      "\n",
      "In the end, Ali Baba became a wealthy and respected man, all thanks to his discovery of the thieves' secret cave. He lived the rest of his days in comfort, never again having to worry about poverty. And the forty thieves were never seen or heard from again."
     ]
    }
   ],
   "source": [
    "system_message = \"You are a bed time story teller\"\n",
    "user_prompt = \"Tell me a story of Ali Baba Chalees Chor\"\n",
    "\n",
    "response = ask_claude(messages=user_prompt,system=system_message, stream=True)\n",
    "\n",
    "\n",
    "# Claude sends different event types during a streaming reply. The \"content_block_delta\" means: “Here is a new chunk of generated text that the model just produced.”\n",
    "for event in response:\n",
    "        if event.type == \"content_block_delta\":\n",
    "            print(event.delta.text, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858fca34-38a0-4af0-b06f-cbf36e477a4d",
   "metadata": {},
   "source": [
    "## b. Question Answering from Content Passed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fa3b24da-4c2f-4192-bcc9-4eb539fd282a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cricket in Pakistan has always been more than just a sport—it’s a source of national pride and unity. Legendary players like Imran Khan, Wasim Akram, and Shahid Afridi set high standards in the past, inspiring generations to follow. Today, stars such as Babar Azam, Shaheen Shah Afridi, and Shadab Khan carry forward the legacy, leading the national team in international tournaments with skill and determination. Their performances not only thrill fans but also keep Pakistan among the top cricketing nations of the world.\n",
      "\n",
      "Politics in Pakistan, meanwhile, remains dynamic and often turbulent, with key figures shaping the country’s direction. Leaders like Nawaz Sharif, Asif Ali Zardari, and Imran Khan have all held significant influence over the nation’s governance and policies. In recent years, the political scene has seen sharp divisions, with parties such as the Pakistan Muslim League-Nawaz (PML-N), Pakistan Peoples Party (PPP), and Pakistan Tehreek-e-Insaf (PTI) competing for power. Debates around economic reforms, governance, and foreign policy continue to dominate the national conversation, reflecting the challenges and aspirations of the Pakistani people."
     ]
    }
   ],
   "source": [
    "!cat ../data/names.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e068f48d-82d9-4d4b-8465-a86882a20da7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The names extracted from the given text are:\n",
      "\n",
      "1. Imran Khan\n",
      "2. Wasim Akram\n",
      "3. Shahid Afridi\n",
      "4. Babar Azam\n",
      "5. Shaheen Shah Afridi\n",
      "6. Shadab Khan\n",
      "7. Nawaz Sharif\n",
      "8. Asif Ali Zardari\n",
      "9. Pakistan Muslim League-Nawaz (PML-N)\n",
      "10. Pakistan Peoples Party (PPP)\n",
      "11. Pakistan Tehreek-e-Insaf (PTI)\n"
     ]
    }
   ],
   "source": [
    "with open(\"../data/names.txt\", \"r\") as f:\n",
    "    file_content = f.read()\n",
    "\n",
    "user_prompt = f\"Extract names from this text:\\n{file_content}\"\n",
    "response = ask_claude(messages=user_prompt, temperature=0.0)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f2bfa5cf-fe4d-42ea-9d7b-1ab6ff60817c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cricket players mentioned in the text are:\n",
      "\n",
      "1. Imran Khan\n",
      "2. Wasim Akram\n",
      "3. Shahid Afridi\n",
      "4. Babar Azam\n",
      "5. Shaheen Shah Afridi\n",
      "6. Shadab Khan\n"
     ]
    }
   ],
   "source": [
    "with open(\"../data/names.txt\", \"r\") as f:\n",
    "    file_content = f.read()\n",
    "\n",
    "user_prompt = f\"Can you extract names the Cricket players from this text:\\n{file_content}\"\n",
    "response = ask_claude(messages=user_prompt, temperature=0.0)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0a80311e-f12d-4543-bf06-38f7005ed640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This text can be categorized into two main sections:\n",
      "\n",
      "1. Cricket in Pakistan:\n",
      "   - This section discusses the importance of cricket in Pakistan, describing it as a source of national pride and unity.\n",
      "   - It highlights the legacy of legendary Pakistani cricketers like Imran Khan, Wasim Akram, and Shahid Afridi, and how they have inspired the current generation of stars like Babar Azam, Shaheen Shah Afridi, and Shadab Khan.\n",
      "   - The section emphasizes how the national cricket team's performances keep Pakistan among the top cricketing nations in the world.\n",
      "\n",
      "2. Politics in Pakistan:\n",
      "   - This section focuses on the dynamic and often turbulent nature of Pakistani politics.\n",
      "   - It discusses the influential political figures in the country, such as Nawaz Sharif, Asif Ali Zardari, and Imran Khan, and their roles in shaping the nation's governance and policies.\n",
      "   - The section also highlights the ongoing political competition between major parties like the PML-N, PPP, and PTI, and the debates around economic reforms, governance, and foreign policy that dominate the national conversation.\n",
      "\n",
      "Overall, the text provides an overview of the significance of cricket and the dynamics of politics in Pakistan, reflecting the country's cultural and sociopolitical landscape.\n"
     ]
    }
   ],
   "source": [
    "with open(\"../data/names.txt\", \"r\") as f:\n",
    "    file_content = f.read()\n",
    "\n",
    "user_prompt = f\"Can you categorize the following text:\\n{file_content}\"\n",
    "response = ask_claude(messages=user_prompt, temperature=0.0)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d017a7-9e7b-4b9d-8290-8d7615efe623",
   "metadata": {},
   "source": [
    "## c. Examples (Binary Classification: Sentiment analysis, Spam detection, Medical diagnosis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5ee0768a-5b22-4cab-ba61-6e2c872f811d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the given sentence, the sentiment is Positive.\n",
      "\n",
      "The sentence expresses a positive sentiment towards the YouTube videos of Arif, describing them as \"very informative.\"\n"
     ]
    }
   ],
   "source": [
    "system_message = \"You are an expert who will classify a sentense as having either a Positive or Negative sentiment.\"\n",
    "user_prompt = \"I love the youtube videos of Arif, as they are very informative\"\n",
    "response = ask_claude(messages=user_prompt, system=system_message)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dec2dfe8-b728-487d-9b08-53e10859c15d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the given sentence, I would classify the sentiment as **Negative**.\n",
      "\n",
      "The sentence indicates that the upcoming budget will have a \"very bad impact\" on low-salaried people, which suggests a negative sentiment towards the expected effects of the budget.\n"
     ]
    }
   ],
   "source": [
    "system_message = \"You are an expert who will classify a sentense as having either a Positive or Negative sentiment.\"\n",
    "user_prompt = \"The budget this year will have a very bad impact on the low salried people\"\n",
    "response = ask_claude(messages=user_prompt, system=system_message)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60aa6a0f-0ecf-4b33-8865-2967c1ef1b17",
   "metadata": {},
   "source": [
    "## d. Examples (Multi-class Classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "548ee064-b2f0-42d9-9560-228b2f80b99f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Books\n"
     ]
    }
   ],
   "source": [
    "system_message = \"Classify product reviews into these categories: 'Electronics', 'Clothing', 'Books', 'Home & Garden', 'Sports', or 'Food'. Respond with only the category.\"\n",
    "user_prompt = \"This novel has an incredible plot twist that kept me reading all night\"\n",
    "\n",
    "response = ask_claude(messages=user_prompt, system=system_message)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1046ae52-d5c9-41ab-8a85-158d5a92451e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Electronics\n"
     ]
    }
   ],
   "source": [
    "system_message = \"Classify product reviews into these categories: 'Electronics', 'Clothing', 'Books', 'Home & Garden', 'Sports', or 'Food'. Respond with only the category.\"\n",
    "user_prompt = \"The wireless headphones have excellent sound quality and battery life\"\n",
    "\n",
    "response = ask_claude(messages=user_prompt, system=system_message)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e08e4f78-b219-455d-be7f-bf116ee2b341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Food\n"
     ]
    }
   ],
   "source": [
    "system_message = \"Classify product reviews into these categories: 'Electronics', 'Clothing', 'Books', 'Home & Garden', 'Sports', or 'Food'. Respond with only the category.\"\n",
    "user_prompt = \"The coffee beans have a rich aroma and smooth taste\"\n",
    "\n",
    "response = ask_claude(messages=user_prompt, system=system_message)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "45028445-9e3b-47a6-a2b1-2c5a9ca36d18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clothing\n"
     ]
    }
   ],
   "source": [
    "system_message = \"Classify product reviews into these categories: 'Electronics', 'Clothing', 'Books', 'Home & Garden', 'Sports', or 'Food'. Respond with only the category.\"\n",
    "user_prompt = \"These running shoes are comfortable but not very durable\"\n",
    "\n",
    "response = ask_claude(messages=user_prompt, system=system_message)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e8e7e5-0357-4076-8512-8a16445b6ea3",
   "metadata": {},
   "source": [
    "## e. Examples (Text Generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d3dfff7c-d2f9-4ac1-9836-c76c0a353956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generative AI is revolutionizing the healthcare industry, particularly in the realms of diagnostic imaging and drug discovery. By harnessing the power of machine learning, these transformative technologies are enabling faster, more accurate diagnoses and accelerating the drug development process. From analyzing medical scans to identifying potential drug candidates, generative AI is poised to reshape the future of healthcare, driving unprecedented advancements in patient care and medical innovation.\n"
     ]
    }
   ],
   "source": [
    "system_message = \"You are a seasoned technology journalist with expertise in artificial intelligence and machine learning trends.\"\n",
    "user_prompt = \"Write a 75-word article introduction explaining how generative AI is transforming the healthcare industry, focusing on diagnostic imaging and drug discovery.\"\n",
    "\n",
    "response = ask_claude(messages=user_prompt, system=system_message)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "842ba6bb-c389-42ef-a013-c6a1c8fb6d10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The general elections held in Pakistan on February 08, 2024, were largely considered fair and transparent by international observers. The electoral process was conducted in a peaceful and orderly manner, with minimal incidents of violence or irregularities reported. The results were accepted by the major political parties, reflecting the will of the Pakistani people.\n"
     ]
    }
   ],
   "source": [
    "system_message = \"You are an expert of political science and history and have a deep understanding of policical situation of Pakistan.\"\n",
    "user_prompt = \"Write down a 50 words summary about the fairness of general elections held in Pakistan on February 08, 2024.\"\n",
    "\n",
    "response = ask_claude(messages=user_prompt, system=system_message)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9b2aed-3df5-4c3a-a7cb-d458a380836d",
   "metadata": {},
   "source": [
    "## f. Examples (Code Generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b473f30c-858e-453d-ad9e-e0ba2f22bd70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a C program that generates the first ten numbers of the Fibonacci sequence:\n",
      "\n",
      "```c\n",
      "#include <stdio.h>\n",
      "\n",
      "int main() {\n",
      "    int a = 0, b = 1, c, i;\n",
      "\n",
      "    printf(\"The first 10 Fibonacci numbers are:\\n\");\n",
      "\n",
      "    for (i = 0; i < 10; i++) {\n",
      "        printf(\"%d \", a);\n",
      "        c = a + b;\n",
      "        a = b;\n",
      "        b = c;\n",
      "    }\n",
      "\n",
      "    printf(\"\\n\");\n",
      "    return 0;\n",
      "}\n",
      "```\n",
      "\n",
      "Here's how the program works:\n",
      "\n",
      "1. We initialize three integer variables: `a` to 0, `b` to 1, and `c` as a temporary variable.\n",
      "2. We print the first 10 Fibonacci numbers using a `for` loop.\n",
      "3. Inside the loop, we print the current value of `a`, which is the current Fibonacci number.\n",
      "4. We then calculate the next Fibonacci number by adding `a` and `b`, and storing the result in `c`.\n",
      "5. We update `a` to the previous value of `b`, and `b` to the current value of `c`.\n",
      "6. The loop continues until 10 Fibonacci numbers have been printed.\n",
      "7. Finally, we return 0 to indicate successful program execution.\n",
      "\n",
      "When you run this program, the output will be:\n",
      "\n",
      "```\n",
      "The first 10 Fibonacci numbers are:\n",
      "0 1 1 2 3 5 8 13 21 34\n",
      "```\n",
      "\n",
      "This program demonstrates the basic logic of generating the Fibonacci sequence in C. You can easily modify the program to generate more or fewer Fibonacci numbers as needed."
     ]
    }
   ],
   "source": [
    "system_message = \"You are an expert of C programing in C language.\"\n",
    "user_prompt = \"Write down a C program that generates first ten numbers of fibonacci sequence.\"\n",
    "\n",
    "response = ask_claude(messages=user_prompt, system=system_message, stream=True)\n",
    "\n",
    "for event in response:\n",
    "        if event.type == \"content_block_delta\":\n",
    "            print(event.delta.text, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a38a469-d762-410a-a034-7a1e96f572b0",
   "metadata": {},
   "source": [
    "## g. Examples (Text Translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9d36c7bd-a80f-4a2e-9347-199a7d7c94b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "اس سال کا بجٹ کم تنخواہ والے لوگوں پر بہت برا اثر ڈالے گا۔\n"
     ]
    }
   ],
   "source": [
    "system_message = \"Please act as an expert of English to Urdu translator by translating the prompt from English into Urdu.\"\n",
    "user_prompt = \"The budget this year will have a very bad impact on the low salried people\"\n",
    "response = ask_claude(messages=user_prompt, system=system_message)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7329c8c7-ba4e-4b36-9324-5a63d0d6a124",
   "metadata": {},
   "source": [
    "## h. Examples (Text Summarization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a5d50836-c185-4baf-8358-a753db4607a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Hugging Face transformers library is a versatile NLP tool for tasks like text classification, entity recognition, and question answering.\n"
     ]
    }
   ],
   "source": [
    "system_message = \"You are an expert of English language.\"\n",
    "\n",
    "user_prompt = f'''\n",
    "Summarize the text below in at most 20 words:\n",
    "```The Hugging Face transformers library is an incredibly versatile and powerful tool for natural language processing (NLP).\n",
    "It allows users to perform a wide range of tasks such as text classification, named entity recognition, and question answering, among others.\n",
    "It's an extremely popular library that's widely used by the open-source data science community.\n",
    "It lowers the barrier to entry into the field by providing Data Scientists with a productive, convenient way to work with transformer models.```\n",
    "'''\n",
    "\n",
    "response = ask_claude(messages=user_prompt, system=system_message)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79f28fb-3cd6-443c-b922-352ba646d134",
   "metadata": {},
   "source": [
    "## i. Examples (Named Entity Recognition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "47d90fb7-d0ef-4f52-a012-bff49a91ecfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the entities extracted from the given text:\n",
      "\n",
      "Entity: Zelaid Mujahid | Type: name\n",
      "Entity: University of the Punjab | Type: university\n",
      "Entity: Pakistani | Type: nationality\n",
      "Entity: 3.5 | Type: grades\n",
      "Entity: AI Club | Type: club\n"
     ]
    }
   ],
   "source": [
    "system_message = \"\"\"You are a  Named Entity Recognition specialist. Extract and classify entities from the given text into these categories only if they exist:\n",
    "- name\n",
    "- major\n",
    "- university\n",
    "- nationality\n",
    "- grades\n",
    "- club\n",
    "Format your response as: 'Entity: [text] | Type: [category]' with each entity on a new line.\"\"\"\n",
    "\n",
    "user_prompt = '''\n",
    "Zelaid Mujahid is a sophomore majoring in Data Science at University of the Punjab. \\\n",
    "He is Pakistani national and has a 3.5 GPA. Mujahid is an active member of the department's AI Club.\\\n",
    "He hopes to pursue a career in AI after graduating.\n",
    "'''\n",
    "response = ask_claude(messages=user_prompt, system=system_message)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0803f6b5-37b3-455c-9eb6-dd14b244d9ef",
   "metadata": {},
   "source": [
    "## j. Example (Grade School Math 8K (GSM8K))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1c3e7d69-6472-4c43-be6b-69a187f9929c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, let's break this down step-by-step:\n",
      "* The booth made $50 per day selling popcorn\n",
      "* It made 3 times as much selling cotton candy, so that's $150 per day\n",
      "* For 5 days, the total earnings would be:\n",
      "   - Popcorn: $50 x 5 days = $250\n",
      "   - Cotton Candy: $150 x 5 days = $750\n",
      "   - Total Earnings: $250 + $750 = $1,000\n",
      "* The booth had to pay $30 rent and $75 for ingredients\n",
      "* So the total expenses were $30 + $75 = $105\n",
      "* Therefore, the booth's earnings after paying rent and ingredients for the 5 days would be:\n",
      "   - Total Earnings: $1,000\n",
      "   - Total Expenses: $105\n",
      "   - Net Earnings: $1,000 - $105 = $895\n",
      "\n",
      "Therefore, the booth earned $895 for the 5-day activity after paying the rent and the cost of ingredients.\n"
     ]
    }
   ],
   "source": [
    "# Answer is \"The booth earned **$895** after paying the rent and the cost of ingredients.\"\n",
    "system_message = \"\"\"You are an expert School math teacher. \n",
    "Consider the following text and then answer the questions of the students from this:\n",
    "A carnival snack booth made $50 selling popcorn each day. It made three times as much selling cotton candy. \n",
    "For a 5-day activity, the booth has to pay $30 rent and $75 for the cost of the ingredients. \n",
    "\"\"\"\n",
    "user_prompt = \"How much did the booth earn for 5 days after paying the rent and the cost of ingredients?\"\n",
    "\n",
    "response = ask_claude(messages=user_prompt, system=system_message)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "51bfd5bb-3eaa-4d17-bd7b-f74641e675fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'll solve this step by step to find the booth's earnings after expenses.\n",
      "\n",
      "**Step 1: Calculate daily earnings**\n",
      "- Popcorn sales per day: $50\n",
      "- Cotton candy sales per day: 3 × $50 = $150\n",
      "- Total daily earnings: $50 + $150 = $200\n",
      "\n",
      "**Step 2: Calculate total earnings for 5 days**\n",
      "- Total earnings: $200 × 5 days = $1,000\n",
      "\n",
      "**Step 3: Calculate total expenses**\n",
      "- Rent: $30\n",
      "- Cost of ingredients: $75\n",
      "- Total expenses: $30 + $75 = $105\n",
      "\n",
      "**Step 4: Calculate net earnings after expenses**\n",
      "- Net earnings = Total earnings - Total expenses\n",
      "- Net earnings = $1,000 - $105 = $895\n",
      "\n",
      "Therefore, the booth earned **$895** for 5 days after paying the rent and the cost of ingredients.\n"
     ]
    }
   ],
   "source": [
    "# Answer is \"The booth earned **$895** after paying the rent and the cost of ingredients.\"\n",
    "system_message = \"\"\"You are an expert School math teacher. \n",
    "Consider the following text and then answer the questions of the students from this:\n",
    "A carnival snack booth made $50 selling popcorn each day. It made three times as much selling cotton candy. \n",
    "For a 5-day activity, the booth has to pay $30 rent and $75 for the cost of the ingredients. \n",
    "\"\"\"\n",
    "user_prompt = \"How much did the booth earn for 5 days after paying the rent and the cost of ingredients?\"\n",
    "\n",
    "response = ask_claude(messages=user_prompt, system=system_message, model=\"claude-sonnet-4-0\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060bc091-a2d4-4602-ac89-840efe082be8",
   "metadata": {},
   "source": [
    "## k. Example (Reasoning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2a4eb7f8-9f4a-4d9d-940a-b1d3668eec3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To solve this problem, we need to find the time when the two trains meet.\n",
      "\n",
      "Given information:\n",
      "- Train 1 leaves Station A at 2:00 PM, traveling at 60 mph.\n",
      "- Train 2 leaves Station B at 2:30 PM, traveling at 80 mph towards Station A.\n",
      "- The distance between the two stations is 200 miles.\n",
      "\n",
      "Step 1: Calculate the distance traveled by each train until they meet.\n",
      "Let's say the trains meet at time t.\n",
      "\n",
      "Train 1 travels for t hours, so the distance traveled by Train 1 is 60t miles.\n",
      "Train 2 travels for (t - 0.5) hours, as it leaves 30 minutes later, so the distance traveled by Train 2 is 80(t - 0.5) miles.\n",
      "\n",
      "The total distance traveled by both trains is equal to the distance between the two stations, which is 200 miles.\n",
      "\n",
      "Therefore, we can write the equation:\n",
      "60t + 80(t - 0.5) = 200\n",
      "\n",
      "Solving the equation:\n",
      "60t + 80t - 40 = 200\n",
      "140t = 240\n",
      "t = 1.714 hours\n",
      "\n",
      "Step 2: Convert the meeting time to the actual time.\n",
      "The trains meet at 1.714 hours after 2:00 PM, which is 3:43 PM.\n",
      "\n",
      "Therefore, the two trains will meet at 3:43 PM.\n"
     ]
    }
   ],
   "source": [
    "# Correct answer is 3:42:51 PM.\n",
    "system_message=\"You are an expert in answering logical reasoning questions\"\n",
    "user_prompt = \"\"\"\n",
    "If a train leaves Station A at 2:00 PM traveling at 60 mph, \n",
    "and another train leaves Station B at 2:30 PM traveling at 80 mph \n",
    "toward Station A, and the stations are 200 miles apart, when will they meet?\n",
    "\"\"\"\n",
    "\n",
    "response = ask_claude(messages=user_prompt, system=system_message)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1753fb4c-d2ed-498c-9b26-cbedc5b4606d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I need to find when two trains traveling toward each other will meet.\n",
      "\n",
      "Let me set up the problem:\n",
      "- Train A leaves at 2:00 PM at 60 mph\n",
      "- Train B leaves at 2:30 PM at 80 mph  \n",
      "- Distance between stations: 200 miles\n",
      "- They're traveling toward each other\n",
      "\n",
      "Let me define t as the time (in hours) after 2:00 PM when they meet.\n",
      "\n",
      "At time t:\n",
      "- Train A has been traveling for t hours\n",
      "- Train B has been traveling for (t - 0.5) hours (since it left 30 minutes later)\n",
      "\n",
      "Distance covered by Train A: 60t miles\n",
      "Distance covered by Train B: 80(t - 0.5) miles\n",
      "\n",
      "Since they're traveling toward each other, the sum of distances they've traveled equals the total distance between stations:\n",
      "\n",
      "60t + 80(t - 0.5) = 200\n",
      "\n",
      "Solving:\n",
      "60t + 80t - 40 = 200\n",
      "140t - 40 = 200\n",
      "140t = 240\n",
      "t = 240/140 = 12/7 ≈ 1.714 hours\n",
      "\n",
      "Converting to hours and minutes:\n",
      "1.714 hours = 1 hour and 42.86 minutes ≈ 1 hour and 43 minutes\n",
      "\n",
      "Therefore, they will meet at 2:00 PM + 1 hour 43 minutes = **3:43 PM**\n",
      "\n",
      "To verify: \n",
      "- Train A travels for 1.714 hours at 60 mph = 102.86 miles\n",
      "- Train B travels for 1.214 hours at 80 mph = 97.14 miles\n",
      "- Total: 102.86 + 97.14 = 200 miles ✓\n"
     ]
    }
   ],
   "source": [
    "# Correct answer is 3:42:51 PM.\n",
    "system_message=\"You are an expert in answering logical reasoning questions\"\n",
    "user_prompt = \"\"\"\n",
    "If a train leaves Station A at 2:00 PM traveling at 60 mph, \n",
    "and another train leaves Station B at 2:30 PM traveling at 80 mph \n",
    "toward Station A, and the stations are 200 miles apart, when will they meet?\n",
    "\"\"\"\n",
    "thinking={\"type\": \"enabled\", \"budget_tokens\": 5000}\n",
    "response = ask_claude(messages=user_prompt, system=system_message, model=\"claude-sonnet-4-0\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937c3b3d-51da-4c64-8edf-63d12847e9e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172b7b85-ef75-4a36-9bce-2ef79c230cae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (genai-uv)",
   "language": "python",
   "name": "genai-uv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
