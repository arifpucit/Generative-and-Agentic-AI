{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2ac0634-3688-482b-a9f2-bedec303c39b",
   "metadata": {},
   "source": [
    "---   \n",
    " <img align=\"left\" width=\"75\" height=\"75\"  src=\"https://upload.wikimedia.org/wikipedia/en/c/c8/University_of_the_Punjab_logo.png\"> \n",
    "\n",
    "<h1 align=\"center\">Department of Data Science</h1>\n",
    "\n",
    "---\n",
    "<h3><div align=\"right\">Instructor: Muhammad Arif Butt, Ph.D.</div></h3>    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6fe310",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "<h1 align=\"center\">Lec-08: Accessing Open-Source AI Models using Local Run-time Frameworks</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef15071",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Learning agenda of this notebook\n",
    "\n",
    "1. Local LLM Runtime Frameworks\n",
    "2. Accessing AI Models via Ollama GUI and CLI\n",
    "3. Accessing AI Models via LLaMA.cpp GUI and CLI\n",
    "4. Programmatically Accessing AI Models running on Local Machine by Ollama\n",
    "    - Using  Ollama’s Native Python Library\n",
    "    - Using  OpenAI's `Chat Completion` API\n",
    "    - Using  OpenAI's `Responses` API\n",
    "5. Hello World Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf953ee3-956b-4397-a596-cc8113380477",
   "metadata": {},
   "source": [
    "# <span style='background :lightgreen' >Recap: Ways to Access Open Source LLMs</span>\n",
    "### (i) Access Open-Source Models via Cloud-Based Providers (Driving a fully automatic car — everything managed for you)\n",
    "\n",
    "### (ii) Run Open-Source Models locally using runtimes (Driving an automatic car — local but simple, no gears or engineering)\n",
    "\n",
    "### (iii) Use Open-Source Models via Hugging Face `pipeline()` API (Driving a manual car — you see more of the mechanics, but still a car someone else built)\n",
    "\n",
    "### (iv) Load and run models directly from Hugging Face Hub using `AutoModel/AutoTokenizer` (Opening the hood and adjusting or replacing engine components)\n",
    "\n",
    "\n",
    "### (v) Fine-Tune LLMs using full fine-tuning or PEFT methods (LoRA / QLoRA / adapters) (Upgrading and re-calibrating the engine to suit your driving style)\n",
    "\n",
    "### (vi) Build and train an AI Model from scratch using PyTorch / TensorFlow (Designing and building the entire car from raw parts — full control, full responsibility)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85518a7-adc5-42ca-b1be-e2fc07c117f0",
   "metadata": {},
   "source": [
    "# <span style='background :lightgreen' >1. Local LLM Runtime Frameworks</span>\n",
    "\n",
    "\n",
    "## Why is Local Inference Significant?\n",
    "1. **Privacy & Security:** Data stays within your own system, so sensitive information is not sent to third-party cloud servers. This is especially important in healthcare, finance, and government sectors. Keeping data local lowers exposure to external threats and minimizes the risk of data breaches.\n",
    "3. **Cost Efficiency:** Reduces cloud-related expenses such as API usage fees, bandwidth, and compute costs.\n",
    "4. **Low Latency:** Running models locally reduces network delay, resulting in faster responses. This is critical for real-time applications like chatbots and interactive systems.\n",
    "5. **Offline Operation:** Models continue to work without an internet connection. Useful in environments with limited or unreliable connectivity (e.g., remote areas, edge devices).\n",
    "6. **Customization & Control:** You have full control over the model, data, updates, and fine-tuning without platform restrictions.\n",
    "7. **Regulatory Compliance:** Helps organizations follow data privacy laws by keeping sensitive data inside their own systems. This supports compliance with regulations such as GDPR (European Union data protection law) and CCPA (California’s data privacy law), which require careful handling and protection of personal data.\n",
    "\n",
    "## Common Local LLM Runtime Frameworks\n",
    "- **LM Studio (https://lmstudio.ai/):**\n",
    "    - A GUI-based desktop application (Windows/macOS/Linux) designed primarily for end users.\n",
    "    - Enables users to download, manage, and chat with local LLMs without any coding.\n",
    "    - Focused on simplicity and accessibility rather than developer-oriented integration.\n",
    "    - Best suited for non-technical users or quick local experimentation.\n",
    "- **GPT4All ( https://gpt4all.io/):**\n",
    "    - It combines a beginner-friendly desktop chat application ((Windows/macOS/Linux)) with developer SDKs (Python, Node.js, C++), enabling both offline local chatting and programmatic integration of open-source models.\n",
    "    - Provides a beginner-friendly desktop chat application (Windows/macOS/Linux) for offline local inference.\n",
    "    - Also offers developer SDKs (Python, Node.js, C++) for programmatic integration.\n",
    "    - Supports running open-source models locally with optional API-based interaction.\n",
    "    - Suitable for both casual users and developers who need lightweight integration capabilities.\n",
    "- **Ollama (https://ollama.com/):**\n",
    "    - User-friendly runtime built on top of llama.cpp\n",
    "    - Provides automatic model download, versioning, and management\n",
    "    - Supports local inference with a CLI, browser-based interface, and OpenAI-compatible REST API for easy integration\n",
    "    - Very simple to set up and use, making it ideal for quick prototyping\n",
    "    - Slightly slower than raw llama.cpp due to wrapper overhead and Not designed for heavy production workloads\n",
    "    - Best suited for:\n",
    "        - Developers who want quick local deployment with minimal configuration.\n",
    "        - Prototyping applications that require an OpenAI-style API locally.\n",
    "- **llama.cpp (https://github.com/ggml-org/llama.cpp):**\n",
    "    - Lightweight C/C++ implementation for efficient inference of LLaMA and other GGUF-based models\n",
    "    - Supports quantization for CPU and limited GPU acceleration\n",
    "    - Highly optimized for resource-constrained environments, including edge devices and Apple Silicon\n",
    "    - Provides fine-grained control over model execution and performance tuning\n",
    "    - Includes a built-in lightweight HTTP server and can be paired with browser-based GUIs for interactive usage\n",
    "    - Does not include advanced serving features such as multi-user scheduling, dynamic batching, or production-grade APIs.\n",
    "    - Best suited for:\n",
    "        - CPU-based inference and edge deployments\n",
    "        - Researchers or developers needing low-level control and aggressive quantization\n",
    "        - Embedded systems and resource-constrained environments.\n",
    "- **vLLM (https://github.com/vllm-project/vllm):**\n",
    "    - [Efficient Memory Management for Large Language Model Serving with PagedAttention](https://arxiv.org/pdf/2309.06180)\n",
    "    - High-performance LLM inference and serving engine, originally developed at UC Berkeley\n",
    "    - Designed primarily for GPU-based deployment and large-scale serving\n",
    "    - Implements advanced optimizations like PagedAttention and continuous batching for high throughput and efficient memory usage\n",
    "    - Supports OpenAI-compatible APIs, making it suitable for production integration\n",
    "    - Optimized for multi-user, high-concurrency workloads in server environments.\n",
    "    - Best suited for:\n",
    "        - Production-grade deployments.\n",
    "        - High-throughput APIs and enterprise-scale applications.\n",
    "        - Multi-GPU inference infrastructure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ffe0d9-9b89-4a1b-8f92-f6d825bd91c5",
   "metadata": {},
   "source": [
    "# <span style='background :lightgreen' >2. Accessing AI Models via Ollama GUI and CLI</span>\n",
    "\n",
    "<h3 align=\"center\"><div class=\"alert alert-success\" style=\"margin: 20px\">Ollama is an open-source platform that makes it easier to work with open-source AI models, providing features like model downloading, running it on your local box, and interacting through CLI or via APIs</h3>\n",
    "\n",
    "\n",
    "## a. Architecture of Ollama:\n",
    "- **Server Component:**\n",
    "    - When you install and run Ollama, it launches a local server that listens on http://localhost:11434\n",
    "    - This server is responsible for managing models (loading, unloading, switching) and handling inference requests.\n",
    "    - The API it exposes follows a REST architecture, making it accessible through HTTP requests.\n",
    "    - So whatever LLM you choose to run using  ollama, your request at the end of the day will be passed to the ollama server, which runs on your localhost at port 11434.\n",
    "- **Client Interaction:**\n",
    "    - Applications can interact with the Ollama server in three  ways:\n",
    "        - Using the official Ollama CLI, which directly communicate with the local server. \n",
    "        - Using Ollama-native client like `ollama.chat()` in Python or JavaScript.\n",
    "        - Using the OpenAI client library and using `client.chat.completions.create()` or `client.responses.create()`, since Ollama exposes endpoint (http://localhost:11434/v1) that mimic the OpenAI API format.\n",
    "    - Clients can issue requests for text generation, chat-style completions, streaming responses, and model management.\n",
    "    - The Ollama server then handles the request, executes inference on your local hardware, and returns the response to the client.\n",
    "- **Networked Environment:**\n",
    "    - Although Ollama defaults to localhost, the server can be run on a different machine in the same network.\n",
    "    - In that setup, clients simply target the remote machine’s IP and port 11434.\n",
    "    - This allows you to centralize inference on a powerful machine (GPU server) while keeping clients lightweight.\n",
    "    - So Ollama can run on a private-IP machine inside a LAN and be accessed locally via CLI/GUI or remotely via HTTP using Python clients (including the OpenAI-compatible API), and it can also be deployed on a public-IP machine for internet access, but doing so safely requires a reverse proxy, authentication, and TLS since Ollama itself provides no built-in security.\n",
    "\n",
    "\n",
    "\n",
    "## b. Supported Models:\n",
    "\n",
    "| Model Type                   | Capabilities                             | Examples                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |\n",
    "| ---------------------------- | ---------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| **Text-only LLMs**           | Chat, reasoning, summarization, code     | **[llama4](https://ollama.com/library/llama4)** — next-gen chat/coding models (Behemoth, Maverick, Scout)<br>**[llama3.3:70b](https://ollama.com/library/llama3.3)** — reasoning-focused text model<br>**[llama3.1](https://ollama.com/library/llama3.1)** — general-purpose LLM family (8b/70b/405b)<br>**[llama3.2](https://ollama.com/library/llama3.2)** — ultra-efficient mini LLMs (1b/3b)<br>**[gemma2](https://ollama.com/library/gemma2)** — Google-optimized compact chat models (2b/9b/27b)<br>**[qwen2.5](https://ollama.com/library/qwen2.5)** — multilingual, high-performance LLMs (0.5b-72b)<br>**[mistral:7b](https://ollama.com/library/mistral)** — efficient French-backed reasoning model<br>**[phi4:14b](https://ollama.com/library/phi4)** — Microsoft reasoning-optimized compact model |\n",
    "| **Reasoning Models**         | Advanced step-by-step reasoning, math    | **[deepseek-r1](https://ollama.com/library/deepseek-r1)** — chain-of-thought reasoning (1.5b-671b)<br>**[openthinker](https://ollama.com/library/openthinker)** — open long-reasoning (7b/32b)<br>**[magistral:24b](https://ollama.com/library/magistral)** — synthetic reasoning from Mistral<br>**[phi4-reasoning:14b](https://ollama.com/library/phi4-reasoning)** — deliberate reasoning variant<br>**[qwq:32b](https://ollama.com/library/qwq)** — Qwen 32b benchmark-level reasoner                                                                                                                                                                                                                                                                                          |\n",
    "| **Code-Specific**            | Programming, code completion, debugging  | **[qwen2.5-coder](https://ollama.com/library/qwen2.5-coder)** — multilingual code LLM (0.5b-32b)<br>**[deepseek-coder-v2](https://ollama.com/library/deepseek-coder-v2)** — strongest open-source coding model (16b/236b)<br>**[codestral:22b](https://ollama.com/library/codestral)** — Mistral coding assistant<br>**[starcoder2](https://ollama.com/library/starcoder2)** — permissive code generator (3b/7b/15b)<br>**[granite-code](https://ollama.com/library/granite-code)** — IBM enterprise code LLM (3b-34b)                                                                                                                                                                                                                                                             |\n",
    "| **Multimodal (Vision)**      | Image understanding + text generation    | **[llama4-vision](https://ollama.com/library/llama4-vision)** — image + text multimodal<br>**[llama3.2-vision](https://ollama.com/library/llama3.2-vision)** — mini multimodal family (11b/90b)<br>**[qwen2.5vl](https://ollama.com/library/qwen2.5vl)** — visual-language model (3b/7b/32b/72b)<br>**[llava](https://ollama.com/library/llava)** — LLaMA-based visual chatbot (7b/13b/34b)<br>**[minicpm-v:8b](https://ollama.com/library/minicpm-v)** — small, fast VLM                                                                                                                                                                                                                                                                                                          |\n",
    "| **Small/Efficient**          | Lightweight for edge devices             | **[smollm2](https://ollama.com/library/smollm2)** — ultra-tiny edge LLMs (135m/360m/1.7b)<br>**[tinyllama:1.1b](https://ollama.com/library/tinyllama)** — extremely small LLM baseline<br>**[phi3:3.8b](https://ollama.com/library/phi3)** — Microsoft tiny reasoning model<br>**[nemotron-mini:4b](https://ollama.com/library/nemotron-mini)** — NVIDIA compact assistant model<br>**[gemma2:2b](https://ollama.com/library/gemma2)** — tiny Google chat model                                                                                                                                                                                                                                                                                                                    |\n",
    "| **Tool-calling/Agents**      | External API interaction, function calls | **[llama3.1-tools](https://ollama.com/library/llama3.1-tools)** — tool-enabled LLaMA<br>**[qwen2.5-tools](https://ollama.com/library/qwen2.5-tools)** — function-calling Qwen family<br>**[mistral-nemo:12b](https://ollama.com/library/mistral-nemo)** — Mistral API-aware model<br>**[command-r+](https://ollama.com/library/command-r%2B)** — RAG/agent orchestration model (35b/104b)<br>**[hermes3](https://ollama.com/library/hermes3)** — tool-driven fine-tune<br>**[granite3-tools](https://ollama.com/library/granite3-tools)** — IBM agent foundation model                                                                                                                                                                                                             |\n",
    "| **Embedding Models**         | Semantic vector generation (RAG)         | **[mxbai-embed-large](https://ollama.com/library/mxbai-embed-large)** — high-accuracy embedding (335m)<br>**[nomic-embed-text](https://ollama.com/library/nomic-embed-text)** — optimized text embeddings<br>**[bge-m3](https://ollama.com/library/bge-m3)** — multilingual embedding (567m)<br>**[snowflake-arctic-embed2](https://ollama.com/library/snowflake-arctic-embed2)** — enterprise embedding (568m)<br>**[all-minilm](https://ollama.com/library/all-minilm)** — lightweight MiniLM (22m/33m)                                                                                                                                                                                                                                                                          |\n",
    "| **MoE (Mixture of Experts)** | High performance, efficient inference    | **[deepseek-r1-moe](https://ollama.com/library/deepseek-r1)** — MoE reasoning flagship (671b)<br>**[deepseek-v3](https://ollama.com/library/deepseek-v3)** — frontier MoE model (671b)<br>**[mixtral](https://ollama.com/library/mixtral)** — sparse expert transformer (8×7B / 8×22B)<br>**[qwen2.5-moe](https://ollama.com/library/qwen2.5)** — MoE Qwen variants                                                                                                                                                                                                                                                                                                                                                                                                                |\n",
    "| **Specialized Domain**       | Math, medical, multilingual              | **[qwen2-math](https://ollama.com/library/qwen2-math)** — math-specialized LLM (1.5b/7b/72b)<br>**[medllama2:7b](https://ollama.com/library/medllama2)** — biomedical LLaMA<br>**[aya-expanse](https://ollama.com/library/aya-expanse)** — multilingual research model (8b/32b)<br>**[mathstral:7b](https://ollama.com/library/mathstral)** — math-finetuned Mistral<br>**[sailor2](https://ollama.com/library/sailor2)** — multilingual navigation models (1b/8b/20b)                                                                                                                                                                                                                                                                                                            \n",
    "\n",
    "## c. Download and Install Ollama\n",
    "- There are different ways to download and install **ollama**:\n",
    "    1. Download Ollama for your Mac, Linux or Windows machine by visiting: https://ollama.com/download\n",
    "    2. On your terminal (inside your virtual environment), give the command `uva add ollama` or  `pip install ollama`\n",
    "    3. Download docker image of ollama available on dockerhub and run it inside a container\n",
    "- To check if ollama has been installed on your machine, open a terminal and give the command `ollama`:\n",
    "```\n",
    "Available Commands:\n",
    "  serve       Start ollama\n",
    "  create      Create a model\n",
    "  show        Show information for a model\n",
    "  run         Run a model\n",
    "  stop        Stop a running model\n",
    "  pull        Pull a model from a registry\n",
    "  push        Push a model to a registry\n",
    "  signin      Sign in to ollama.com\n",
    "  signout     Sign out from ollama.com\n",
    "  list        List models\n",
    "  ps          List running models\n",
    "  cp          Copy a model\n",
    "  rm          Remove a model\n",
    "  launch      Launch an integration with Ollama\n",
    "  help        Help about any command\n",
    "Use \"ollama [command] --help\" for more information about a specific command.\n",
    "```\n",
    "- To ensure Ollama application is running in the background, just open a browser and go to http://localhost:11434/ and it will display a message saying **\"ollama is running\"**. If this is not the case, then on the terminal give the command `ollama serve` and try again :)\n",
    "\n",
    "<h1 align=\"center\"><div class=\"alert alert-success\" color=magenta style=\"margin: 20px\">Ollama lets you download models from Ollama Hub or the Hugging Face Hub.</h1>\n",
    "\n",
    "## d. Download Models from Ollama Hub Using the **Ollama GUI**\n",
    "- Open the ollama UI, from where you can search and download models from the official remote repository (https://ollama.com/search). Once downloaded the models are stored locally, making it available for future use without the requirement of Internet.\n",
    "- The models hosted in ollama's official library are GGUF quantized versions. GGUF stands for “GPT-Generated Unified Format\", which is a binary file format used to store large language model weights and metadata in a way that’s efficient for local inference.\n",
    "- Many new models are too large to fit on widely available GPUs, therefore, you can use **Ollama's cloud**, which is a new way to run open models using datacenter-grade hardware. It however includes hourly and daily limits to ensure fair usage.\n",
    "- Ollama's cloud models require an account on ollama.com and require being signed in to ollama.com, and you need to use the Ollama desktop application to sign in.\n",
    "    - Step 1: Open Ollama Desktop App, Click on the Ollama icon in your Mac menu bar (top right) Or open the Ollama application from your Applications folder\n",
    "    - Step 2: Click on \"Sign In\" in the menu. You'll be redirected to ollama.com to authenticate. Sign in or create an account. Authorize the application\n",
    "    - Step 3: Verify Authentication After signing in through the desktop app, try your command again:\n",
    "- **Available Models on the Cloud are:**\n",
    "    - `gpt-oss:20b-cloud`\n",
    "    - `gpt-oss:120b-cloud`\n",
    "    - `qwen3-coder:480b-cloud`\n",
    "    - `deepseek-v3.1:671b-cloud`\n",
    "\n",
    ">- If you are using Ollama Cloud models, then Ollama UI gives you facility to enable/disable web search tool and thinking facility."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05178ed9-6eb0-4368-a029-e8482697ee3d",
   "metadata": {},
   "source": [
    "## e. Download Models from Ollama Hub Using  **Ollama CLI**\n",
    "- Open Command prompt/Powershell on Windows or a Terminal on Mac and use the following commands:\n",
    "\n",
    "```bash\n",
    "$ ollama                                     # List the possible commands\n",
    "$ ollama pull llama3.2:1b                    # Download the model with the specific Model-ID from a remote repository (https://ollama.com/library) and stores it locally (/Users/arif/.ollama/models/), making it available for future use with ollama run.\n",
    "$ ollama list                                # list all the models you have downloaded on your machine\n",
    "$ ollama show gpt-oss:20b-cloud              # Display the model architecture, parameters, context window size, embedding length, quantization , special tokens, capabilites and  license\n",
    "$ ollama run  llama3.2                       # Interactive session, from which finally you can exit using /exit or /bye or /quitList\n",
    "$ ollama run  llama3.2 [--verbiose] \"Query\"  # Give a single line prompt to get the response. The --verbose option display additional information about the tokens and evaluation duration\n",
    "$ ollama            # List\n",
    "$ ollama run llama3.2 \"Can you extract names the Cricket players from the following text\" < ../data/names.txt           # Give the contents of a file using the I/O redirection operator and ask questions from the passed text\n",
    "$ cat ../data/names.txt | ollama run llama3.2 \"Can you extract names the Cricket players from the following text\"       # Give the contents of a file using the pipe operator and ask questions from the passed text\n",
    "$ curl https://www.arifbutt.me/ | ollama run llama3.2:latest \"Give me a single line description about Dr. Arif Butt from the given HTML text\" # Scrape a web page and send the html contents and ask Q from it\n",
    "\n",
    "# Ollama exposes multiple REST API endpoints, /api/generate is one option used to generate text (it is stateless and donot keep track of past messages in the request body)\n",
    "$ curl -X POST http://localhost:11434/api/generate -d '{ \"model\": \"llama3.2\", \"prompt\":\"What is the capital of Pakistan?\", \"stream\":true}'   \n",
    "# The endpoint /api/chat is used for Multi-turn conversational chat as it keeps track of past messages in the request body. Supports system, user, assistant roles.\n",
    "$ curl -X POST http://localhost:11434/api/chat -d '{\"model\": \"llama3.2\",\"messages\": [{\"role\": \"user\", \"content\": \"Hi, my name is Arif.\"}], \"Stream\":false}' \n",
    "# Send the full chat history in the next conversation\n",
    "$ curl -X POST http://localhost:11434/api/chat -d '{\"model\": \"llama3.2\",\"messages\": [{\"role\": \"user\", \"content\": \"Hi, my name is Arif.\"},{\"role\": \"assistant\", \"content\": \"Nice to meet you, Arif!\"},{\"role\": \"user\", \"content\": \"What is my name?\"}], \"stream\":false}'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280d5030-ce87-4b63-b882-8b0349de3f88",
   "metadata": {},
   "source": [
    "<img align=\"right\" width=\"800\"  src=\"../images/gguf_1.jpg\"  >\n",
    "\n",
    "## f. Download Models from Hugging Face Hub\n",
    "### Step 1: Find a GGUF model on Hugging Face\n",
    "- Ollama only runs models in the GGUF format because it is built on llama.cpp which reads GGUF files for efficient inference\n",
    "- Visit the models tab of hugging face, https://huggingface.co/models and search for \"gguf\" and you will see different models by unsloth, Qwen, Phi, Mistral and  TheBloke\n",
    "- **GPT-Generated Unified Format (GGUF)** is a modern, unified model file format designed for efficient local inference, especially for *LLaMA-family* of models using tools like **llama.cpp** and **Ollama**.\n",
    "    - *GGUF File Container:* The outer wrapper that holds all model components in a single binary file, eliminating the ned of multi-file setups and ensures consistency between model components.\n",
    "    - *Metadata Layer:* This layer stores model configuration and descriptive information, including model architecture and dimensions, context length, special tokens (BOS, EOS, PAD) and inference-related parameters.\n",
    "    - *Tokenization Layer:* This layer defines how raw text is converted into tokens.\n",
    "    - *Quantization Layer:* This layer describes how weights are compressed using different quantization formats like Q4, and Q8. This enables running large models on limited hardware.\n",
    "    - *Model Weights and Tensors:* This layer contains the actual neural network parameters stored as optimized tensors.\n",
    "\n",
    "\n",
    "### Step 2: Download one of the GGUF model to your computer\n",
    "- Click on ‘Files and Versions’ on the model page: https://huggingface.co/unsloth/gemma-3-1b-it-GGUF/tree/main and select the version\n",
    "- A model’s raw size in memory is roughly: `Parameters × Bytes per parameter`. The bytes per parameter depends on quantization level:\n",
    "    - FP16 (16-bit float): 2 bytes/parameter\n",
    "    - INT8 quantization: 1 byte/parameter\n",
    "    - INT4 quantization: 0.5 bytes/parameter\n",
    "- So memory use scales with both parameter count and quantization method.\n",
    "    - 1.5B model → ~3–3.5 GB RAM (unquantized FP16).\n",
    "    - 7B model → ~14–16 GB RAM (unquantized FP16).\n",
    "    - With quantization, the requirement drops by 2×–4×, which is why Ollama and llama.cpp can run 7B models comfortably on machines with 8–16 GB RAM.\n",
    "\n",
    "- **Download via Browser**\n",
    "- Let us download the 8-bit quantized version from this link: https://huggingface.co/unsloth/gemma-3-1b-it-GGUF/blob/main/gemma-3-1b-it-Q8_0.gguf\n",
    "- Click the download button or clone the repository.\n",
    "- **Download Using Command Line**\n",
    "```bash\n",
    "# Windows\n",
    "huggingface-cli download TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF tinyllama-1.1b-chat-v1.0.Q8_0.gguf --local-dir C:\\models\\TinyLlama_safe\n",
    "\n",
    "# Linux/macOS\n",
    "huggingface-cli download TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF tinyllama-1.1b-chat-v1.0.Q8_0.gguf --local-dir ~/models/TinyLlama_safe\n",
    "```\n",
    "\n",
    "- I have downloaded models inside the `../models/gemma/` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39636bd2-365c-4d89-8b35-b1fc1f6d38cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 2088504\n",
      "-rw-r--r--@ 1 arif  staff   1.0G 19 Dec 22:03 gemma-3-1b-it-Q8_0.gguf\n",
      "-rw-r--r--@ 1 arif  staff   114B 19 Dec 22:32 Modelfile\n"
     ]
    }
   ],
   "source": [
    "!ls -lh ../models/gemma/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6699d6a3-6a1c-4d80-b909-7e1e37245631",
   "metadata": {},
   "source": [
    "### Step 3: Create a Modelfile\n",
    "- A **Modelfile** in Ollama is a small text configuration file that tells Ollama how to use a specific model.\n",
    "- Think of it as a bridge between the GGUF model file and Ollama’s chat interface, it defines which model to load and how the conversation should behave.\n",
    "- **Why is it needed?** Ollama itself cannot automatically know:\n",
    "  - Which GGUF model to load.\n",
    "  - How to handle prompts, messages, and system instructions.\n",
    "  - What generation parameters to use (temperature, top-p, etc.).\n",
    "- The Modelfile encapsulates all this information so you can run your model with consistent behavior.\n",
    "- It makes customizing models easier without modifying the GGUF file itself.\n",
    "- The only required section in the Modelfile is the `FROM` section, which points to your model file you just downloaded (where the model weights live)\n",
    "```\n",
    "FROM ../models/gemma/gemma-3-1b-it-Q8_0.gguf\n",
    "\n",
    "SYSTEM \"You are a helpful assistant that answers briefly.\"\n",
    "\n",
    "PARAMETER temperature 0.7\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3b8fdba-582d-4df8-940f-40a0aac830bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FROM gemma-3-1b-it-Q8_0.gguf\n",
      "SYSTEM \"You are a helpful assistant that answers briefly.\"\n",
      "PARAMETER temperature 0.7\n"
     ]
    }
   ],
   "source": [
    "!cat ../models/gemma/Modelfile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca2f226-792c-4640-aa98-88ffb48e22a4",
   "metadata": {},
   "source": [
    "### Step 4 — Build the model using ollama create\n",
    "- Use the `ollama create` command which will do the following tasks:\n",
    "    - Copy your local GGUF file into Ollama’s internal model store, verifying its SHA-256 hash.\n",
    "    - Parsed the GGUF to detect model architecture (Gemma-3-Instruct) and created new “layers” that include your system prompt and parameters.\n",
    "    - Wrote a new manifest and registered a new Ollama model name, so you can run it like any other built-in model.\n",
    "- Once done the `ollama list` command will display the new model that you can run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6bb20e71-aeba-43a0-a473-1c7e755e1a37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components ⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Ggathering model components \u001b[K\n",
      "copying file sha256:616dfb049ad14288d971d96f5ca4953fdebbf1e3cd407ad159f3bfd47090201d 100% \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1Ggathering model components \u001b[K\n",
      "copying file sha256:616dfb049ad14288d971d96f5ca4953fdebbf1e3cd407ad159f3bfd47090201d 100% \u001b[K\n",
      "parsing GGUF ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1Ggathering model components \u001b[K\n",
      "copying file sha256:616dfb049ad14288d971d96f5ca4953fdebbf1e3cd407ad159f3bfd47090201d 100% \u001b[K\n",
      "parsing GGUF \u001b[K\n",
      "using existing layer sha256:616dfb049ad14288d971d96f5ca4953fdebbf1e3cd407ad159f3bfd47090201d \u001b[K\n",
      "using autodetected template gemma3-instruct \u001b[K\n",
      "using existing layer sha256:611659bb976ab58f946ebc9c9e66020426bacc4b0ff52e15aab9e45b2cb50a69 \u001b[K\n",
      "using existing layer sha256:69dc5ab0f2b794b9cd8d996ba66cac95eea1c3e912335bacfd6a8022ecea54fd \u001b[K\n",
      "writing manifest \u001b[K\n",
      "success \u001b[K\u001b[?25h\u001b[?2026l\n"
     ]
    }
   ],
   "source": [
    "!ollama create mygemma -f ../models/gemma/Modelfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe206301-1a40-49b9-a8e3-acb26ab026b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                        ID              SIZE      MODIFIED     \n",
      "mygemma:latest              a631de1ab12d    1.1 GB    6 weeks ago     \n",
      "deepseek-v3.1:671b-cloud    d3749919e45f    -         7 weeks ago     \n",
      "gpt-oss:20b-cloud           875e8e3a629a    -         7 weeks ago     \n",
      "gpt-oss:120b-cloud          569662207105    -         7 weeks ago     \n",
      "qwen3-coder:480b-cloud      e30e45586389    -         7 weeks ago     \n",
      "nomic-embed-text:latest     0a109f422b47    274 MB    7 weeks ago     \n",
      "tinyllama:latest            2644915ede35    637 MB    7 weeks ago     \n",
      "deepseek-r1:1.5b            e0979632db5a    1.1 GB    7 months ago    \n",
      "llama3.2:1b                 baf6a787fdff    1.3 GB    8 months ago    \n",
      "llama3.2:latest             a80c4f17acd5    2.0 GB    8 months ago    \n"
     ]
    }
   ],
   "source": [
    "!ollama list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c7d9a2-69b0-475a-b5d2-76d6dfa4231c",
   "metadata": {},
   "source": [
    "### Step 5 - Run your new Model via CLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49bdd802-ab34-4614-b989-2eef574cee0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?2026h\u001b[?25l\u001b[1G⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?25l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[K\u001b[?25h\u001b[?2026l\u001b[2K\u001b[1G\u001b[?25hIslam\u001b[?25l\u001b[?25habad\u001b[?25l\u001b[?25h.\u001b[?25l\u001b[?25h\n",
      "\n",
      "\u001b[?25l\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!ollama run mygemma:latest \"What is the capital of Pakistan\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f64bed-2dbb-46ce-84b3-7f0217891140",
   "metadata": {},
   "source": [
    "# <span style='background :lightgreen' >3. Accessing AI Models via LLaMA.cpp (GUI and CLI)</span>\n",
    "\n",
    "## a. Download and Install Llama.cpp\n",
    "- There are different ways to download and install **llama.cpp** on your machine: https://github.com/ggerganov/llama.cpp\n",
    "    1. Install llama.cpp using `brew install llama.cpp` on Mac or (use `winget` for Windows or `nix` for Linux)\n",
    "    2. Download pre-built binaries from the releases page\n",
    "    3. Run with Docker - see our Docker documentation\n",
    "    4. Clone the repository, and build using `cmake`\n",
    "## b. Inference Using llama.cpp \n",
    "- You can manually download the raw GGUF file from Hugging Face and access them via command line interface of llama.cpp\n",
    "- llama.cpp does not automatically download models from Hugging Face unless you use the -hf flag.\n",
    "- Models must be in GGUF format, otherwise you may need conversion using the provided conversion utilities in the llama.cpp repo.\n",
    "- Below are the commands that you practice at your time (Replace `llama-cli` with `llama-cli.exe` for Windows):\n",
    "```bash\n",
    "$ llama-cli --help        # Shows general usage, options, and flags available for the CLI.\n",
    "$ llama-cli -hf ggml-org/gemma-3-1b-it-GGUF    # Automatically downloads the specified Hugging Face GGUF model and runs it locally.\n",
    "$ llama-cli -m /Users/arif/Documents/genai-course/models/gemma/gemma-3-1b-it-Q8_0.gguf # Run Interactive Session with a Local Model\n",
    "$ llama-cli -m /Users/arif/Documents/genai-course/models/gemma/gemma-3-1b-it-Q8_0.gguf -n 256 --temp 0.7 # Run With Additional Parameters\n",
    "```\n",
    "- You can start the llama-server and load the model. The embedded server often exposes a lightweight Web UI at `http://localhost:PORT` where you can interact with the model via browser.\n",
    "```bash\n",
    "$ llama-server -m /Users/arif/Documents/genai-course/models/gemma/gemma-3-1b-it-Q8_0.gguf  --port 8080 \n",
    "$ llama-server -m /Users/arif/Documents/genai-course/models/gemma/gemma-3-1b-it-Q8_0.gguf  -c 8192 -np 4 --port 8080  # -c specifies context window size. -np 4 specifies how many users or requests the server can handle simultaneously.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c826ca0-bd98-488c-a76b-8a33ae49822a",
   "metadata": {},
   "source": [
    "# <span style='background :lightgreen' >4. Programmatically Accessing AI Models running on Local Machine by Ollama </span>\n",
    "\n",
    "## a. Using  Ollama’s Native Python Library\n",
    "- The `ollama.chat()` method is the primary interface for conversational interactions with Llama models through Ollama."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2599e604-9bf2-4c50-aef3-3f1f208299b5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content: The capital of Pakistan is Islamabad.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ChatResponse</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">model</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'llama3.2:1b'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">created_at</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'2026-02-12T04:54:09.501799Z'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">done</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">done_reason</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'stop'</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">total_duration</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">281283791</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">load_duration</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">101627666</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">prompt_eval_count</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">36</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">prompt_eval_duration</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">99912459</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">eval_count</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">eval_duration</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">74809626</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">message</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Message</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">role</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'assistant'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">content</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'The capital of Pakistan is Islamabad.'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">thinking</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">images</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">tool_name</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">tool_calls</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">logprobs</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mChatResponse\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mmodel\u001b[0m=\u001b[32m'llama3.2:1b'\u001b[0m,\n",
       "    \u001b[33mcreated_at\u001b[0m=\u001b[32m'2026-02-12T04:54:09.501799Z'\u001b[0m,\n",
       "    \u001b[33mdone\u001b[0m=\u001b[3;92mTrue\u001b[0m,\n",
       "    \u001b[33mdone_reason\u001b[0m=\u001b[32m'stop'\u001b[0m,\n",
       "    \u001b[33mtotal_duration\u001b[0m=\u001b[1;36m281283791\u001b[0m,\n",
       "    \u001b[33mload_duration\u001b[0m=\u001b[1;36m101627666\u001b[0m,\n",
       "    \u001b[33mprompt_eval_count\u001b[0m=\u001b[1;36m36\u001b[0m,\n",
       "    \u001b[33mprompt_eval_duration\u001b[0m=\u001b[1;36m99912459\u001b[0m,\n",
       "    \u001b[33meval_count\u001b[0m=\u001b[1;36m8\u001b[0m,\n",
       "    \u001b[33meval_duration\u001b[0m=\u001b[1;36m74809626\u001b[0m,\n",
       "    \u001b[33mmessage\u001b[0m=\u001b[1;35mMessage\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mrole\u001b[0m=\u001b[32m'assistant'\u001b[0m,\n",
       "        \u001b[33mcontent\u001b[0m=\u001b[32m'The capital of Pakistan is Islamabad.'\u001b[0m,\n",
       "        \u001b[33mthinking\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "        \u001b[33mimages\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "        \u001b[33mtool_name\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "        \u001b[33mtool_calls\u001b[0m=\u001b[3;35mNone\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[33mlogprobs\u001b[0m=\u001b[3;35mNone\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ollama\n",
    "import rich\n",
    "\n",
    "response = ollama.chat(\n",
    "                        model = 'llama3.2:1b', # \"deepseek-v3.1:671b-cloud\",  \"gpt-oss:20b-cloud\", \"gpt-oss:120b-cloud\", \"qwen3-coder:480b-cloud\"\n",
    "                        messages = [{'role': 'system',  'content': \"You are a helpful assistant\"}, {'role': 'user', 'content': \"What is capital of Pakistan?\"}],\n",
    "                        stream = False,                # Default: False - set True for streaming\n",
    "                        format=None,                   # Default: None - 'json' for JSON output\n",
    "                        tools=None,                    # Default: None - List of tool definitions\n",
    "                        options={                      # Model-specific options\n",
    "                                'temperature': 0.7,         # Default: 0.8 (0.0-2.0, creativity)\n",
    "                                'top_p': 0.9,               # Default: 0.9 (nucleus sampling)\n",
    "                                }\n",
    "                    )\n",
    "\n",
    "print(\"Content:\", response.message.content)\n",
    "rich.print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e2b89f-040c-4993-895b-d26dbd4facb3",
   "metadata": {},
   "source": [
    "## b. Using  OpenAI's `Chat Completion` API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "593058b8-7443-4a68-b354-669b0b5041af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of Pakistan is **Islamabad**.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# Create an OpenAI client instance and specify the base_url as 'http://localhost:11434/v1' → Instead of OpenAI’s cloud (https://api.openai.com/v1), it’s pointing to a local server.\n",
    "# This tells the client: “Send requests to my local Ollama API server, not OpenAI’s servers.”\n",
    "client = OpenAI(base_url=\"http://localhost:11434/v1\", api_key='abc')   # Since Ollama don’t really need authentication. Give any string and it will work\n",
    "\n",
    "# Use OpenAI's Chat Completions API (routed through Hugging Face)\n",
    "response = client.chat.completions.create(\n",
    "                                            model = \"mygemma\",    # \"deepseek-v3.1:671b-cloud\",  \"gpt-oss:20b-cloud\", \"gpt-oss:120b-cloud\", \"qwen3-coder:480b-cloud\"\n",
    "                                            messages=[{'role': 'system',  'content': \"You are a helpful assistant\"}, {'role': 'user', 'content': \"What is capital of Pakistan?\"}],\n",
    "                                            temperature=0.7,\n",
    "                                            max_tokens=500,\n",
    "                                            stream=False\n",
    "                                        )\n",
    "# Extract and print the output text\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf919fef-385a-4448-929f-c8c8cf03d81c",
   "metadata": {},
   "source": [
    "## c. Using  OpenAI's `Responses` API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b40d6fe8-1d76-4124-b1ff-f47350cd1573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of Pakistan is **Islamabad**.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# Create an OpenAI client instance and specify the base_url as 'http://localhost:11434/v1' → Instead of OpenAI’s cloud (https://api.openai.com/v1), it’s pointing to a local server.\n",
    "# This tells the client: “Send requests to my local Ollama API server, not OpenAI’s servers.”\n",
    "client = OpenAI(base_url=\"http://localhost:11434/v1\", api_key=\"abc\") # Since Ollama is running locally, so it don’t really need authentication. Give any string and it will work\n",
    "\n",
    "# Using the Responses API\n",
    "response = client.responses.create(\n",
    "                                    model=\"gpt-oss:20b-cloud\",      # \"deepseek-v3.1:671b-cloud\",  \"gpt-oss:20b-cloud\", \"gpt-oss:120b-cloud\", \"qwen3-coder:480b-cloud\"\n",
    "                                    #input= \"What is the color of the sky? Tell me in a single line.\"      \n",
    "                                    input=[{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"What is the capital of Pakistan.\"}]\n",
    "                                    )\n",
    "\n",
    "# Extract and print the output text\n",
    "print(response.output_text) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06518b1e-c73d-483e-a7db-f527602d4c1c",
   "metadata": {},
   "source": [
    "# <span style='background :lightgreen' >5. Hello World Examples </span>\n",
    "## Writing a Function for our ease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fff6fe83-181a-4675-bb6d-8fc24b9a29c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(base_url=\"http://localhost:11434/v1\", api_key=\"ollama\")\n",
    "\n",
    "def ask_ollama(\n",
    "                user_prompt: str,\n",
    "                developer_prompt: str = \"You are a helpful assistant that provides concise answers.\",\n",
    "                model: str =  \"deepseek-v3.1:671b-cloud\",       # \"llama3.2\",\n",
    "                temperature: float = 0.7,\n",
    "                stream: bool = False\n",
    "            ):\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "    messages=[{\"role\": \"system\", \"content\": developer_prompt}, {\"role\": \"user\", \"content\": user_prompt}],\n",
    "    model=model,\n",
    "    temperature=temperature,\n",
    "    stream=stream\n",
    "    )    \n",
    "    \n",
    "    if stream:                    # Return streaming generator if requested\n",
    "        return response\n",
    "    return response.choices[0].message.content   # Return the aggregated text output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a543756c-5b3d-4ea1-b49d-454734fe1d85",
   "metadata": {},
   "source": [
    "## Examples (Question Answering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "97914ac2-2303-4f43-a201-3e989098a674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Islamabad.\n"
     ]
    }
   ],
   "source": [
    "user_prompt = \"Which is the capital of Pakistan?\"\n",
    "response = ask_ollama(user_prompt=user_prompt, model=\"mygemma\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "296d0f67-3d24-4d4c-9ea2-d95db0f07b76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a short bedtime story of Ali Baba and the Forty Thieves:\n",
      "\n",
      "Ali Baba, a poor woodcutter, once saw forty thieves approach a magic cave. Their leader said, \"Open Sesame!\" and a door in the rock opened. After they left, Ali Baba used the same words, entered, and found treasures. He took some gold home.\n",
      "\n",
      "His greedy brother, Cassim, learned the secret but forgot the words inside the cave. Trapped, the thieves found and killed him. Ali Baba, with help from clever slave Morgiana, tricked and defeated the thieves who sought revenge—ending their threat forever.\n",
      "\n",
      "Ali Baba shared the treasure and lived happily ever after. Goodnight!"
     ]
    }
   ],
   "source": [
    "user_prompt = \"Tell me a bedtime story of Ali baba chalees chor\"\n",
    "\n",
    "# Get streaming generator\n",
    "response = ask_ollama(user_prompt=user_prompt, stream=True)\n",
    "\n",
    "# Read streaming chunks\n",
    "for chunk in response:\n",
    "    if chunk.choices[0].delta.content:\n",
    "        print(chunk.choices[0].delta.content, end=\"\", flush=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feeaabc8-fc2d-4180-9c68-4755a465fa5d",
   "metadata": {},
   "source": [
    "##  Question Answering from Content Passed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "50c192a4-c3c7-49f6-ba07-8218f5f7e53e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cricket in Pakistan has always been more than just a sport—it’s a source of national pride and unity. Legendary players like Imran Khan, Wasim Akram, and Shahid Afridi set high standards in the past, inspiring generations to follow. Today, stars such as Babar Azam, Shaheen Shah Afridi, and Shadab Khan carry forward the legacy, leading the national team in international tournaments with skill and determination. Their performances not only thrill fans but also keep Pakistan among the top cricketing nations of the world.\n",
      "\n",
      "Politics in Pakistan, meanwhile, remains dynamic and often turbulent, with key figures shaping the country’s direction. Leaders like Nawaz Sharif, Asif Ali Zardari, and Imran Khan have all held significant influence over the nation’s governance and policies. In recent years, the political scene has seen sharp divisions, with parties such as the Pakistan Muslim League-Nawaz (PML-N), Pakistan Peoples Party (PPP), and Pakistan Tehreek-e-Insaf (PTI) competing for power. Debates around economic reforms, governance, and foreign policy continue to dominate the national conversation, reflecting the challenges and aspirations of the Pakistani people."
     ]
    }
   ],
   "source": [
    "!cat ../data/names.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "41407ef2-610d-4625-addf-ed9775816d3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The names extracted from the text are:\n",
      "\n",
      "Imran Khan  \n",
      "Wasim Akram  \n",
      "Shahid Afridi  \n",
      "Babar Azam  \n",
      "Shaheen Shah Afridi  \n",
      "Shadab Khan  \n",
      "Nawaz Sharif  \n",
      "Asif Ali Zardari\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "with open(\"../data/names.txt\", \"r\") as f:\n",
    "    file_content = f.read()\n",
    "\n",
    "user_prompt = f\"Extract names from this text:\\n{file_content}\"\n",
    "response = ask_ollama(user_prompt=user_prompt, model=\"deepseek-r1:1.5b\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a75ac3-6cb6-495d-901b-64ec84f92704",
   "metadata": {},
   "source": [
    "## Examples (Binary Classification: Sentiment analysis, Spam detection, Medical diagnosis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "35ac47a2-05ba-4a8f-ae6c-4a47e21804a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive\n"
     ]
    }
   ],
   "source": [
    "developer_prompt = \"You are an expert who will classify a sentense as having either a Positive or Negative sentiment.\"\n",
    "user_prompt = \"I love the youtube videos of Arif, as they are very informative\"\n",
    "response = ask_ollama(user_prompt=user_prompt, developer_prompt=developer_prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee8c2ae-c7b6-4ccd-b468-34d0aba811bf",
   "metadata": {},
   "source": [
    "## Examples (Multi-class Classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b2d5a3d3-369b-4c67-900e-8525252297b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Books\n"
     ]
    }
   ],
   "source": [
    "developer_prompt = \"Classify product reviews into these categories: 'Electronics', 'Clothing', 'Books', 'Home & Garden', 'Sports', or 'Food'. \\\n",
    "Respond with only the category.\"\n",
    "user_prompt = \"This novel has an incredible plot twist that kept me reading all night\"\n",
    "response = ask_ollama(user_prompt=user_prompt, developer_prompt=developer_prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a0b4bcd8-0fe9-47eb-aadb-d5f4f43d8eca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Electronics\n"
     ]
    }
   ],
   "source": [
    "system_role = \"Classify product reviews into these categories: 'Electronics', 'Clothing', 'Books', 'Home & Garden', 'Sports', or 'Food'. \\\n",
    "Respond with only the category.\"\n",
    "user_prompt = \"The wireless headphones have excellent sound quality and battery life\"\n",
    "response = ask_ollama(user_prompt, system_role)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3168300-8b5c-4b31-b87b-27df68d48f11",
   "metadata": {},
   "source": [
    "## Examples (Text Generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ca4b2173-ee79-450b-ae16-e06ce5c31fc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pakistan's February 8, 2024, general elections were marred by significant allegations of pre-poll manipulation and election day irregularities, casting doubts over their fairness. Widespread arrests, internet shutdowns, and delayed results fueled opposition claims of systematic rigging to favor the military-backed establishment. The polls were contentious, failing to gain broad acceptance.\n"
     ]
    }
   ],
   "source": [
    "developer_prompt = \"You are an expert of political science and history and have a deep understanding of policical situation of Pakistan.\"\n",
    "user_prompt = \"Write down a 50 words summary about the fairness of general elections held in Pakistan on February 08, 2024.\"\n",
    "response = ask_ollama(user_prompt=user_prompt, developer_prompt=developer_prompt, temperature=1.0)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708f7920-6fb2-48b0-84d2-838ecc18ca0d",
   "metadata": {},
   "source": [
    "## Examples (Code Generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d59be2fe-5bfd-49e8-824d-a637a0f31450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a C program that generates the first ten numbers of the Fibonacci sequence:\n",
      "\n",
      "```c\n",
      "#include <stdio.h>\n",
      "\n",
      "int main() {\n",
      "    int n = 10; // Number of Fibonacci numbers to generate\n",
      "    int first = 0, second = 1, next;\n",
      "    \n",
      "    printf(\"First %d numbers in Fibonacci sequence:\\n\", n);\n",
      "    \n",
      "    for (int i = 0; i < n; i++) {\n",
      "        if (i <= 1) {\n",
      "            next = i;\n",
      "        } else {\n",
      "            next = first + second;\n",
      "            first = second;\n",
      "            second = next;\n",
      "        }\n",
      "        printf(\"%d \", next);\n",
      "    }\n",
      "    \n",
      "    printf(\"\\n\");\n",
      "    return 0;\n",
      "}\n",
      "```\n",
      "\n",
      "**Output:**\n",
      "```\n",
      "First 10 numbers in Fibonacci sequence:\n",
      "0 1 1 2 3 5 8 13 21 34\n",
      "```\n",
      "\n",
      "**Alternative version using an array:**\n",
      "\n",
      "```c\n",
      "#include <stdio.h>\n",
      "\n",
      "int main() {\n",
      "    int n = 10;\n",
      "    int fib[10];\n",
      "    \n",
      "    fib[0] = 0;\n",
      "    fib[1] = 1;\n",
      "    \n",
      "    printf(\"First %d numbers in Fibonacci sequence:\\n\", n);\n",
      "    printf(\"%d %d \", fib[0], fib[1]);\n",
      "    \n",
      "    for (int i = 2; i < n; i++) {\n",
      "        fib[i] = fib[i-1] + fib[i-2];\n",
      "        printf(\"%d \", fib[i]);\n",
      "    }\n",
      "    \n",
      "    printf(\"\\n\");\n",
      "    return 0;\n",
      "}\n",
      "```\n",
      "\n",
      "Both programs produce the same output. The first version uses a more memory-efficient approach with only three variables, while the second version stores all Fibonacci numbers in an array, which can be useful if you need to access previous values later in the program."
     ]
    }
   ],
   "source": [
    "developer_prompt = \"You are an expert of C programing in C language.\"\n",
    "user_prompt = \"Write down a C program that generates first ten numbers of fibonacci sequence.\"\n",
    "response = ask_ollama(user_prompt=user_prompt, developer_prompt=developer_prompt, stream=True)\n",
    "\n",
    "\n",
    "# Read streaming chunks\n",
    "for chunk in response:\n",
    "    if chunk.choices[0].delta.content:\n",
    "        print(chunk.choices[0].delta.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55ae35a-50d3-4fe5-9a93-dc05bd6a296e",
   "metadata": {},
   "source": [
    "## Examples (Text Translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aef23005-f562-4dbb-a5d4-e74c5f92a5c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "اس سال کا بجٹ کم تنخواہ والے لوگوں پر بہت برا اثر ڈالے گا۔\n"
     ]
    }
   ],
   "source": [
    "user_prompt = \"\"\"\n",
    "Please act as an expert of English to Urdu translator by translating the given sentence from English into Urdu.\n",
    "'The budget this year will have a very bad impact on the low salried people'\n",
    "\"\"\"\n",
    "response = ask_ollama(user_prompt=user_prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af00142-19f0-44ad-9770-71ce9a41b75f",
   "metadata": {},
   "source": [
    "## Examples (Text Summarization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6b320659-90d7-4c64-b4b8-252f416c8212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hugging Face's transformers library is a popular, versatile NLP tool that simplifies working with transformer models.\n"
     ]
    }
   ],
   "source": [
    "developer_prompt = \"You are an expert of English language.\"\n",
    "\n",
    "user_prompt = f'''\n",
    "Summarize the text below in at most 20 words:\n",
    "```The Hugging Face transformers library is an incredibly versatile and powerful tool for natural language processing (NLP).\n",
    "It allows users to perform a wide range of tasks such as text classification, named entity recognition, and question answering, among others.\n",
    "It's an extremely popular library that's widely used by the open-source data science community.\n",
    "It lowers the barrier to entry into the field by providing Data Scientists with a productive, convenient way to work with transformer models.```\n",
    "'''\n",
    "\n",
    "response = ask_ollama(user_prompt=user_prompt, developer_prompt=developer_prompt, temperature=0.2)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90283e29-1d76-4a9d-a50e-3879238f3bce",
   "metadata": {},
   "source": [
    "## Examples (Named Entity Recognition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "34f034fa-a588-478c-b7b2-677ce56d9c0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity: Zelaid Mujahid | Type: name\n",
      "Entity: Data Science | Type: major\n",
      "Entity: University of the Punjab | Type: university\n",
      "Entity: Pakistani | Type: nationality\n",
      "Entity: 3.5 GPA | Type: grades\n",
      "Entity: AI Club | Type: club\n"
     ]
    }
   ],
   "source": [
    "developer_prompt = \"\"\"You are a  Named Entity Recognition specialist. Extract and classify entities from the given text into these categories only if they exist:\n",
    "- name\n",
    "- major\n",
    "- university\n",
    "- nationality\n",
    "- grades\n",
    "- club\n",
    "Format your response as: 'Entity: [text] | Type: [category]' with each entity on a new line.\"\"\"\n",
    "\n",
    "user_prompt = '''\n",
    "Zelaid Mujahid is a sophomore majoring in Data Science at University of the Punjab. \\\n",
    "He is Pakistani national and has a 3.5 GPA. Mujahid is an active member of the department's AI Club.\\\n",
    "He hopes to pursue a career in AI after graduating.\n",
    "'''\n",
    "\n",
    "response = ask_ollama(user_prompt=user_prompt, developer_prompt=developer_prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e925be-6c1a-4115-ab3f-2963489f7adc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d5faf4-ae0c-4427-8ea2-3018da1dfee3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd06f81-e3c0-4b05-a8a2-ea474ac04bf8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (genai-uv)",
   "language": "python",
   "name": "genai-uv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
